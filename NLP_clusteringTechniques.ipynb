{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete guide to various clustering techniques experimented and used for short text clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-9c8_u2yn because the default path (/ms/dist/wmanalytics/PROJ/homedir/1.0/eisdkuqa/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "/ms/dist/python/PROJ/boto/2.46.1/lib/boto/__init__.py:1142: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/ms/dist/python/PROJ/boto/2.46.1/lib/boto/pyami/config.py:98: DeprecationWarning: invalid escape sequence \\s\n",
      "  match = re.match(\"^#import[\\s\\t]*([^\\s^\\t]*)[\\s\\t]*$\", line)\n",
      "/ms/dist/python/PROJ/gensim/3.8.1-py37/exec/lib/gensim/corpora/dictionary.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "/ms/dist/python/PROJ/scipy/1.4.1-ms1-py37/exec/lib/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "/ms/dist/python/PROJ/nltk/3.4.5/lib/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/ms/dist/python/PROJ/nltk/3.4.5/lib/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence, defaultdict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n",
      "SPACY loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ms/dist/python/PROJ/protobuf/3.10.0-py37/exec/lib/google/protobuf/descriptor.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from google.protobuf.pyext import _message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "'''Kernel Python Version 3.6.10 '''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "import jsonschema\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/someppath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "# spacy_model_data_path = \"/Users/pranjalpathak/opt/anaconda3/envs/Python_3.6/lib/python3.6/site-packages/en_core_web_lg/en_core_web_lg-2.2.5\"\n",
    "nlp = spacy.load('en_core_web_lg')  # disabling: nlp = spacy.load(spacy_data_path, disable=['ner'])\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# TF & Keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers as initializers, regularizers, constraints, optimizers\n",
    "from keras.layers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "# from keras.layers.core import Input, Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.models import Sequential, Model, load_model\n",
    "import tensorflow_hub as hub\n",
    "print(\"TensorFlow loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-6c324144-205e-4d12-964b-149f197d24ab\" \n",
       "            onclick=\"_export_df('6c324144-205e-4d12-964b-149f197d24ab')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"6c324144-205e-4d12-964b-149f197d24ab\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We have a client that wants to order a Platinu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i stop process of spousal continuation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can reg D forms be sent via US Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is qualifying under Reg D only done digitally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can you do an ira to roth conversion inkind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT\n",
       "0  We have a client that wants to order a Platinu...\n",
       "1  how do i stop process of spousal continuation ...\n",
       "2                can reg D forms be sent via US Mail\n",
       "3      Is qualifying under Reg D only done digitally\n",
       "4        can you do an ira to roth conversion inkind"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(map(lambda x: x.split('\\t')[1].strip() if len(x.split('\\t')) > 1 else x, corpus.split('\\n'))), columns=['TEXT'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Preprocessing Methods:\n",
    "\n",
    "1. Basic cleaning module\n",
    "2. Spacy cleaning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "------------------------------------------------------------------------------------\n",
    "1. BASIC CLEANING MODULE (without contextual word replacement)\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "## Sample execution ##\n",
    "\n",
    "vocab_list=[\"Morgan Stanley\", \"MS\", \"KORE\", \"MS@Work\", \"?\", \"is\"]\n",
    "resources_dir_path = \"/v/region/na/appl/mswm/ainlp/data/ainlp_dev/Pretrained_Models/Resources/\"\n",
    "preprocessText = basic_preprocessing(resources_dir_path, vocab_list)\n",
    "df[clean_text_col] = preprocessText.clean(df[text_col])\n",
    "\"\"\"\n",
    "\n",
    "class basic_preprocessing:\n",
    "    \"\"\"\n",
    "    Perform basic pre-processing steps on any textual data. To add customization, pass a list of keyowrds which needs to \n",
    "    be excluded from lemmatization or stop-word removal. Use  'vocab_list'  param to pass a customized list of keywords!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resources_dir_path, vocab_list=[], custom=False):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n",
    "        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n",
    "        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n",
    "        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n",
    "        self.vocab_list = vocab_list\n",
    "        self.custom = custom\n",
    "        self.load_resources()\n",
    "        return\n",
    "\n",
    "    def load_resources(self):\n",
    "        # stopwords, special_stopwords, special_characters\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])    \n",
    "        self.stopwords = sorted(set(self.stopwords))\n",
    "\n",
    "        # Vocabulary Dictionary (**custom**)\n",
    "        # - for marking keywords to retain during pre-processing step\n",
    "        # - modify spacy's stopword checker\n",
    "        self.vocab_list += [\"Morgan Stanley\", \"MS\", \"MSO\", \"MS@Work\"]\n",
    "        self.vocab_list = list(map(str.lower, self.vocab_list))\n",
    "        self.vocab_list = sorted(set(self.vocab_list))\n",
    "        # modify stop_words list\n",
    "        self.stopwords = set(self.stopwords).difference(self.vocab_list)\n",
    "        # custom regex using these vocab\n",
    "        self.vocab_dict = {w: \"_\".join(w.split()) for w in self.vocab_list}\n",
    "        self.regex_custom = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n",
    "        # for not_stopword in self.vocab_dict.values():\n",
    "        #     nlp.vocab[not_stopword].is_stop = False\n",
    "   \n",
    "        # contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "        # chat-words\n",
    "        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.chat_words_map_dict, self.chat_words_list = {}, []\n",
    "            chat_words = [x.rstrip() for x in f.readlines()]\n",
    "            for line in chat_words:\n",
    "                cw = line.split(\"=\")[0]\n",
    "                cw_expanded = line.split(\"=\")[1]\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "            self.chat_words_list = set(self.chat_words_list)\n",
    "        # emoticons\n",
    "        with open(self.emoticons_file, \"r\") as f:\n",
    "            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n",
    "        # emojis\n",
    "        self.emojis = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        # greetings\n",
    "        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.greeting_words = [x.rstrip() for x in f.readlines()]\n",
    "        # signature\n",
    "        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.signature_words = [x.rstrip() for x in f.readlines()]\n",
    "        # spell-corrector\n",
    "        self.spell_checker = SpellChecker()   \n",
    "        return\n",
    "\n",
    "    \n",
    "    def custom_cleaning(self, text, reset=False):\n",
    "        \"\"\" *** Perform custom cleaning here... *** \"\"\"\n",
    "        # use global variable \"regex_custom\" in this...\n",
    "        key = \"_\"\n",
    "        text = str(text).strip().lower()\n",
    "        if reset is False:\n",
    "            # custom replacement\n",
    "            text = re.sub(r\"directed[\\s\\,]*share[\\s\\,]*program(?=[\\s\\(\\[]*dsp[\\s\\)\\]]*)\", \"\", text.lower(), re.IGNORECASE)\n",
    "            # compile using a dict of words and their expansions, and sub them if found!\n",
    "            match_and_sub = self.regex_custom.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n",
    "        else:\n",
    "            # reverse the change! - use this at the end of preprocessing\n",
    "            text = text.replace(key, \" \")\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    def clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "\n",
    "            # normalize text to \"utf-8\" encoding\n",
    "            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "       \n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "\n",
    "            # <---------- CUSTOM CLEANING -------------->\n",
    "            #\n",
    "            # Mark imp keywords such as: Domain specific, Question words(wh-words), etc, using \"vocabulary\".\n",
    "            # Create a \"vocab_dict\" first.  This dict shall be used to join these keywords (i.e. join them using \"_\" ), \n",
    "            # during pre-processing step, and later un-joined.\n",
    "            if self.custom:\n",
    "                sent = self.custom_cleaning(sent, reset=False)\n",
    "            #\n",
    "            # <---------- CUSTOM CLEANING -------------->\n",
    "\n",
    "            # remove Emojis ðŸ”¥ðŸ”¥\n",
    "            sent = self.emojis.sub(r'', sent)\n",
    "\n",
    "            # remove emoticons\n",
    "            sent = self.emoticons.sub(r'', sent)\n",
    "\n",
    "            # remove common chat-words\n",
    "            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n",
    "\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            \n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            \n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            \n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            \n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "\n",
    "            # removing punctuations\n",
    "            # - disable them, when sentence structure needs to be retained.\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n",
    "            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)       # sentence delimitters\n",
    "\n",
    "            # keep only text & numbers\n",
    "            # - enable them, when only text and numbers matter\n",
    "            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            \n",
    "            # correct spelling mistakes\n",
    "            # - enable them when english spelling mistakes matter\n",
    "            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n",
    "            \n",
    "            # remove 'modifed' self.stop_words\n",
    "            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords)\n",
    "            \n",
    "            # lemmatize\n",
    "            sent = \" \".join(token.lemma_ for token in nlp(sent) if token.text not in self.stopwords)\n",
    "            \n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "\n",
    "            # <---------- CUSTOM CLEANING -------------->\n",
    "            #\n",
    "            # revers the custom cleaning step!\n",
    "            if self.custom:\n",
    "                sent = self.custom_cleaning(sent, reset=True)\n",
    "            #\n",
    "            # <---------- CUSTOM CLEANING -------------->\n",
    "\n",
    "            cleaned_sentences.append(sent.strip().lower())\n",
    "        return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": [
     12,
     25
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "------------------------------------------------------------------------------------\n",
    "2. SPACY CLEANING MODULE\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "## Sample execution ##\n",
    "\n",
    "preprocessText = spacy_preprocessing(resources_dir_path)\n",
    "process_results = preprocessText.run_pipeline(df[__text_col__].tolist(), operations=[\"basic\",\"deep\",\"spacy\"])\n",
    "df[__clean_text_col__] = [\" \".join(x[\"lemma_list\"]).lower() for x in process_results]\n",
    "\"\"\"\n",
    "\n",
    "class spacy_preprocessing:\n",
    "    \"\"\"\n",
    "    Performs Spacy cleaning, and genrate spacy text features such as pos, tokens, ner, dependency, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, resources_dir_path):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.synonym_file = os.path.join(resources_dir_path, \"synonyms_noun_verb.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.load_resources()\n",
    "        return\n",
    "\n",
    "    def load_resources(self):\n",
    "        self.stopwords = []\n",
    "        self.synonym_dict = {}\n",
    "        self.contractions = {}\n",
    "        # stopwords\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "            f.close()\n",
    "        # special_stopwords\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "            f.close()\n",
    "        # special_characters\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "            f.close()\n",
    "        print(\"stopwords loaded.\")\n",
    "        # synonyms\n",
    "        # POS <tab> Root_Word <tab> Synonym1 <tab> Synonym2 <tab> Synonym3\n",
    "        # e.g. : verb    acknowledge    know    admit    understand\n",
    "        with open(self.synonym_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            synonym_list = [x.rstrip() for x in f.readlines()]\n",
    "            for line in synonym_list:\n",
    "                if len(line) < 4 or re.search(\"^#\", line):\n",
    "                    continue\n",
    "                syn_words = line.lower().split(\"\\t\")  # root word\n",
    "                pos = syn_words[0]                    # POS tag\n",
    "                hwd = syn_words[1]                    # synonyms\n",
    "                syn_words.pop(0)\n",
    "                syn_words.pop(0)\n",
    "                for s in syn_words:\n",
    "                    self.synonym_dict.setdefault(s, {})[pos] = hwd\n",
    "            f.close()\n",
    "        print(\"synonyms_noun_verb loaded.\")\n",
    "        # contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "            f.close()\n",
    "        print(\"contractions loaded.\")\n",
    "\n",
    "    def get_pos_list(self, results):\n",
    "        word_list, pos_ans, ne_list, start_end_list, lemma_list = [], [], [], [], []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                # 1. save tokens\n",
    "                word_list.append(token['word'])\n",
    "                # 2. save pos\n",
    "                pos_syn = \"\"\n",
    "                pos = token['pos'].lower()\n",
    "                pos_ans.append(token['pos'])\n",
    "                # 3. save lemmas\n",
    "                lemma = token['lemma'].lower()\n",
    "                # 3.1. filter lemmas\n",
    "                if lemma in self.stopwords \\\n",
    "                    and lemma not in ['want', 'against', 'further', 'online', 'same', 'under','what', 'want', 'when', 'own'] \\\n",
    "                    or lemma in [\":\", \"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"\\\\\", '-pron-', '_', 'card num', '\"'] \\\n",
    "                    or pos == \":\" \\\n",
    "                    or pos == \".\" \\\n",
    "                    or re.search('^([\\W]*)$]', lemma) \\\n",
    "                    or len(lemma) >= 30:\n",
    "                    # print(\"filter:\", lemma)\n",
    "                    continue\n",
    "                # 2.1. filter pos\n",
    "                if re.search('^nn', pos):\n",
    "                    pos_syn = 'noun'\n",
    "                elif re.search('^v', pos):\n",
    "                    pos_syn = 'verb'\n",
    "                elif re.search('^adj', pos):\n",
    "                    pos_syn = 'adj'\n",
    "                # 3.2. filter lemmas again\n",
    "                if lemma in list(self.synonym_dict.keys()) and pos_syn in list(self.synonym_dict[lemma].keys()):\n",
    "                    lemma = self.synonym_dict[lemma][pos_syn]\n",
    "                # save attributes\n",
    "                lemma_list.append(lemma)\n",
    "                ne_list.append(token['ner'])\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        return word_list, pos_ans, ne_list, start_end_list, lemma_list\n",
    "\n",
    "    def basic_cleaning(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            sent = str(sent).strip()\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "    def deep_cleaning(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            sent = str(sent).strip()\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"(\\d+\\,\\d+)\", \" \", sent)\n",
    "            sent = re.sub(r\"\\;\", \" \", sent)\n",
    "            sent = re.sub(r\"\\!\", \" \", sent)\n",
    "            sent = re.sub(r\"\\(\", \" \", sent)\n",
    "            sent = re.sub(r\"\\)\", \" \", sent)\n",
    "            sent = re.sub(r\"\\?\", \" \", sent)\n",
    "            sent = re.sub(r\"\\{\", \" \", sent)\n",
    "            sent = re.sub(r\"\\}\", \" \", sent)\n",
    "            sent = re.sub(r\"\\[\", \" \", sent)\n",
    "            sent = re.sub(r\"\\]\", \" \", sent)\n",
    "            sent = re.sub(r\"\\:\", \"\", sent)\n",
    "            sent = re.sub(r\"\\-\", \"\", sent)\n",
    "            sent = re.sub(r\"\\_\", \" \", sent)\n",
    "            sent = re.sub(r\"\\s{2,}\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\?,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \"\", sent)\n",
    "            sent = re.sub('\\\"', \"\", sent)\n",
    "            sent = re.sub(r\"'s\\b\", \"\", sent)\n",
    "            sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "            # ---OPTIONAL---  keep only text & numbers\n",
    "            sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            sent = sent.strip().lower()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "    def run_spacy(self, doc, operations='tokenize,ssplit,pos,lemma,ner'):\n",
    "        \"\"\"\n",
    "        Spacy nlp pipeline to generate spacy text features such as pos, tokens, ner, dependency.\n",
    "        Accepts doc=nlp(text)\n",
    "        \"\"\"\n",
    "        # spacy doc\n",
    "        doc_json = doc.to_json()  # Includes all operations given by spacy pipeline\n",
    "\n",
    "        # Get text\n",
    "        text = doc_json['text']\n",
    "\n",
    "        # ---------------------------------------- OPERATIONS  -----------------------------------------\n",
    "        # 1. Extract Entity List\n",
    "        entity_list = doc_json[\"ents\"]\n",
    "\n",
    "        # 2. Create token lib\n",
    "        token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "        # init output json\n",
    "        output_json = {}\n",
    "        output_json[\"sentences\"] = []\n",
    "\n",
    "        # Perform spacy operations on each sent in text\n",
    "        for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "\n",
    "            # init parsers\n",
    "            parse = \"\"\n",
    "            basicDependencies = []\n",
    "            enhancedDependencies = []\n",
    "            enhancedPlusPlusDependencies = []\n",
    "\n",
    "            # init output json\n",
    "            out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "            output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "            # 3. Split sentences by indices(i), add labels (pos, ner, dep, etc.)\n",
    "            for token in doc_json[\"tokens\"]:\n",
    "\n",
    "                if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                    # >>> Extract Entity label\n",
    "                    ner = \"O\"\n",
    "                    for entity in entity_list:\n",
    "                        if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                            ner = entity[\"label\"]\n",
    "\n",
    "                    # >>> Extract dependency info\n",
    "                    dep = token[\"dep\"]\n",
    "                    governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                    governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]:\n",
    "                                                                                     token_lib[token[\"head\"]][\"end\"]]\n",
    "                    dependent = token[\"id\"] + 1\n",
    "                    dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "\n",
    "                    # >>> Extract lemma\n",
    "                    lemma = doc[token[\"id\"]].lemma_\n",
    "\n",
    "                    # 4. Add dependencies\n",
    "                    basicDependencies.append({\"dep\": dep,\n",
    "                                              \"governor\": governor,\n",
    "                                              \"governorGloss\": governorGloss,\n",
    "                                              \"dependent\": dependent,\n",
    "                                              \"dependentGloss\": dependentGloss})\n",
    "\n",
    "                    # 5. Add tokens\n",
    "                    out_token = {\"index\": token[\"id\"] + 1,\n",
    "                                 \"word\": dependentGloss,\n",
    "                                 \"originalText\": dependentGloss,\n",
    "                                 \"characterOffsetBegin\": token[\"start\"],\n",
    "                                 \"characterOffsetEnd\": token[\"end\"]}\n",
    "\n",
    "                    # 6. Add lemmas\n",
    "                    if \"lemma\" in operations:\n",
    "                        out_token[\"lemma\"] = lemma\n",
    "\n",
    "                    # 7. Add POS tagging\n",
    "                    if \"pos\" in operations:\n",
    "                        out_token[\"pos\"] = token[\"tag\"]\n",
    "\n",
    "                    # 8. Add NER\n",
    "                    if \"ner\" in operations:\n",
    "                        out_token[\"ner\"] = ner\n",
    "\n",
    "                    # Update output json\n",
    "                    out_sentence[\"tokens\"].append(out_token)\n",
    "\n",
    "            # 9. Add dependencies operation\n",
    "            if \"parse\" in operations:\n",
    "                out_sentence[\"parse\"] = parse\n",
    "                out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "                out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "                out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "        # ---------------------------------------- OPERATIONS  -----------------------------------------\n",
    "        return output_json\n",
    "\n",
    "    # main\n",
    "    def clean(self, input_sentences, operations=[\"basic\",\"deep\",\"spacy\"]):\n",
    "        \"\"\"\n",
    "        Main module to execute pipeline. Accepts list of strings, and desired operations.\n",
    "        \"\"\"\n",
    "        sentences = input_sentences\n",
    "\n",
    "        # run basic cleaning\n",
    "        if \"basic\" in list(map(str.lower, operations)): sentences = self.basic_cleaning(sentences)\n",
    "\n",
    "        # run deep cleaning\n",
    "        if \"deep\" in list(map(str.lower, operations)): sentences = self.deep_cleaning(sentences)\n",
    "\n",
    "        # run spacy pipeline\n",
    "        if \"spacy\" in list(map(str.lower, operations)):\n",
    "            batch_size = min(int(np.ceil(len(input_sentences)/100)), 500)\n",
    "            sentences = [self.run_spacy(doc) for doc in nlp.pipe(sentences, batch_size=batch_size, n_threads=-1)]\n",
    "            # save spacy results\n",
    "            # word_list, pos_list, ner_list, start_end_list, lemma_list\n",
    "            spacy_sentences = []\n",
    "            for sent in sentences:\n",
    "                word_list, pos_ans, ne_list, start_end_list, lemma_list = self.get_pos_list(sent)\n",
    "                spacy_sentences.append({\"word_list\": word_list,\n",
    "                                        \"pos_list\": pos_ans,\n",
    "                                        \"ner_list\": ne_list,\n",
    "                                        \"start_end_list\": start_end_list,\n",
    "                                        \"lemma_list\": lemma_list})\n",
    "            sentences = spacy_sentences\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [
     0,
     90
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# RESOURCES DIRECTORY\n",
    "\n",
    "# chatwords.txt\n",
    "\"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\"\n",
    "\n",
    "## contractions.json\n",
    "\"\"\"\n",
    "{\"it's\": \"it is\", \"o'clock\": \"of the clock\", \"I'd've\": \"I would have\", \"how's\": \"how is\", \"you'll\": \"you will\", \"could've\": \"could have\", \"that'd've\": \"that would have\", \"I'll've\": \"I will have\", \"she's\": \"she is\", \"there'd\": \"there would\", \"you've\": \"you have\", \"why've\": \"why have\", \"should've\": \"should have\", \"he's\": \"he is\", \"'cause\": \"because\", \"y'all're\": \"you all are\", \"i'd\": \"i would\", \"it'll've\": \"it will have\", \"they'd've\": \"they would have\", \"when's\": \"when is\", \"they'll\": \"they will\", \"we'd've\": \"we would have\", \"where've\": \"where have\", \"will've\": \"will have\", \"it'd\": \"it would\", \"they'll've\": \"they will have\", \"couldn't\": \"could not\", \"hasn't\": \"has not\", \"who'll've\": \"who will have\", \"shan't\": \"shall not\", \"we're\": \"we are\", \"y'all'd\": \"you all would\", \"hadn't\": \"had not\", \"doesn't\": \"does not\", \"aren't\": \"are not\", \"let's\": \"let us\", \"that'd\": \"that would\", \"you'd've\": \"you would have\", \"y'all\": \"you all\", \"won't\": \"will not\", \"isn't\": \"is not\", \"can't\": \"cannot\", \"ma'am\": \"madam\", \"wouldn't've\": \"would not have\", \"who've\": \"who have\", \"what'll\": \"what will\", \"don't\": \"do not\", \"y'all'd've\": \"you all would have\", \"haven't\": \"have not\", \"how'd'y\": \"how do you\", \"what's\": \"what is\", \"might've\": \"might have\", \"we've\": \"we have\", \"would've\": \"would have\", \"mightn't've\": \"might not have\", \"where'd\": \"where did\", \"mightn't\": \"might not\", \"she'd\": \"she would\", \"how'd\": \"how did\", \"won't've\": \"will not have\", \"y'all've\": \"you all have\", \"oughtn't\": \"ought not\", \"here's\": \"here is\", \"I'd\": \"I would\", \"mayn't\": \"may not\", \"why's\": \"why is\", \"who'll\": \"who will\", \"shouldn't\": \"should not\", \"I'm\": \"I am\", \"they're\": \"they are\", \"what're\": \"what are\", \"when've\": \"when have\", \"she'll\": \"she will\", \"mustn't\": \"must not\", \"what've\": \"what have\", \"I'll\": \"I will\", \"you're\": \"you are\", \"he'll\": \"he will\", \"there's\": \"there is\", \"must've\": \"must have\", \"weren't\": \"were not\", \"it'll\": \"it will\", \"who's\": \"who is\", \"to've\": \"to have\", \"needn't've\": \"need not have\", \"they'd\": \"they would\", \"that's\": \"that is\", \"oughtn't've\": \"ought not have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"i've\": \"i have\", \"shan't've\": \"shall not have\", \"shouldn't've\": \"should not have\", \"i'd've\": \"i would have\", \"we'd\": \"we would\", \"ain't\": \"is not\", \"mustn't've\": \"must not have\", \"this's\": \"this is\", \"it'd've\": \"it would have\", \"what'll've\": \"what will have\", \"we'll've\": \"we will have\", \"she'd've\": \"she would have\", \"where's\": \"where is\", \"he'd\": \"he would\", \"so've\": \"so have\", \"so's\": \"so as\", \"you'll've\": \"you will have\", \"sha'n't\": \"shall not\", \"she'll've\": \"she will have\", \"how'll\": \"how will\", \"i'll've\": \"i will have\", \"wasn't\": \"was not\", \"needn't\": \"need not\", \"I've\": \"I have\", \"we'll\": \"we will\", \"there'd've\": \"there would have\", \"didn't\": \"did not\", \"i'm\": \"i am\", \"re \":\"re\"}\n",
    "\"\"\"\n",
    "\n",
    "# emojis.txt\n",
    "\"\"\"\n",
    "u\"\\U0001F600-\\U0001F64F\" \n",
    "u\"\\U0001F300-\\U0001F5FF\"  \n",
    "u\"\\U0001F680-\\U0001F6FF\"  \n",
    "u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "u\"\\U00002702-\\U000027B0\"\n",
    "u\"\\U000024C2-\\U0001F251\"\n",
    "\"\"\"\n",
    "\n",
    "# emoticons.json\n",
    "\"\"\"\n",
    "{\n",
    "    \":\\u2011\\\\)\": \"Happy face or smiley\",\n",
    "    \":\\\\)\": \"Happy face or smiley\",\n",
    "    \":-\\\\]\": \"Happy face or smiley\",\n",
    "    \":\\\\]\": \"Happy face or smiley\",\n",
    "    \":-3\": \"Happy face smiley\",\n",
    "    \":3\": \"Happy face smiley\",\n",
    "    \":->\": \"Happy face smiley\",\n",
    "    \":>\": \"Happy face smiley\",\n",
    "    \"8-\\\\)\": \"Happy face smiley\",\n",
    "    \":o\\\\)\": \"Happy face smiley\",\n",
    "    \":-\\\\}\": \"Happy face smiley\",\n",
    "    \":\\\\}\": \"Happy face smiley\",\n",
    "    \":-\\\\)\": \"Happy face smiley\",\n",
    "    \":c\\\\)\": \"Happy face smiley\",\n",
    "    \":\\\\^\\\\)\": \"Happy face smiley\",\n",
    "    \"=\\\\]\": \"Happy face smiley\",\n",
    "    \"=\\\\)\": \"Happy face smiley\",\n",
    "    \":\\u2011D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \":D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"8\\u2011D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"8D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"X\\u2011D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"XD\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"=D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"=3\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \"B\\\\^D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    \":-\\\\)\\\\)\": \"Very happy\",\n",
    "    \":\\u2011\\\\(\": \"Frown, sad, andry or pouting\",\n",
    "    \":-\\\\(\": \"Frown, sad, andry or pouting\",\n",
    "    \":\\\\(\": \"Frown, sad, andry or pouting\",\n",
    "    \":\\u2011c\": \"Frown, sad, andry or pouting\",\n",
    "    \":c\": \"Frown, sad, andry or pouting\",\n",
    "    \":\\u2011<\": \"Frown, sad, andry or pouting\",\n",
    "    \":<\": \"Frown, sad, andry or pouting\",\n",
    "    \":\\u2011\\\\[\": \"Frown, sad, andry or pouting\",\n",
    "    \":\\\\[\": \"Frown, sad, andry or pouting\",\n",
    "    \":-\\\\|\\\\|\": \"Frown, sad, andry or pouting\",\n",
    "    \">:\\\\[\": \"Frown, sad, andry or pouting\",\n",
    "    \":\\\\{\": \"Frown, sad, andry or pouting\",\n",
    "    \":@\": \"Frown, sad, andry or pouting\",\n",
    "    \">:\\\\(\": \"Frown, sad, andry or pouting\",\n",
    "    \":'\\u2011\\\\(\": \"Crying\",\n",
    "    \":'\\\\(\": \"Crying\",\n",
    "    \":'\\u2011\\\\)\": \"Tears of happiness\",\n",
    "    \":'\\\\)\": \"Tears of happiness\",\n",
    "    \"D\\u2011':\": \"Horror\",\n",
    "    \"D:<\": \"Disgust\",\n",
    "    \"D:\": \"Sadness\",\n",
    "    \"D8\": \"Great dismay\",\n",
    "    \"D;\": \"Great dismay\",\n",
    "    \"D=\": \"Great dismay\",\n",
    "    \"DX\": \"Great dismay\",\n",
    "    \":\\u2011O\": \"Surprise\",\n",
    "    \":O\": \"Surprise\",\n",
    "    \":\\u2011o\": \"Surprise\",\n",
    "    \":o\": \"Surprise\",\n",
    "    \":-0\": \"Shock\",\n",
    "    \"8\\u20110\": \"Yawn\",\n",
    "    \">:O\": \"Yawn\",\n",
    "    \":-\\\\*\": \"Kiss\",\n",
    "    \":\\\\*\": \"Kiss\",\n",
    "    \":X\": \"Kiss\",\n",
    "    \";\\u2011\\\\)\": \"Wink or smirk\",\n",
    "    \";\\\\)\": \"Wink or smirk\",\n",
    "    \"\\\\*-\\\\)\": \"Wink or smirk\",\n",
    "    \"\\\\*\\\\)\": \"Wink or smirk\",\n",
    "    \";\\u2011\\\\]\": \"Wink or smirk\",\n",
    "    \";\\\\]\": \"Wink or smirk\",\n",
    "    \";\\\\^\\\\)\": \"Wink or smirk\",\n",
    "    \":\\u2011,\": \"Wink or smirk\",\n",
    "    \";D\": \"Wink or smirk\",\n",
    "    \":\\u2011P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \":P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \"X\\u2011P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \"XP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \":\\u2011\\u00de\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \":\\u00de\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \":b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \"d:\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \"=p\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \">:P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \":\\u2011/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \":/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \":-[.]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \">:[(\\\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \">:/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \":[(\\\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \"=/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \"=[(\\\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \":L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \"=L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \":S\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    \":\\u2011\\\\|\": \"Straight face\",\n",
    "    \":\\\\|\": \"Straight face\",\n",
    "    \":$\": \"Embarrassed or blushing\",\n",
    "    \":\\u2011x\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    \":x\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    \":\\u2011#\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    \":#\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    \":\\u2011&\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    \":&\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    \"O:\\u2011\\\\)\": \"Angel, saint or innocent\",\n",
    "    \"O:\\\\)\": \"Angel, saint or innocent\",\n",
    "    \"0:\\u20113\": \"Angel, saint or innocent\",\n",
    "    \"0:3\": \"Angel, saint or innocent\",\n",
    "    \"0:\\u2011\\\\)\": \"Angel, saint or innocent\",\n",
    "    \"0:\\\\)\": \"Angel, saint or innocent\",\n",
    "    \":\\u2011b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    \"0;\\\\^\\\\)\": \"Angel, saint or innocent\",\n",
    "    \">:\\u2011\\\\)\": \"Evil or devilish\",\n",
    "    \">:\\\\)\": \"Evil or devilish\",\n",
    "    \"\\\\}:\\u2011\\\\)\": \"Evil or devilish\",\n",
    "    \"\\\\}:\\\\)\": \"Evil or devilish\",\n",
    "    \"3:\\u2011\\\\)\": \"Evil or devilish\",\n",
    "    \"3:\\\\)\": \"Evil or devilish\",\n",
    "    \">;\\\\)\": \"Evil or devilish\",\n",
    "    \"\\\\|;\\u2011\\\\)\": \"Cool\",\n",
    "    \"\\\\|\\u2011O\": \"Bored\",\n",
    "    \":\\u2011J\": \"Tongue-in-cheek\",\n",
    "    \"#\\u2011\\\\)\": \"Party all night\",\n",
    "    \"%\\u2011\\\\)\": \"Drunk or confused\",\n",
    "    \"%\\\\)\": \"Drunk or confused\",\n",
    "    \":-###..\": \"Being sick\",\n",
    "    \":###..\": \"Being sick\",\n",
    "    \"<:\\u2011\\\\|\": \"Dump\",\n",
    "    \"\\\\(>_<\\\\)\": \"Troubled\",\n",
    "    \"\\\\(>_<\\\\)>\": \"Troubled\",\n",
    "    \"\\\\(';'\\\\)\": \"Baby\",\n",
    "    \"\\\\(\\\\^\\\\^>``\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    \"\\\\(\\\\^_\\\\^;\\\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    \"\\\\(-_-;\\\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    \"\\\\(~_~;\\\\) \\\\(\\u30fb\\\\.\\u30fb;\\\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    \"\\\\(-_-\\\\)zzz\": \"Sleeping\",\n",
    "    \"\\\\(\\\\^_-\\\\)\": \"Wink\",\n",
    "    \"\\\\(\\\\(\\\\+_\\\\+\\\\)\\\\)\": \"Confused\",\n",
    "    \"\\\\(\\\\+o\\\\+\\\\)\": \"Confused\",\n",
    "    \"\\\\(o\\\\|o\\\\)\": \"Ultraman\",\n",
    "    \"\\\\^_\\\\^\": \"Joyful\",\n",
    "    \"\\\\(\\\\^_\\\\^\\\\)/\": \"Joyful\",\n",
    "    \"\\\\(\\\\^O\\\\^\\\\)\\uff0f\": \"Joyful\",\n",
    "    \"\\\\(\\\\^o\\\\^\\\\)\\uff0f\": \"Joyful\",\n",
    "    \"\\\\(__\\\\)\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    \"_\\\\(\\\\._\\\\.\\\\)_\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    \"<\\\\(_ _\\\\)>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    \"<m\\\\(__\\\\)m>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    \"m\\\\(__\\\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    \"m\\\\(_ _\\\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    \"\\\\('_'\\\\)\": \"Sad or Crying\",\n",
    "    \"\\\\(/_;\\\\)\": \"Sad or Crying\",\n",
    "    \"\\\\(T_T\\\\) \\\\(;_;\\\\)\": \"Sad or Crying\",\n",
    "    \"\\\\(;_;\": \"Sad of Crying\",\n",
    "    \"\\\\(;_:\\\\)\": \"Sad or Crying\",\n",
    "    \"\\\\(;O;\\\\)\": \"Sad or Crying\",\n",
    "    \"\\\\(:_;\\\\)\": \"Sad or Crying\",\n",
    "    \"\\\\(ToT\\\\)\": \"Sad or Crying\",\n",
    "    \";_;\": \"Sad or Crying\",\n",
    "    \";-;\": \"Sad or Crying\",\n",
    "    \";n;\": \"Sad or Crying\",\n",
    "    \";;\": \"Sad or Crying\",\n",
    "    \"Q\\\\.Q\": \"Sad or Crying\",\n",
    "    \"T\\\\.T\": \"Sad or Crying\",\n",
    "    \"QQ\": \"Sad or Crying\",\n",
    "    \"Q_Q\": \"Sad or Crying\",\n",
    "    \"\\\\(-\\\\.-\\\\)\": \"Shame\",\n",
    "    \"\\\\(-_-\\\\)\": \"Shame\",\n",
    "    \"\\\\(\\u4e00\\u4e00\\\\)\": \"Shame\",\n",
    "    \"\\\\(\\uff1b\\u4e00_\\u4e00\\\\)\": \"Shame\",\n",
    "    \"\\\\(=_=\\\\)\": \"Tired\",\n",
    "    \"\\\\(=\\\\^\\\\\\u00b7\\\\^=\\\\)\": \"cat\",\n",
    "    \"\\\\(=\\\\^\\\\\\u00b7\\\\\\u00b7\\\\^=\\\\)\": \"cat\",\n",
    "    \"=_\\\\^=\\t\": \"cat\",\n",
    "    \"\\\\(\\\\.\\\\.\\\\)\": \"Looking down\",\n",
    "    \"\\\\(\\\\._\\\\.\\\\)\": \"Looking down\",\n",
    "    \"\\\\^m\\\\^\": \"Giggling with hand covering mouth\",\n",
    "    \"\\\\(\\\\\\u30fb\\\\\\u30fb?\": \"Confusion\",\n",
    "    \"\\\\(?_?\\\\)\": \"Confusion\",\n",
    "    \">\\\\^_\\\\^<\": \"Normal Laugh\",\n",
    "    \"<\\\\^!\\\\^>\": \"Normal Laugh\",\n",
    "    \"\\\\^/\\\\^\": \"Normal Laugh\",\n",
    "    \"\\\\\\uff08\\\\*\\\\^_\\\\^\\\\*\\uff09\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^<\\\\^\\\\) \\\\(\\\\^\\\\.\\\\^\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(^\\\\^\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^\\\\.\\\\^\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^_\\\\^\\\\.\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^_\\\\^\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^\\\\^\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^J\\\\^\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\*\\\\^\\\\.\\\\^\\\\*\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\(\\\\^\\u2014\\\\^\\\\\\uff09\": \"Normal Laugh\",\n",
    "    \"\\\\(#\\\\^\\\\.\\\\^#\\\\)\": \"Normal Laugh\",\n",
    "    \"\\\\\\uff08\\\\^\\u2014\\\\^\\\\\\uff09\": \"Waving\",\n",
    "    \"\\\\(;_;\\\\)/~~~\": \"Waving\",\n",
    "    \"\\\\(\\\\^\\\\.\\\\^\\\\)/~~~\": \"Waving\",\n",
    "    \"\\\\(-_-\\\\)/~~~ \\\\($\\\\\\u00b7\\\\\\u00b7\\\\)/~~~\": \"Waving\",\n",
    "    \"\\\\(T_T\\\\)/~~~\": \"Waving\",\n",
    "    \"\\\\(ToT\\\\)/~~~\": \"Waving\",\n",
    "    \"\\\\(\\\\*\\\\^0\\\\^\\\\*\\\\)\": \"Excited\",\n",
    "    \"\\\\(\\\\*_\\\\*\\\\)\": \"Amazed\",\n",
    "    \"\\\\(\\\\*_\\\\*;\": \"Amazed\",\n",
    "    \"\\\\(\\\\+_\\\\+\\\\) \\\\(@_@\\\\)\": \"Amazed\",\n",
    "    \"\\\\(\\\\*\\\\^\\\\^\\\\)v\": \"Laughing,Cheerful\",\n",
    "    \"\\\\(\\\\^_\\\\^\\\\)v\": \"Laughing,Cheerful\",\n",
    "    \"\\\\(\\\\(d[-_-]b\\\\)\\\\)\": \"Headphones,Listening to music\",\n",
    "    \"\\\\(-\\\"-\\\\)\": \"Worried\",\n",
    "    \"\\\\(\\u30fc\\u30fc;\\\\)\": \"Worried\",\n",
    "    \"\\\\(\\\\^0_0\\\\^\\\\)\": \"Eyeglasses\",\n",
    "    \"\\\\(\\\\\\uff3e\\uff56\\\\\\uff3e\\\\)\": \"Happy\",\n",
    "    \"\\\\(\\\\\\uff3e\\uff55\\\\\\uff3e\\\\)\": \"Happy\",\n",
    "    \"\\\\(\\\\^\\\\)o\\\\(\\\\^\\\\)\": \"Happy\",\n",
    "    \"\\\\(\\\\^O\\\\^\\\\)\": \"Happy\",\n",
    "    \"\\\\(\\\\^o\\\\^\\\\)\": \"Happy\",\n",
    "    \"\\\\)\\\\^o\\\\^\\\\(\": \"Happy\",\n",
    "    \":O o_O\": \"Surprised\",\n",
    "    \"o_0\": \"Surprised\",\n",
    "    \"o\\\\.O\": \"Surpised\",\n",
    "    \"\\\\(o\\\\.o\\\\)\": \"Surprised\",\n",
    "    \"oO\": \"Surprised\",\n",
    "    \"\\\\(\\\\*\\uffe3m\\uffe3\\\\)\": \"Dissatisfied\",\n",
    "    \"\\\\(\\u2018A`\\\\)\": \"Snubbed or Deflated\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# greeting_words.txt\n",
    "\"\"\"\n",
    "\"awesome to see you\",\n",
    "\"dear\",\n",
    "\"dear sir\",\n",
    "\"dear madam\",\n",
    "\"dear ma'am\",\n",
    "\"dear friend\",\n",
    "\"dear sister\",\n",
    "\"dear brother\",\n",
    "\"dear sire\",\n",
    "\"dear sirs\",\n",
    "\"dear sir/madam\",\n",
    "\"dear maam\",\n",
    "\"dear maams\",\n",
    "\"feels warm to see you\",\n",
    "\"feels good to see you\",\n",
    "\"feels pretty good to see you\",\n",
    "\"feels nice to see you\",\n",
    "\"feels awesome to see you\",\n",
    "\"feels great to see you\",\n",
    "\"feels wonderful to see you\",\n",
    "\"gentlemen and ladies\",\n",
    "\"gmorning\",\n",
    "\"good morning\",\n",
    "\"goodmorning\",\n",
    "\"morning\",\n",
    "\"great day\",\n",
    "\"good day\",\n",
    "\"g'day\",\n",
    "\"g' day\",\n",
    "\"good afternoon\",\n",
    "\"g'afternoon\",\n",
    "\"g'noon\",\n",
    "\"g' noon\",\n",
    "\"good evening\",\n",
    "\"g'evening\",\n",
    "\"g' evening\",\n",
    "\"g'eve\",\n",
    "\"good greeting\",\n",
    "\"good greetings\",\n",
    "\"greetings\",\n",
    "\"greeting\",\n",
    "\"good to see you\",\n",
    "\"pretty good to see you\",\n",
    "\"great to see you\",\n",
    "\"hi\",\n",
    "\"hello\",\n",
    "\"hey\",\n",
    "\"helloo\",\n",
    "\"hellooo\",\n",
    "\"howdy\",\n",
    "\"heyyou\",\n",
    "\"heyy\",\n",
    "\"hii\",\n",
    "\"hie\",\n",
    "\"hola\",\n",
    "\"hii\",\n",
    "\"heyi\",\n",
    "\"heyy\",\n",
    "\"hiii\",\n",
    "\"helloo\",\n",
    "\"heya\",\n",
    "\"hey there\",\n",
    "\"hi there\",\n",
    "\"hello there\",\n",
    "\"hi team\",\n",
    "\"hello team\",\n",
    "\"hey team\",\n",
    "\"how are you\",\n",
    "\"how're you\",\n",
    "\"how are you doing\",\n",
    "\"how ya doin'\",\n",
    "\"how ya doin\",\n",
    "\"how is everything\",\n",
    "\"how is everything going\",\n",
    "\"how's everything going\",\n",
    "\"how is you\",\n",
    "\"how's you\",\n",
    "\"how is it going\",\n",
    "\"how's it going\",\n",
    "\"how's it goin'\",\n",
    "\"how's it goin\",\n",
    "\"how's life treating you\",\n",
    "\"how is life treating you\",\n",
    "\"how is life been treating you\",\n",
    "\"how's life been treating you\",\n",
    "\"how are thing\",\n",
    "\"how're thing\",\n",
    "\"how're things\",\n",
    "\"how are things\",\n",
    "\"how're things with you\",\n",
    "\"how are things with you\",\n",
    "\"how you'd been\",\n",
    "\"how you had been\",\n",
    "\"how you been\",\n",
    "\"how have you been\",\n",
    "\"how've you been\",\n",
    "\"its good seeing you\",\n",
    "\"its nice seeing you\",\n",
    "\"its great seeing you\",\n",
    "\"its awesome seeing you\",\n",
    "\"its wonderful seeing you\",\n",
    "\"ladies and gentlemen\",\n",
    "\"ladies & gentlemen\",\n",
    "\"ladies and gents\",\n",
    "\"ladies & gents\",\n",
    "\"mornin\",\n",
    "\"moriningg\",\n",
    "\"morining\",\n",
    "\"nice to see you\",\n",
    "\"obliged to see you\",\n",
    "\"respected\",\n",
    "\"respected teacher\",\n",
    "\"respected concerned\",\n",
    "\"respected sir\",\n",
    "\"respected madam\",\n",
    "\"respected ma'am\",\n",
    "\"respected sire\",\n",
    "\"respected sirs\",\n",
    "\"respected sir/madam\",\n",
    "\"respected maam\",\n",
    "\"respected maams\",\n",
    "\"very nice to see you\",\n",
    "\"wonderful to see you\",\n",
    "\"what is up\",\n",
    "\"what's up\",\n",
    "\"what's upp\",\n",
    "\"what is upp\",\n",
    "\"what is cracking\",\n",
    "\"what's cracking\",\n",
    "\"what is good\",\n",
    "\"what's good\",\n",
    "\"what is happening\",\n",
    "\"what's happening\",\n",
    "\"what is new\",\n",
    "\"what's new\",\n",
    "\"what is neww\",\n",
    "\"\"\"\n",
    "\n",
    "# signature_words.txt\n",
    "\"\"\"\n",
    "\"all best\",\n",
    "\"all the best\",\n",
    "\"all the bests\",\n",
    "\"all my best\",\n",
    "\"all my regards\",\n",
    "\"all my regard\",\n",
    "\"all my regds\",\n",
    "\"all my rgds\",\n",
    "\"best wishes\",\n",
    "\"best regards\",\n",
    "\"best regard\",\n",
    "\"best regds\",\n",
    "\"best rgds\",\n",
    "\"bGIF\",\n",
    "\"ciao\",\n",
    "\"cordially\",\n",
    "\"cheers\",\n",
    "\"kind regards\",\n",
    "\"kind regard\",\n",
    "\"kind regds\",\n",
    "\"kind rgds\",\n",
    "\"kindest regards\",\n",
    "\"kindest regard\",\n",
    "\"kindest regds\",\n",
    "\"kindest rgds\",\n",
    "\"many cheers\",\n",
    "\"many thanks\",\n",
    "\"many many thanks\",\n",
    "\"my best\",\n",
    "\"my regards\",\n",
    "\"my regard\",\n",
    "\"my regds\",\n",
    "\"my rgds\",\n",
    "\"sent from my android\",\n",
    "\"sent from my iphone\",\n",
    "\"sent from my phone\",\n",
    "\"sincerely\",\n",
    "\"regards\",\n",
    "\"regard\",\n",
    "\"regds\",\n",
    "\"rgds\",\n",
    "\"thanks\",\n",
    "\"thanks and regards\",\n",
    "\"thanks and regard\",\n",
    "\"thanks and regrds\",\n",
    "\"thanks and regrd\",\n",
    "\"thanks and rgards\",\n",
    "\"thanks and rgard\",\n",
    "\"thanks and regds\",\n",
    "\"thanks and regd\",\n",
    "\"thanks and rgds\",\n",
    "\"thanks and rgd\",\n",
    "\"thanks & regards\",\n",
    "\"thanks & regard\",\n",
    "\"thanks & regrds\",\n",
    "\"thanks & regrd\",\n",
    "\"thanks & rgards\",\n",
    "\"thanks & rgard\",\n",
    "\"thanks & regds\",\n",
    "\"thanks & regd\",\n",
    "\"thanks & rgds\",\n",
    "\"thanks & rgd\",\n",
    "\"thankx and regards\",\n",
    "\"thankx and regard\",\n",
    "\"thx and regards\",\n",
    "\"thankx & regards\",\n",
    "\"thankx & regard\",\n",
    "\"thx & regards\",\n",
    "\"thank and regards\",\n",
    "\"thank and regard\",\n",
    "\"thank you and regards\",\n",
    "\"thank you and regard\",\n",
    "\"thankyou and regards\",\n",
    "\"thankyou and regard\",\n",
    "\"thank you & regards\",\n",
    "\"thank you & regard\",\n",
    "\"thankyou & regards\",\n",
    "\"thankyou & regard\",\n",
    "\"thank & regards\",\n",
    "\"thank & regard\",\n",
    "\"thanking you and regards\",\n",
    "\"thanking you and regard\",\n",
    "\"thankingyou and regards\",\n",
    "\"thankingyou and regard\",\n",
    "\"thanks so much\",\n",
    "\"thanks you\",\n",
    "\"thank you\",\n",
    "\"thankyou\",\n",
    "\"thxxx\",\n",
    "\"thankingu\",\n",
    "\"thankyou so much\",\n",
    "\"thanking you\",\n",
    "\"thankingyou\",\n",
    "\"thank you so much\",\n",
    "\"take care\",\n",
    "\"talk soon\",\n",
    "\"will talk soon\",\n",
    "\"will talk very soon\",\n",
    "\"will talk sooner\",\n",
    "\"warmest\",\n",
    "\"warm regards\", \n",
    "\"warm regds\",\n",
    "\"warm rgds\",\n",
    "\"warm regard\", \n",
    "\"warmest regards\",\n",
    "\"warmest regds\",\n",
    "\"warmest rgds\",\n",
    "\"warmest regard\",\n",
    "\"warmly\",\n",
    "\"with many cheers\",\n",
    "\"with many thanks\",\n",
    "\"with warmth\",\n",
    "\"with warmest\",\n",
    "\"with warm regards\",\n",
    "\"with warm regds\",\n",
    "\"with warm rgds\",\n",
    "\"with warm regard\",\n",
    "\"with regards\",\n",
    "\"with regard\",\n",
    "\"with regds\",\n",
    "\"with rgds\",\n",
    "\"your cordially\",\n",
    "\"yours cordially\",\n",
    "\"your truly\",\n",
    "\"yours truly\",\n",
    "\"your sincerely\",\n",
    "\"yours sincerely\",\n",
    "\"your loving\",\n",
    "\"yours loving\",\n",
    "\"your lovingly\",\n",
    "\"yours lovingly\",\n",
    "\"your honestly\",\n",
    "\"yours honestly\"\n",
    "\"\"\"\n",
    "\n",
    "# special_characters.txt\n",
    "\"\"\"\n",
    ",\n",
    ".\n",
    "\"\n",
    ":\n",
    ")\n",
    "(\n",
    "-\n",
    "!\n",
    "?\n",
    "|\n",
    ";\n",
    "'\n",
    "$\n",
    "&\n",
    "/\n",
    "[\n",
    "]\n",
    ">\n",
    "%\n",
    "=\n",
    "#\n",
    "*\n",
    "+\n",
    "\\\\\n",
    "â€¢\n",
    "~\n",
    "@\n",
    "Â£\n",
    "Â·\n",
    "_\n",
    "{\n",
    "}\n",
    "Â©\n",
    "^\n",
    "Â®\n",
    "`\n",
    "<\n",
    "â†’\n",
    "Â°\n",
    "â‚¬\n",
    "â„¢\n",
    "â€º\n",
    "â™¥\n",
    "â†\n",
    "Ã—\n",
    "Â§\n",
    "â€³\n",
    "â€²\n",
    "Ã‚\n",
    "â–ˆ\n",
    "Â½\n",
    "Ã \n",
    "â€¦\n",
    "\"\n",
    "\"\"\n",
    "â˜…\n",
    "â€“\n",
    "â—\n",
    "Ã¢\n",
    "â–º\n",
    "âˆ’\n",
    "Â¢\n",
    "Â²\n",
    "Â¬\n",
    "â–‘\n",
    "Â¶\n",
    "â†‘\n",
    "Â±\n",
    "Â¿\n",
    "â–¾\n",
    "â•\n",
    "Â¦\n",
    "â•‘\n",
    "â€•\n",
    "Â¥\n",
    "â–“\n",
    " - \n",
    "--\n",
    "â€¹\n",
    "â”€\n",
    "â–’\n",
    "ï¼š\n",
    "Â¼\n",
    "âŠ•\n",
    "â–¼\n",
    "â–ª\n",
    "â€ \n",
    "â– \n",
    "'\n",
    "â–€\n",
    "Â¨\n",
    "â–„\n",
    "â™«\n",
    "â˜†\n",
    "Ã©\n",
    "Â¯\n",
    "â™¦\n",
    "Â¤\n",
    "â–²\n",
    "Ã¨\n",
    "Â¸\n",
    "Â¾\n",
    "Ãƒ\n",
    "â‹…\n",
    "'\n",
    "âˆž\n",
    "âˆ™\n",
    "ï¼‰\n",
    "â†“\n",
    "ã€\n",
    "â”‚\n",
    "ï¼ˆ\n",
    "Â»\n",
    "ï¼Œ\n",
    "â™ª\n",
    "â•©\n",
    "â•š\n",
    "Â³\n",
    "ãƒ»\n",
    "â•¦\n",
    "â•£\n",
    "â•”\n",
    "â•—\n",
    "â–¬\n",
    "â¤\n",
    "Ã¯\n",
    "Ã˜\n",
    "Â¹\n",
    "â‰¤\n",
    "â€¡\n",
    "âˆš\n",
    "\"\"\"\n",
    "    \n",
    "# special_stopwords.txt\n",
    "\"\"\"\n",
    "https\n",
    "http\n",
    "india\n",
    "usa\n",
    "com\n",
    "team\n",
    "please\n",
    "pls\n",
    "let\n",
    "know\n",
    "respond\n",
    "name\n",
    "jan\n",
    "feb\n",
    "mar\n",
    "apr\n",
    "may\n",
    "jun\n",
    "jul\n",
    "aug\n",
    "sep\n",
    "oct\n",
    "nov\n",
    "dec\n",
    "asap\n",
    "january\n",
    "february\n",
    "march\n",
    "april\n",
    "june\n",
    "july\n",
    "august\n",
    "september\n",
    "october\n",
    "november\n",
    "december\n",
    "dear\n",
    "kindly\n",
    "regards\n",
    "screenshot\n",
    "sunday\n",
    "monday\n",
    "tuesday\n",
    "wednesday\n",
    "thursday\n",
    "friday\n",
    "saturday\n",
    "pfa\n",
    "bangalore\n",
    "mumbai\n",
    "pune\n",
    "thanks\n",
    "thank\n",
    "thanking\n",
    "thanks\n",
    "thnx\n",
    "thankyou\n",
    "thanksyou\n",
    "takecare\n",
    "welcome\n",
    "grateful\n",
    "yep\n",
    "yeah\n",
    "nope\n",
    "noo\n",
    "\"\"\"\n",
    "    \n",
    "# stopwords.txt\n",
    "\"\"\"\n",
    "``\n",
    "''\n",
    ".\n",
    ",\n",
    "'\n",
    "\"\n",
    "/\n",
    "?\n",
    "-\n",
    ":\n",
    "#\n",
    "-lsb-\n",
    "-rsb-\n",
    "a\n",
    "at\n",
    "about\n",
    "above\n",
    "after\n",
    "again\n",
    "against\n",
    "all\n",
    "am\n",
    "an\n",
    "and\n",
    "any\n",
    "are\n",
    "as\n",
    "be\n",
    "because\n",
    "been\n",
    "before\n",
    "being\n",
    "below\n",
    "between\n",
    "both\n",
    "but\n",
    "did\n",
    "does\n",
    "do\n",
    "doing\n",
    "don\n",
    "down\n",
    "during\n",
    "each\n",
    "few\n",
    "for\n",
    "from\n",
    "further\n",
    "had\n",
    "has\n",
    "have\n",
    "having\n",
    "he\n",
    "here\n",
    "hers\n",
    "herself\n",
    "him\n",
    "himself\n",
    "i\n",
    "if\n",
    "into\n",
    "is\n",
    "it\n",
    "its\n",
    "itself\n",
    "just\n",
    "me\n",
    "more\n",
    "most\n",
    "myself\n",
    "nor\n",
    "noo\n",
    "no\n",
    "nope\n",
    "nada\n",
    "now\n",
    "off\n",
    "once\n",
    "online\n",
    "only\n",
    "or\n",
    "other\n",
    "ours\n",
    "ourselves\n",
    "out\n",
    "over\n",
    "own\n",
    "please\n",
    "'s\n",
    "s\n",
    "same\n",
    "should\n",
    "so\n",
    "some\n",
    "such\n",
    "t\n",
    "than\n",
    "that\n",
    "the\n",
    "theirs\n",
    "them\n",
    "themselves\n",
    "then\n",
    "there\n",
    "these\n",
    "this\n",
    "those\n",
    "through\n",
    "too\n",
    "under\n",
    "until\n",
    "up\n",
    "very\n",
    "want\n",
    "was\n",
    "were\n",
    "while\n",
    "will\n",
    "with\n",
    "w8\n",
    "w88\n",
    "you\n",
    "yo\n",
    "yay\n",
    "yoo\n",
    "yeda\n",
    "yuu\n",
    "yooo\n",
    "&\n",
    "my\n",
    "of\n",
    "on\n",
    "she\n",
    "to\n",
    "we\n",
    "in\n",
    "they\n",
    "\"\"\"\n",
    "    \n",
    "# synonyms_noun_verb.txt\n",
    "\"\"\"\n",
    "verb\tacknowledge\tknow\tadmit\tunderstand\n",
    "verb\tactivate\ttrigger\n",
    "verb\taggregate\taccumulate\tcollect\tgather\n",
    "verb\talert\tnotify\tinform\n",
    "verb\tallocate\tallot\tdesignate\n",
    "verb\tallow\tpermit\n",
    "verb\tanswer\treply\trespond\n",
    "verb\tapprove\taccept\tvalidate\n",
    "verb\task\tinquire\n",
    "verb\tattempt\ttry\tendeavor\n",
    "verb\tcalculate\tcompute\n",
    "verb\tcancel\tterminate\trevoke\tvoid\twaive\n",
    "verb\tcheck\texamine\tinspect\n",
    "verb\tcircumvent\tavoid\tbypass\tsidestep\n",
    "verb\tclose\tshut\tshut down\tturn off\n",
    "verb\tcompare\tcontrast\tcollate\tdifferenciate\n",
    "verb\tcomply\tagree\tconfirm\tacknowledge\tsatisfy\tobey\n",
    "verb\tconcentrate\tfocus\n",
    "verb\tconnect\tjoin\tlink\n",
    "verb\tconsist\tcomprise\tinclude\tcontain\n",
    "verb\tconsolidate\tunify\n",
    "verb\tcontact\tassociate\n",
    "verb\tcontinue\tmaintain\n",
    "verb\tdecease\tdie\texpire\n",
    "verb\tdecide\tdetermine\n",
    "verb\tdeduct\tsubtract\twithhold\n",
    "verb\tdefend\tprotect\tguard\n",
    "verb\tdefine\tdescribe\n",
    "verb\tdelay\tlag\tpostpone\thold-up\n",
    "verb\tdelete\tremove\terase\teliminate\n",
    "verb\tdelink\tdisconnect\tde-link\n",
    "verb\tdescribe\texplain\n",
    "verb\tdisclose\treveal\n",
    "verb\tdonate\tcontribute\n",
    "verb\tduplicate\tcopy\treplicate\n",
    "verb\tedit\trevise\tcorrect\temend\tchange\tmodify\tcustomize\tamend\talter\trevise\tfix\tupdate\n",
    "verb\tenergize\tinvigorate\tstimulate\tstrenghthen\n",
    "verb\tenlarge\texpand\tmagnify\n",
    "verb\tenroll\tregister\n",
    "verb\tentail\tencompass\tinvolve\tcomprise\n",
    "verb\testimate\tappraisal\tassessment\tmeasure\n",
    "verb\texcess\tsurplus\n",
    "verb\texclude\teliminate\tomit\n",
    "verb\texecute\tcarry out\n",
    "verb\tfill\tfill out\n",
    "verb\tfind\tdiscover\tfind out\trecover\n",
    "verb\tfix\tcorrect\tresolve\n",
    "verb\tfollow\tcome after\n",
    "verb\tforbid\tprohibit\tban\n",
    "verb\tgenerate\tyield\tbring forth\n",
    "verb\tget\tacquire\tobtain\n",
    "verb\tgray\tgray out\tgrey\tgrey out\n",
    "verb\thandle\ttreat\tdeal\tmanage\n",
    "verb\thappen\toccur\ttake place\n",
    "verb\thelp\taid\tassist\tsupport\n",
    "verb\thide\tconceal\n",
    "verb\thurry\trush\thasten\taccelerate\texpedite\n",
    "verb\timport\tbring into\n",
    "verb\tinvest\tfund\n",
    "verb\tleave\texit\texit out\n",
    "verb\tlend\tloan\n",
    "verb\tliquidate\tdissolve\tconvert to cash\tsell off\n",
    "verb\tlog in\taccess\n",
    "verb\tmark\tlabel\ttag\n",
    "verb\tmean\timply\tdenote\tsignify\n",
    "verb\tmeet\tencounter\n",
    "verb\tmerge\tunify\tunite\tblend\tfuse\tcombine\n",
    "verb\tprint\tprint out\n",
    "verb\tprove\tauthenticate\tverify\tvalidate\n",
    "verb\tprovide\tsupply\tcater\toffer\n",
    "verb\tpull\tpull up\n",
    "verb\traise\tlift\televate\tbring up\n",
    "verb\tremit\tsend\tsend out\tresend\tsubmit\tresubmit\n",
    "verb\treturn\tcome back\trefund\tgive back\n",
    "verb\treview\treexamine\n",
    "verb\tsearch\tlook for\tseek\tbrowse\n",
    "verb\tsee\tview\twatch\n",
    "verb\tseparate\tdivide\tsplit\tsplit up\tbreak up\n",
    "verb\tshow\tshow up\tdisplay\treveal\tunhide\n",
    "verb\tspeak\ttalk\n",
    "verb\tstart\tbegin\tcommence\tlaunch\n",
    "verb\tstop\tcease\n",
    "verb\tstore\tsave\n",
    "verb\tthink\tconsider\n",
    "verb\ttransfer\ttransfer out\n",
    "verb\tun-enroll\tunenroll\tuneroll\n",
    "verb\tunhide\tshow\n",
    "verb\tuse\tutilize\tapply\n",
    "verb\twant\twould like\n",
    "# noun synonyms\n",
    "noun\tability\tcapability\n",
    "noun\taccount\tacct\taccnt\taccts\taccnts\tacccount\taccont\tacc\tacct\taccountss\taccunts\taccts\taccountss\taccnts\taccoun\taccnts\t\n",
    "noun\taccuracy\tprecision\tcorrectness\n",
    "noun\taddendum\tappendix\n",
    "noun\taddress\tlocation\taddrees\taddres\taddtress\tadresss\n",
    "noun\tadjustment\tmodification\tamendment\n",
    "noun\tadmin\tadminister\tadministrator\n",
    "noun\tadvice\tsuggestion\n",
    "noun\taffidavit\toath\tstatement\n",
    "noun\taggregation\taddition\n",
    "noun\tagreement\tconsensus\n",
    "noun\tallocation\tallotment\tportion\n",
    "noun\talternative\tsubstitute\tback-up\n",
    "noun\tanalysis\tanalytics\n",
    "noun\tappointment\tmeeting\n",
    "noun\tapproval\tconsent\tpermission\t\n",
    "noun\tarbitration\tadjudication\tjudgment\n",
    "noun\tarticle\treport\tpiece\n",
    "noun\tasset\tproperty\n",
    "noun\tassistance\thelp\tsupport\n",
    "noun\tattachment\taccessory\n",
    "noun\taverage\tmediocre\t\n",
    "noun\taward\tprize\ttrophy\n",
    "noun\tbackup\tsubstitute\talternate\n",
    "noun\tbank\tfinancial institution\tfinance company\n",
    "noun\tbankruptcy\tinsolvency\n",
    "noun\tbaseline\tbasis\n",
    "noun\tbatch\tgroup\tlot\n",
    "noun\tbenchmark\tstandard\n",
    "noun\tbeneficial\tbenefit\tadvantage\n",
    "noun\tbeneficiary\tinheritor\their\n",
    "noun\tboard\tcommittee\n",
    "noun\tbond\tfixed-income securities\tcorporate bonds\tgovernment bonds\n",
    "noun\tbranch\tdivision\tdepartment\n",
    "noun\tbroker\tdiscount brokers\tfull service brokers\n",
    "noun\tbrokerage\tbrokerage company\n",
    "noun\tbuilding\tconstruction\t\n",
    "noun\tbuyer\tpurchaser\tshopper\tcustomer\n",
    "noun\tcalculation\tcomputation\n",
    "noun\tcancellation\tanullment\n",
    "noun\tcapacity\tquantity\n",
    "noun\tcashier\tclerk\tteller\ttreasurer\n",
    "noun\tcategory\tclass\n",
    "noun\tcharacter\tpersonality\tdisposition\n",
    "noun\tcharity\tgoodwill\n",
    "noun\tchart\tgraph\n",
    "noun\tcitizenship\tnationality\n",
    "noun\tclaim\tdemand\n",
    "noun\tclient\tuser\tcustomer\n",
    "noun\tclosure\ttermination\tshutdown\n",
    "noun\tcoin\tmoney\tdollar\tcurrency\tcash\n",
    "noun\tcollege\tuniversity\n",
    "noun\tcomment\tremark\n",
    "noun\tcommodity\tmerchandise\tgoods\tproduct\n",
    "noun\tcompetitor\tchallenger\topponent\n",
    "noun\tcompliance\tconformity\n",
    "noun\tcomponent\tconstituent\n",
    "noun\tconsequence\tresult\toutcome\n",
    "noun\tconservator\tcustodian\tguardian\n",
    "noun\tcontext\tsituation\tcircumstances\n",
    "noun\tcontinuation\tmaintenance\n",
    "noun\tcontract\tarrangement\n",
    "noun\tconversation\ttalk\n",
    "noun\tconversion\talteration\n",
    "noun\tcorporate\tcompany\n",
    "noun\tcorrect\taccurate\tprecise\n",
    "noun\tcredit\tloan\tadvance\n",
    "noun\tcustodial\tprotective\n",
    "noun\tcustodian\tprotector\tguardian\n",
    "noun\tcutoff\tinterruption\tpause\n",
    "noun\tcycle\tcourse\tperiod\n",
    "noun\tdashboard\thome page\n",
    "noun\tdeath\tdecease\tdemise\tcasualty\n",
    "noun\tdebit\tpayment made\n",
    "noun\tdebt\tliability\tamount owed\n",
    "noun\tdecent\tgood\trespectable\n",
    "noun\tdecision\tjudgement\tchoice\n",
    "noun\tdefault\tnonpayment\n",
    "noun\tdelivery\tshipment\tconsignment\tdistribute\n",
    "noun\tedelivery\tedelivey\tedleivery\tedlivery\teedelivery\n",
    "noun\tverb\tdiffer\tdiverge\tdiversify\treverse\n",
    "noun\tdifference\tdistinction\tdivergence\t\n",
    "noun\tdisbursement\texpenditure\tspending\n",
    "noun\tdisclosure\tannouncement\trevelation\n",
    "noun\tdiscount\tconcession\n",
    "noun\tdiscrepancy\tconflict\tdisagreement\n",
    "noun\tdistribution\tdispersion\n",
    "noun\tdividend\tgain\tpremium\treturn\n",
    "noun\tdivorce\tbreakup\tseparation\n",
    "noun\tdocument\tfile\n",
    "noun\tdraft\tplan\tblueprint\n",
    "noun\tdurable\treliable\tlasting\tdependable\tstable\n",
    "noun\teffective\tproductive\tefficient\n",
    "adj\telectronic\telectronically\n",
    "noun\teligibility\tqualification\n",
    "noun\temployee\tworker\n",
    "noun\twork\tworks\n",
    "noun\ttrade\ttrading\n",
    "noun\tenforcement\timposition\n",
    "noun\tsub-user\tsubuser\tsubusername\n",
    "noun\tenough\tadequate\tsufficient\n",
    "noun\tregistration\tenrollment\n",
    "adj\tentire\tcomplete\twhole\n",
    "noun\tphone\tiphone\n",
    "noun\tmercedes\tmercedes-benz\n",
    "noun\tentitlement\tprivilege\tauthorization\n",
    "noun\tequity\tshare\tstock\n",
    "noun\terror\tmistake\twrong\tinaccuracy\n",
    "noun\tescalation\tincrease\tgrowth\trise\n",
    "noun\tescheatment\ttransfer of financial assets\n",
    "noun\tescrow\tbond\tdeed\n",
    "noun\tesign\telectronic signature\n",
    "noun\tevaluation\tassessment\tappraisal\tjudgement\n",
    "noun\texample\tinstance\n",
    "noun\texception\tbarring\tomission\n",
    "noun\texist\tlive\toccur\n",
    "noun\texperience\tmaturity\tknowledge\n",
    "noun\texpiration\ttermination\n",
    "noun\texplanation\tclarification\n",
    "noun\textension\tcontinuation\n",
    "noun\textensive\tthorough\n",
    "adj\texternal\textrinsic\toutside\n",
    "noun\tfacility\tconvenience\tease\n",
    "noun\tfamily\tkin\tbeneficiary\n",
    "noun\tfeature\tcharacteristic\n",
    "noun\tfee\tcharge\n",
    "noun\tfinancial\tfiscal\teconomic\n",
    "noun\tflag\tmarker\tindicator\n",
    "noun\tfluctuate\toscillate\twaver\n",
    "noun\tforce\teffort\n",
    "noun\tforeign\talien\toverseas\n",
    "noun\tforfeiture\tloss\n",
    "noun\tinstructore\tmentor\ttutor\n",
    "adj\tinternal\tintrinsic\tinside\n",
    "noun\tmobile\tcell\n",
    "noun\tmso\tmsonline\n",
    "noun\tpassword\tpin\tkey\tpasscode\tpasssword\tpassward\tpasswod\tpasswrd\n",
    "noun\tstatement\tstmnt\tstmt\n",
    "noun\tsupervisor\tdirector\n",
    "\n",
    "noun\t3D Advisor Desktop\t3D\n",
    "noun\tActive Assets Account\tAAA\n",
    "noun\tMorgan Stanley Active Assets California Tax Free Trust\tAACT\n",
    "noun\tMorgan Stanley Active Assets California Tax-Free Trust\tAACT\n",
    "noun\tMorgan Stanley Active Assets Tax Free Trust\tAAIF\n",
    "noun\tMorgan Stanley Active Assets Tax-Free Trust\tAAIF\n",
    "noun\tMorgan Stanley Active Assets Institutional Government Securities Trust\tAAIG\n",
    "noun\tNew York Municipal Money Market Trust\tAANY\n",
    "noun\tMorgan Stanley Active Assets Tax Free Trust\tAATF\n",
    "noun\tMorgan Stanley Active Assets Tax-Free Trust\tAATF\n",
    "noun\tAmerican Bankers Association\tABA\n",
    "noun\tAmerican Banking Association\tABA\n",
    "noun\tAutomated Customer Account Transfer\tACATS\n",
    "noun\tAccount Cross Reference\tACCR\n",
    "noun\tAutomated Clearing House\tACH\n",
    "noun\tAutomatic Cash Management\tACM\n",
    "noun\tAutomated Confirmation Transactions\tACT\n",
    "noun\tAdjustment\tADJ\n",
    "noun\tAutomatic Data Processing\tADP\n",
    "noun\tAmerican Depository Receipt\tADR\n",
    "noun\tAlternative Flexible Grid\tAFG\n",
    "noun\tAutomatic Funds Transfer\tAFT\n",
    "noun\tAlternative Investment\tAI\n",
    "noun\tAlternative Investments Advisory\tAIA\n",
    "noun\tAmerican Jobs Creation Act\tAJCA\n",
    "noun\tAccount Link Group\tALG\n",
    "noun\tAmerican Stock Exchange\tAMEX\n",
    "noun\tAdmin Wire System\tAMIN\n",
    "noun\tAnti-Money Laundering\tAML\n",
    "noun\tAccount Management Portal\tAMP\n",
    "noun\tAlternative Minimum Tax\tAMT\n",
    "noun\tAdvice of Correction or Cancellation\tAOCC\n",
    "noun\tAdvice of Correction/Cancellation\tAOCC\n",
    "noun\tAddress of Record\tAOR\n",
    "noun\tAsia Pacific regions\tAPJ\n",
    "noun\tAsia Pacific regions\tAPJJ\n",
    "noun\tArmy Post Office\tAPO\n",
    "noun\tActuarial Present Value\tAPV\n",
    "noun\tAdvanced Resources Center\tARC\n",
    "noun\tAuction Rate Preferred Securities\tARPS\n",
    "noun\tAccount Redistribution System\tARS\n",
    "noun\tAccount Surveillance and Fraud\tAS&F\n",
    "noun\tAutomated Stock Access Program\tASAP\n",
    "noun\tAutomatic Segregation\tASG\n",
    "noun\tApplication Service Provider\tASP\n",
    "noun\tAutomated Trade Adjustment\tATA\n",
    "noun\tAccount Type Change\tATC\n",
    "noun\tAutomatic Tracking Report Distribution\tATRD\n",
    "noun\tAdvisory Trade Violations\tATV\n",
    "noun\tAustralian Dollar\tAUD\n",
    "noun\tAssets Under Management\tAUM\n",
    "noun\tAutomated Required Minimum Distribution\tAuto-RMD\n",
    "noun\tAuxiliary Cross Reference Criteria\tAUX\n",
    "noun\tAuxiliary Cross Reference Criteria\tAuxOnUs\n",
    "noun\tAutomated Workflow Director\tAWD\n",
    "noun\tBuy and Hold\tB&H\n",
    "noun\tBenefit Access\tBA\n",
    "noun\tBenefit Access Funds Transfers\tBAFT\n",
    "noun\tBond Anticipation Note\tBAN\n",
    "noun\tBenefit Access Redesign\tBAR\n",
    "noun\tBranch Based Manual Stock Plan\tBBMSP\n",
    "noun\tBusiness Continuity Planning\tBCO\n",
    "noun\tBanking and Cash Services\tBCS\n",
    "noun\tBusiness Development Allowance\tBDA\n",
    "noun\tBank Deposit Program\tBDP\n",
    "noun\tBank Deposit Program Sweep\tBDPS\n",
    "noun\tBroker Dealer Questionnaire\tBDQ\n",
    "noun\tBroker-Dealer Questionnaire\tBDQ\n",
    "noun\tBranch Deposit Scanning\tBDS\n",
    "noun\tBoston Financial Dealer Services\tBFDS\n",
    "noun\tBank Identifier Code\tBIC\n",
    "noun\tBest Interest Contract\tBIC\n",
    "noun\tBroker Identification Number\tBIN\n",
    "noun\tBranch Inquiry Tracking System\tBITS\n",
    "noun\tBookkeeping and Stock Record\tBKSR\n",
    "noun\tBanking and Lending Support\tBLS\n",
    "noun\tBond Market Association\tBMA\n",
    "noun\tBranch Manager Dashboard\tBMD\n",
    "noun\tBranch Office Manager\tBOM\n",
    "noun\tBank of New York\tBONY\n",
    "noun\tBranch Office Support System\tBOSS\n",
    "noun\tBranch Office Terminal Application System\tBOTA\n",
    "noun\tBusiness Process Management\tBPM\n",
    "noun\tBasis Points\tBPS\n",
    "noun\tBrokerage Processing System\tBPS\n",
    "noun\t\tBPS Data Base System Advantage\n",
    "noun\tBusiness Remote Deposit\tBRD\n",
    "noun\tBusiness Requirement Document\tBRD\n",
    "noun\tBoston Stock Exchange\tBSE\n",
    "noun\tBrokerage Service Group\tBSG\n",
    "noun\tBusiness Service Manager\tBSM\n",
    "noun\tBroker Terminal System\tBTS\n",
    "noun\tBroker Transaction System\tBTS\n",
    "noun\tBroker Transaction System 010\tBTS\n",
    "noun\tBroker Transaction System - 232\tBTS 1\n",
    "noun\tBroker Transaction System 232\tBTS 1\n",
    "noun\tBusiness Unit\tBU\n",
    "noun\tBusiness Unit Indicator\tBUI\n",
    "noun\tCorporate Actions\tCA\n",
    "noun\tClient Advisory Center\tCAC\n",
    "noun\tCanadian Dollar\tCAD\n",
    "noun\tCustom Account Groups\tCAG\n",
    "noun\tCash Account Management System\tCAMS\n",
    "noun\tCorporate Actions Notifications and Response\tCANR\n",
    "noun\tCapital Accumulation Plan\tCAP\n",
    "noun\tClass Action Query\tCAQ\n",
    "noun\tCorporate Benefit Access\tCBA\n",
    "noun\tCost Benefit Analysis\tCBA\n",
    "noun\tComplex Business Development Manager\tCBDM\n",
    "noun\tChicago Board Option Exchange\tCBOE\n",
    "noun\tChicago Board of Trade\tCBT\n",
    "noun\tCost Center\tCC\n",
    "noun\tCall Center Branch Handled\tCCBH\n",
    "noun\tComputer to Computer Facility\tCCF\n",
    "noun\tCost Center Manager\tCCM\n",
    "noun\tClient Care Team\tCCT\n",
    "noun\tCertificate of Deposit\tCD\n",
    "noun\tClient Decisioning Module\tCDM\n",
    "noun\tCheck Deposit Operations\tCDO\n",
    "noun\tCollateralized Debt Obligation\tCDO\n",
    "noun\tContingent Deferred Sales Charge\tCDSC\n",
    "noun\tNominee Name for Depository Trust Company\tCEDE\n",
    "noun\tClosed End Funds\tCEF\n",
    "noun\tCost to Equity Ratio\tCER\n",
    "noun\tConsulting and Evaluation Service\tCES\n",
    "noun\tCorporate Equity Solutions\tCES\n",
    "noun\tCoverdell Education Savings Account\tCESA\n",
    "noun\tCommodities Future Trading Commission\tCFTC\n",
    "noun\tCalifornia Tax-Free Daily Income Trust\tCFTI\n",
    "noun\tConsulting Group\tCG\n",
    "noun\tADP Cage System\tCGA\n",
    "noun\tConsulting Group Advisor\tCGA\n",
    "noun\tConsulting Group Capital Markets\tCGCM\n",
    "noun\tCitigroup Global Market Inc\tCGMI\n",
    "noun\tCitigroup Global Market Inc.\tCGMI\n",
    "noun\tConsulting Group Proposal System\tCGPS\n",
    "noun\tClearing House Comparison\tCHC\n",
    "noun\tSwiss Franc\tCHF\n",
    "noun\tClient Interaction and Account Services\tCIAS\n",
    "noun\tCustomer Information Control System\tCICS\n",
    "noun\tCenter for Investment Excellence\tCIE\n",
    "noun\tForeign Securities Identifier\tCINS\n",
    "noun\tCustomer Identification Program\tCIP\n",
    "noun\tCard issuance update\tCIU\n",
    "noun\tConsumer Literature Fulfillment\tCLF\n",
    "noun\tCollateralized Mortgage Applications\tCMA\n",
    "noun\tCash Management Engagement\tCME\n",
    "noun\tCollateralized Mortgage Obligations\tCMO\n",
    "noun\tCage Management System\tCMS\n",
    "noun\tCoverage Management Tool\tCMT\n",
    "noun\tCross Trade\tCMU\n",
    "noun\tOffshore Chinese Renminbi\tCNH\n",
    "noun\tContinuous Net System\tCNS\n",
    "noun\tClose of Business\tCOB\n",
    "noun\tChange of Contact Information\tCOCI\n",
    "noun\tCash on Delivery\tCOD\n",
    "noun\tComputer Output to Laser Disk\tCOLD\n",
    "noun\tCertificate of Participation\tCOP\n",
    "noun\tCash on Receipt\tCOR\n",
    "noun\tCustom Portfolio\tCP\n",
    "noun\tCommodity Pool Operator\tCPO\n",
    "noun\tClient Profile System\tCPS\n",
    "noun\tCentral Processing Unit\tCPU\n",
    "noun\tClient Reporting and Control\tCRC\n",
    "noun\tCentral Registration Depository\tCRD\n",
    "noun\tCommunications Review Group\tCRG\n",
    "noun\tCollateral Risk Management\tCRM\n",
    "noun\tCustomer Relationship Management\tCRM\n",
    "noun\tChallenge Response Question\tCRQ\n",
    "noun\tClient Service Associate\tCSA\n",
    "noun\tConsulting Group Services\tCSG\n",
    "noun\tComplex Service Manager\tCSM\n",
    "noun\tCore Securities and Processing Services\tCSPS\n",
    "noun\tClient Service Representative\tCSR\n",
    "noun\tCustomer Service Representative\tCSR\n",
    "noun\tCashless Exercise\tCSX\n",
    "noun\tCommodity Trading Advisor\tCTA\n",
    "noun\tCommittee on Uniform Security Identification Procedures\tCUSIP\n",
    "noun\tClient Web Support\tCWS\n",
    "noun\tCzech Republic Koruna\tCZK\n",
    "noun\tDebit Account Activity\tDAA\n",
    "noun\tDiscretionary Account Activity\tDAA\n",
    "noun\tDefined Contribution Plans\tDCP\n",
    "noun\tDisqualifying Disposition\tDD\n",
    "noun\tDemand Deposit Account\tDDA\n",
    "noun\tDistribution Calculator\tDECI\n",
    "noun\tDocument Exception Process\tDEP\n",
    "noun\tDiscretionary Fee Waiver Allowance.\tDFWA\n",
    "noun\tDocuments in Process\tDIP\n",
    "noun\tDow Jones Industrial Average\tDJIA\n",
    "noun\tDanish Krone\tDKK\n",
    "noun\tData Network Services\tDNS\n",
    "noun\tDelivery Order\tDO\n",
    "noun\tDelegation of Duty\tDOD\n",
    "noun\tDays On Fail\tDOF\n",
    "noun\tDesignated Order Turnaround\tDOT\n",
    "noun\tDividend Reinvestment Program\tDRIP or DRP\n",
    "noun\tDividend Reinvestment Program\tDRP or DRIP\n",
    "noun\tDirect Registration System\tDRS\n",
    "noun\tDistrict Service Officer\tDSO\n",
    "noun\tDirected Share Program\tDSP\n",
    "noun\tDepository Trust Company\tDTC\n",
    "noun\tDepository Trust and Clearing Corporation\tDTCC\n",
    "noun\tDeficit Tracking System\tDTS\n",
    "noun\tDelivery vs Payment\tDVP\n",
    "noun\tDeposit Withdrawal at Custodian\tDWAC\n",
    "noun\tDollar Weighted Return\tDWR\n",
    "noun\tBuy and Hold\tE&H\n",
    "noun\tExercise and Hold\tE&H\n",
    "noun\tEmployee Benefit Trust\tEBT\n",
    "noun\tEquity Commission Discount\tECD\n",
    "noun\tExpress Credit Line\tECL\n",
    "noun\tEligible Contract Participants\tECP\n",
    "noun\tEnhanced due diligence\tEDD\n",
    "noun\tElectronic Delivery of Username and Password\tEDUP\n",
    "noun\tExecutive Financial Services\tEFS\n",
    "noun\tEconomic Growth and Tax Relief Act\tEGTRRA\n",
    "noun\tEurope Middle East and Africa\tEMEA\n",
    "noun\tEurope, Middle East, and Africa\tEMEA\n",
    "noun\tEvent Management Service Bureau\tEMSB\n",
    "noun\tEndorsement Missing\tEND\n",
    "noun\tEnd of Day\tEOD\n",
    "noun\tEnd of Life\tEOL\n",
    "noun\tExternal Processing Code\tEPC\n",
    "noun\tElectronic Pool Notification\tEPN\n",
    "noun\tEntitlements Provisioning and Reporting\tEPR\n",
    "noun\tEnhancement Request Form\tERF\n",
    "noun\tEmployment Retirement Income Security Act\tERISA\n",
    "noun\tEmployee Stock Option Plan\tESOP\n",
    "noun\tEmployee Stock Purchase Plan\tESPP\n",
    "noun\tElectronic Trade Confirmation\tETC\n",
    "noun\tExchange Traded Funds\tETF\n",
    "noun\tEnd User Computing\tEUC\n",
    "noun\tEuro\tEUR\n",
    "noun\tExercise and Hold\tEXH\n",
    "noun\tFinancial Advisor\tFA\n",
    "noun\tForeign Account Control Group\tFACG\n",
    "noun\tFiduciary Asset Management\tFAM\n",
    "noun\tFinancial Advisor Marketing Center\tFAMC\n",
    "noun\tFormer Advisor Program\tFAP\n",
    "noun\tFinancial Advisor Revenue Sponsor\tFARS\n",
    "noun\tFast Automatic Stock Transfer\tFAST\n",
    "noun\tForeign Account Tax Compliance Act\tFATCA\n",
    "noun\tFinancial Consultant\tFC\n",
    "noun\tFinancial Control Group\tFCG\n",
    "noun\tFirst Data Corporation\tFDC\n",
    "noun\tFederal Deposit Insurance Corporation\tFDIC\n",
    "noun\tFirst Data Resource\tFDR\n",
    "noun\tField Employee Notification and Information Exchange\tFENIX\n",
    "noun\tFront End Sales Charge\tFESC\n",
    "noun\tFirst Fund Distributors Inc.\tFFDI\n",
    "noun\tForeign Financial Institution\tFFI\n",
    "noun\tFederal Home Loan Mortgage Corp\tFHLMC\n",
    "noun\tFixed income\tFI\n",
    "noun\tFirst In First Out\tFIFO\n",
    "noun\tFinancial Industry Number\tFIN\n",
    "noun\tFinancial Industry Regulatory Authority\tFINRA\n",
    "noun\tFinancial Management Account\tFMA\n",
    "noun\tFair Market Value\tFMV\n",
    "noun\tFill or Kill\tFOK\n",
    "noun\tFederal Open Market Committee\tFOMC\n",
    "noun\tFleet Post Office\tFPO\n",
    "noun\tFiduciary Services\tFS\n",
    "noun\tField Services Group\tFSG\n",
    "noun\tFund Solution Account\tFSTA\n",
    "noun\tField Services Unit\tFSU\n",
    "noun\tFull Time Employee\tFTE\n",
    "noun\tFile Transfer Infrastructure\tFTI\n",
    "noun\tFile Transfer Protocol\tFTP\n",
    "noun\tFunds Transfer System\tFTS\n",
    "noun\tFunds Processing\tFUN\n",
    "noun\tForeign Exchange\tFX\n",
    "noun\tFor Your Information\tFYI\n",
    "noun\tGreat British Pence\tGBP\n",
    "noun\tGreat British Pounds\tGBP\n",
    "noun\tGoals Based Wealth Management\tGBWM\n",
    "noun\tGeneration and generation of files\tGEN\n",
    "noun\tGeneration and/or generation of files\tGEN\n",
    "noun\tGeneration or generation of files\tGEN\n",
    "noun\tGlobal Investment Committee\tGIC\n",
    "noun\tGlobal Identification\tGID\n",
    "noun\tGovernment National Mortgage Association\tGNMA\n",
    "noun\tGeneral Obligation\tGO\n",
    "noun\tGroup Policy Object\tGPO\n",
    "noun\tGovernment Securities Clearing Corporation\tGSCC\n",
    "noun\tGlobal Stock Plan Services\tGSPS\n",
    "noun\tGood-Til-Canceled\tGTC\n",
    "noun\tGraphical User Interface\tGUI\n",
    "noun\tGlobal Wealth Management Group\tGWMG\n",
    "noun\tHome Equity Line of Credit\tHELOC\n",
    "noun\tHousehold\tHH\n",
    "noun\tHouseholding\tHH\n",
    "noun\tHong Kong Dollar\tHKD\n",
    "noun\tBuy and Hold\tHLD\n",
    "noun\tHigh net worth\tHNW\n",
    "noun\tHigh Risk Jurisdiction\tHRJ\n",
    "noun\tHigh Risk Transfer\tHRT\n",
    "noun\tHousehold Transfer Application\tHTA\n",
    "noun\tInternational Account Control\tIAC\n",
    "noun\tInternational Bank Account Number\tIBAN\n",
    "noun\tInvestment Banking Division\tIBD\n",
    "noun\tImage Cash Letter\tICL\n",
    "noun\tInvestment Consulting Services\tICS\n",
    "noun\tInstitutional Delivery\tID\n",
    "noun\tMorgan Stanley IDEAS\tIDEAS\n",
    "noun\tInteractive Data Services Incorporated\tIDSI\n",
    "noun\tItems for Attention\tIFA\n",
    "noun\tIndian Financial Systems Code\tIFSC\n",
    "noun\tIndividual Investor's Group\tIIG\n",
    "noun\tIsraeli Shekel\tILS\n",
    "noun\tInstructor led Training\tILT\n",
    "noun\tInstructor-led Training\tILT\n",
    "noun\tInstructorled Training\tILT\n",
    "noun\tInstitutional Money Markets\tIMM\n",
    "noun\tInventory Management System\tIMS\n",
    "noun\tInvestment Management Services\tIMS\n",
    "noun\tInternational Individual Account\tIND\n",
    "noun\tImmediate or Cancel\tIOC\n",
    "noun\tIndication of Interests\tIOI\n",
    "noun\tInterested Party User\tIP user\n",
    "noun\tInitial Public Offering\tIPO\n",
    "noun\tIntegrated Payment System\tIPS\n",
    "noun\tInitial Price Thought\tIPT\n",
    "noun\tIndividual Retirement Account\tIRA\n",
    "noun\tImage Replacement Document\tIRD\n",
    "noun\tImage Replacement Documents\tIRD\n",
    "noun\tInterest Rate Management Application\tIRMA\n",
    "noun\tItem Reference Number\tIRN\n",
    "noun\tInitial rate of Return\tIRR\n",
    "noun\tInternal Revenue Service\tIRS\n",
    "noun\tInvestment Services Fee\tISF\n",
    "noun\tInstitutional Services Group\tISG\n",
    "noun\tInternational Security Identification Number\tISIN\n",
    "noun\tIncentive Stock Option\tISO\n",
    "noun\tInternational Stock Plan Service\tISPS\n",
    "noun\tInternational Stock Purchase Plan\tISPS\n",
    "noun\tInternational Toll Free\tITF\n",
    "noun\tInternational Toll Free Telephone Service\tITF\n",
    "noun\tInternational Toll-Free\tITF\n",
    "noun\tInternational Toll-Free Telephone Service\tITF\n",
    "noun\tMorgan Stanley Tax-Free Daily Income Trust\tITFI\n",
    "noun\tInteractive Voice Response\tIVR\n",
    "noun\tInvest with Rewards\tIwR\n",
    "noun\tJournal Automated Workstation System\tJAWS\n",
    "noun\tJob Control Language\tJCL\n",
    "noun\tJoint Production Number\tJPN\n",
    "noun\tJapanese Yen\tJPY\n",
    "noun\tJoint Tenant with Rights of Survivorship\tJRS\n",
    "noun\tJoint Tenants\tJT\n",
    "noun\tJoint Tenants in Common\tJT-TIC\n",
    "noun\tJoint Tenants With Rights of Survivorship\tJT-WROS\n",
    "noun\tJoint Tenants\tJTTEN\n",
    "noun\tKnowledge Based Authentication\tKBA\n",
    "noun\tKey Performance Indicator\tKPI\n",
    "noun\tKnow Your Customer\tKYC\n",
    "noun\tLiquidity Access Line\tLAL\n",
    "noun\tLocal Area Network\tLAN\n",
    "noun\tLegal and Compliance Division\tLCD\n",
    "noun\tLightweight Directory Access Protocol\tLDAP\n",
    "noun\tLiving Disaster Recovery Planning System\tLDRPS\n",
    "noun\tLondon Interbank Offered Rate\tLIBOR\n",
    "noun\tLast In First Out\tLIFO\n",
    "noun\tLimited Liability Company\tLLC\n",
    "noun\tLeave of Absence\tLOA\n",
    "noun\tLetter of Authorization\tLOA\n",
    "noun\tLiquidation of Assets\tLOA\n",
    "noun\tLetter of Intent\tLOI\n",
    "noun\tLimited Purpose Account\tLPA\n",
    "noun\tLocal Plan Administrator\tLPA\n",
    "noun\tLegacy Smith Barney\tLSB\n",
    "noun\tLocal Signature Page\tLSP\n",
    "noun\tLimited Trading Authority\tLTA\n",
    "noun\tLong Term Incentive Plan\tLTIP\n",
    "noun\tLogical Unit\tLU\n",
    "noun\tMergers and Acquisitions\tM&A\n",
    "noun\tMaster Account View\tMAC\n",
    "noun\tMoving Average Convergence/Divergence\tMACD\n",
    "noun\tMobility Autonomous Driving and Electrificatio\tMADE\n",
    "noun\tShared Mobility Autonomous Driving and Electrificatio\tMADE\n",
    "noun\tManaged Advisory Portfolio Solutions\tMAPS\n",
    "noun\tMorgan Stanley Application Process\tMAPS\n",
    "noun\tMortgage Backed Securities\tMBS\n",
    "noun\tMortgage Backed Securities Clearing Corporation\tMBSCC\n",
    "noun\tMainframe Dual Host\tMDH\n",
    "noun\tMiscellaneous Delivery Order\tMDO\n",
    "noun\tMulti-Dimension System\tMDS\n",
    "noun\tMutual Fund Institutional Money\tMFIM\n",
    "noun\tMutual Fund Master Security Description\tMFMSD\n",
    "noun\tMutual Funds Operations\tMFO\n",
    "noun\tMutual funds order entry\tMFOE\n",
    "noun\tMutual Funds Routing System\tMFRS\n",
    "noun\tMagnetic Ink Character Recognition\tMICR\n",
    "noun\tManagement Information Systems\tMIS\n",
    "noun\tMoney Market Fund\tMMF\n",
    "noun\tMaterial Non-public Information\tMNPI\n",
    "noun\tMexican Peso\tMNX\n",
    "noun\tMarket on Close\tMOC\n",
    "noun\tMulti Product Pattern Alert\tMPPA\n",
    "noun\tMobile Remote Deposit Capture\tMRDC\n",
    "noun\tMargin New Customer Account Inquiry\tMRGI\n",
    "noun\tMargin Customer Account Inquiry\tMRGN\n",
    "noun\tMessage Routing System\tMRS\n",
    "noun\tMorgan Stanley\tMS\n",
    "noun\tMorgan Stanley Active Directory\tMSAD\n",
    "noun\tMaster Security Description\tMSD\n",
    "noun\tMSD Online Inquiry System\tMSDI\n",
    "noun\tMSD Online Inquiry Only\tMSDQ\n",
    "noun\tMorgan Stanley Online\tMSO\n",
    "noun\tMorgan Stanley Private Bank, National Association\tMSPBNA\n",
    "noun\tMunicipal Securities Rulemaking Board\tMSRB\n",
    "noun\tManual Work Items\tMWI\n",
    "noun\tNational Automated Clearing House Association\tNACHA\n",
    "noun\tName and Address Inquiry\tNANQ\n",
    "noun\tNew Account Opening Document Processing\tNAODP\n",
    "noun\tNational Association of Securities Dealers\tNASD\n",
    "noun\tNational Association of Securities Dealers\tNASD \n",
    "noun\tNational Association of Securities Dealers Automated Quotation\tNASDAQ\n",
    "noun\tNet Asset Value\tNAV\n",
    "noun\tNational Best Bid and Offer\tNBBO\n",
    "noun\tNational Branch Services\tNBS\n",
    "noun\tNational Credit Union Administration\tNCUA\n",
    "noun\tNon-Discretionary Account\tNDA\n",
    "noun\tNondiscretionary Account\tNDA\n",
    "noun\tOpen a New Batch\tNEWB\n",
    "noun\tNational Insurance Producer Registry\tNIPR\n",
    "noun\tNational Market Security\tNMS\n",
    "noun\tNational New Accounts\tNNA\n",
    "noun\tNotice of Order Execution\tNOE\n",
    "noun\tNorwegian Krone\tNOK\n",
    "noun\tEveryone except North and South America\tNon-NSA\n",
    "noun\tNon Stock Plan Services\tNon-SPS\n",
    "noun\tNon-Stock Plan Services\tNon-SPS\n",
    "noun\tNon-Profit Organization\tNPO\n",
    "noun\tNet Present Value\tNPV\n",
    "noun\tNon-Qualified Stock Option\tNQ\n",
    "noun\tNonqualified Stock Option\tNQ\n",
    "noun\tNon Qualified Stock Option\tNQSO\n",
    "noun\tNon-Qualified Stock Option\tNQSO\n",
    "noun\tNonqualified Stock Option\tNQSO\n",
    "noun\tNon-Resident Alien\tNRA\n",
    "noun\tNonresident Alien\tNRA\n",
    "noun\tNational Registration System\tNRS\n",
    "noun\tNorth and South America\tNSA\n",
    "noun\tNational Securities Clearing Corporation\tNSCC\n",
    "noun\tNon Sufficient Funds\tNSF\n",
    "noun\tNon-Sufficient Funds\tNSF\n",
    "noun\tNo Transaction Fee\tNTF\n",
    "noun\tNew York Municipal Money Market\tNTFI\n",
    "noun\tNon Traditional Investments\tNTI\n",
    "noun\tNon transferrable security\tNTS\n",
    "noun\tNon-transferrable security\tNTS\n",
    "noun\tNontransferrable security\tNTS\n",
    "noun\tNon Transferable Securities System\tNTSS\n",
    "noun\tNon-Transferable Securities System\tNTSS\n",
    "noun\tNonTransferable Securities System\tNTSS\n",
    "noun\tNew York Futures Exchange\tNYFE\n",
    "noun\tNew York Stock Exchange\tNYSE\n",
    "noun\tNew Zealand Dollar\tNZD\n",
    "noun\tOutside Business Interest\tOBI\n",
    "noun\tOffice of the Comptroller of the Currency\tOCC\n",
    "noun\tOptions Clearing Corporation\tOCC\n",
    "noun\tOrder Entry\tOE\n",
    "noun\tOperations Expenditure Review Committee\tOERC\n",
    "noun\tOffice of Foreign Assets Control\tOFAC\n",
    "noun\tOpen Financial Exchange\tOFX\n",
    "noun\tOwner Generated Activity\tOGA\n",
    "noun\tOriginal Issue Discount\tOID\n",
    "noun\tOnline Award Acceptance\tOLAA\n",
    "noun\tOnline Grant Acceptance\tOLGA\n",
    "noun\tOnline Trading Services\tOLTS\n",
    "noun\tOrder Matching File\tOMF\n",
    "noun\tOrder Matching System\tOMS\n",
    "noun\tOrdinary Share\tORD\n",
    "noun\tOver the Counter\tOTC\n",
    "noun\tOver the Counter Market\tOTC\n",
    "noun\tOver-the-Counter Market\tOTC\n",
    "noun\tOne Time Passcode\tOTP\n",
    "noun\tOffice of Thrift Supervision\tOTS\n",
    "noun\tPurchase and Sale\tP&S\n",
    "noun\tProcess Administrator\tPA\n",
    "noun\tProfessional Alliance Group\tPAG\n",
    "noun\tPeriodic Accumulated Income Distribution\tPAID\n",
    "noun\tPending Account Number\tPAN\n",
    "noun\tPortfolio Advisory Services\tPAS\n",
    "noun\tPortfolio Architect Accounts\tPATA\n",
    "noun\tPrivate Banking Group\tPBG\n",
    "noun\tPersonal Computer\tPC\n",
    "noun\tPublic Complaints Commission\tPCC\n",
    "noun\tPremier Cash Management\tPCM\n",
    "noun\tPolitical Contributions Tracking\tPCT\n",
    "noun\tPortable Document Format\tPDF\n",
    "noun\tPreset Diversification Program\tPDP\n",
    "noun\tProgram Development Exchange\tPDX\n",
    "noun\tPolicy Exceptions and Personal Equipment system\tPEPE\n",
    "noun\tPersonal Environment Trading\tPET\n",
    "noun\tPersonal Financial Management\tPFM\n",
    "noun\tProduct Group Services\tPGS\n",
    "noun\tPhiladelphia Stock Exchange\tPHLX\n",
    "noun\tPersonally Identifiable Information\tPII\n",
    "noun\tPayment in Kind\tPIK\n",
    "noun\tPersonal Identification Number\tPIN\n",
    "noun\tPortfolio Management\tPM\n",
    "noun\tProject Management Office\tPMO\n",
    "noun\tPower of Attorney\tPOA\n",
    "noun\tPayment on Delivery\tPOD\n",
    "noun\tPension Protection Act\tPPA\n",
    "noun\tPrivate Placement Life Insurance\tPPLI\n",
    "noun\tPeriodic Payment System\tPPS\n",
    "noun\tUnacceptable use for Participant\tPPT\n",
    "noun\tPrivate Placement Variable Annuity\tPPVA\n",
    "noun\tPlastic Same Day\tPSC\n",
    "noun\tP&S Input Pending Trades\tPSIPT\n",
    "noun\tPurchase and Sales\tPSO\n",
    "noun\tPublic Switched Telephone Network\tPSTN\n",
    "noun\tParticipant's Trade Corporation\tPTC\n",
    "noun\tPortfolio Performance System\tPTF\n",
    "noun\tPlatform Transformation Initiative\tPTI\n",
    "noun\tPrincipal Trade Review\tPTR\n",
    "noun\tPersonal Unique Identification\tPUID\n",
    "noun\tPrinciple Value\tPV\n",
    "noun\tPrivate Wealth Advisor\tPWA\n",
    "noun\tPower Word Fortitude\tPWF\n",
    "noun\tPrivate Wealth Management\tPWM\n",
    "noun\tQuality Control Packet\tQCP\n",
    "noun\tQualifying Disposition\tQD\n",
    "noun\tQualified Domestic Relations Order\tQDRO\n",
    "noun\tQuarterly Performance Reports\tQPR\n",
    "noun\tQuick Reference Card\tQRC\n",
    "noun\tQualified Retirement Plan\tQRP\n",
    "noun\tQuality Service Approver\tQSA\n",
    "noun\tQualified Supervisory Delegate\tQSD\n",
    "noun\tQuestionable Trade\tQT\n",
    "noun\tRemote Access Control Facility ID\tRACF ID\n",
    "noun\tRevenue Anticipation Note\tRAN\n",
    "noun\tRoyal Bank of Canada\tRBC\n",
    "noun\tRequired Beginning Date\tRBD\n",
    "noun\tReceive Balance Order\tRBO\n",
    "noun\tRequired Death Distribution\tRDD\n",
    "noun\tRetirement Distribution System\tRDS\n",
    "noun\tRegistration ID\tReg\n",
    "noun\tRegistration ID\tReg ID\n",
    "noun\tReorganization Inquiry\tREIQ\n",
    "noun\tReal Estate Investment Trust\tREIT\n",
    "noun\tReal Estate Mortgage Investment Conduit\tREMIC\n",
    "noun\tRegistered Investment Advisor\tRIA\n",
    "noun\tRegulated Investment Company\tRIC\n",
    "noun\tRetail Inventory Management\tRIM\n",
    "noun\tReduced Instruction Set Computer\tRISC\n",
    "noun\tReturned Item Unit\tRIU\n",
    "noun\tRemote\tRJE\n",
    "noun\tRequired Minimum Distribution\tRMD\n",
    "noun\tRequired Minimum Distributions\tRMD\n",
    "noun\tReport Management Distribution System\tRMDS\n",
    "noun\tRegistered non negotiable\tRNN\n",
    "noun\tRegistered non-negotiable\tRNN\n",
    "noun\tRegistered nonnegotiable\tRNN\n",
    "noun\tRights of Accumulation\tROA\n",
    "noun\tRegistered Options Principal\tROP\n",
    "noun\tRate of Return\tROR\n",
    "noun\tRejected Option Trade Notice\tROTN\n",
    "noun\tRest of World\tROW\n",
    "noun\tRetirement Plan Manager\tRPM\n",
    "noun\tRemittance Processing Pipeline\tRPP\n",
    "noun\tRegistered Representative\tRR\n",
    "noun\tRegional Risk Officer\tRRO\n",
    "noun\tRisk Self Assessment\tRSA\n",
    "noun\tRisk Self-Assessment\tRSA\n",
    "noun\tRisk Self Assessment Adaptive Authentication\tRSAAA\n",
    "noun\tRisk Self-Assessment Adaptive Authentication\tRSAAA\n",
    "noun\tRelative Strength Index\tRSI\n",
    "noun\tRestricted Stock Shares\tRSS\n",
    "noun\tRetirement Support Services\tRSS\n",
    "noun\tRestricted Stock Unit\tRSU\n",
    "noun\tReal Time Accounting\tRTA\n",
    "noun\tReal Time Alerts\tRTA\n",
    "noun\tReal-Time Accounting\tRTA\n",
    "noun\tRealtime Accounting\tRTA\n",
    "noun\tRetirement Transfer Type\tRTT\n",
    "noun\tRetirement Transfer Tracking\tRXT\n",
    "noun\tSettlement Date\tS/D\n",
    "noun\tSales Assistant\tSA\n",
    "noun\tSan Antonio Branch Handled\tSABH\n",
    "noun\tSingle Advisory Contract\tSAC\n",
    "noun\tState Administration of Foreign Exchange\tSAFE\n",
    "noun\tStock Administration Plan Processing Handling Inquiry Reporting Execution\tSAPPHIRE\n",
    "noun\tStock Appreciation Right\tSAR\n",
    "noun\tSalary Reduction Simplified Employee Pension\tSAR-SEP\n",
    "noun\tSan Antonio Contact Center\tSAT\n",
    "noun\tSecurities Based Lending\tSBL\n",
    "noun\tStock Clearing Corporation of Philadelphia\tSCCP\n",
    "noun\tStrategic Client Reporting\tSCR\n",
    "noun\tSame Day Funds Settlement\tSDFS\n",
    "noun\tSystems Development Life Cycle\tSDLC\n",
    "noun\tService Delivery Manager\tSDM\n",
    "noun\tSelf Directed Retirement Account\tSDRA\n",
    "noun\tSecurities and Exchange Commission\tSEC\n",
    "noun\tSecurity Architecture\tSecArch\n",
    "noun\tSwedish Krona\tSEK\n",
    "noun\tSecurities Exchange Medallion Program\tSEMP\n",
    "noun\tSimplified Employee Pension\tSEP\n",
    "noun\tSaving for College\tSFC\n",
    "noun\tSingapore Dollar\tSGD\n",
    "noun\tShareholder Accounting\tSHAC\n",
    "noun\tSecurities Industry Association\tSIA\n",
    "noun\tSecurities Industry Automation Corporation\tSIAC\n",
    "noun\tSecurities Industry Automation Corporation Tomorrow\tSIAT\n",
    "noun\tSecurities Information Center\tSIC\n",
    "noun\tStanding Instructions Database\tSID\n",
    "noun\tStock Record History\tSINH\n",
    "noun\tStock Record Inquiry\tSINR\n",
    "noun\tSystematic Investment Plan\tSIP\n",
    "noun\tSecurities Investor Protection Corporation\tSIPC\n",
    "noun\tService Level Agreement\tSLA\n",
    "noun\tStatement Linked Group\tSLG\n",
    "noun\tSeparately Managed Account\tSMA\n",
    "noun\tSeparately Managed Accounts\tSMA\n",
    "noun\tSpecial Memorandum Account\tSMA\n",
    "noun\tSystem Modification Program/Extended\tSMP/E\n",
    "noun\tSign Off from Security System\tSNOF\n",
    "noun\tSign On to Security System\tSNON\n",
    "noun\tSegregation of Duties\tSOD\n",
    "noun\tStock Option Employee ID\tSOE ID\n",
    "noun\tStock Option System\tSOPT\n",
    "noun\tSystem of Record\tSOR\n",
    "noun\tService Professional\tSP\n",
    "noun\tSignature Page\tSP\n",
    "noun\tSecurity Processing Application\tSPA\n",
    "noun\tScratch Pad\tSPAD\n",
    "noun\tStock Plan Connect\tSPC\n",
    "noun\tStockPlan Connect\tSPC\n",
    "noun\tSpecial Instruction Code\tSPIN\n",
    "noun\tSpecial Payment Order\tSPO\n",
    "noun\tSpecial Products Payment System\tSPPS\n",
    "noun\tSenior Complex Risk Officer\tSr. CRO\n",
    "noun\tState Registration Expense Platform\tSREP\n",
    "noun\tService Request Inquiry\tSRI\n",
    "noun\tSenior Registered Options Principal\tSROP\n",
    "noun\tSystem Required Specification\tSRS\n",
    "noun\tService Review Unit\tSRU\n",
    "noun\tStock Settled SAR\tSSAR\n",
    "noun\tStock-Settled SAR\tSSAR\n",
    "noun\tSocial Security Number\tSSN\n",
    "noun\tSingle Sign On\tSSO\n",
    "noun\tSystem Service Request\tSSR\n",
    "noun\tSettled stock Appreciation Rights\tSSRs\n",
    "noun\tSecurities Transfer Association Medallion Program\tSTAMP\n",
    "noun\tSell To Cover\tSTC\n",
    "noun\tState Tuition Plan\tSTP\n",
    "noun\tStraight Through Processing\tSTP\n",
    "noun\tSeparate Trading of Registered Interest and Principal Securities\tSTRIPS\n",
    "noun\tShare Value Plan\tSVP\n",
    "noun\tSaved Wire Beneficiary\tSWB\n",
    "noun\tSociety for Worldwide Interbank Financial Telecommunication\tSWIFT\n",
    "noun\tSystematic Withdrawal Investment Plan\tSWIP\n",
    "noun\tSystematic Withdrawal Plan\tSWP\n",
    "noun\tTreasures or Trading Day\tT\n",
    "noun\tTravel and Expense\tT&E\n",
    "noun\tTrade Date Plus Two Business Days\tT+2\n",
    "noun\tTrade Date\tT/D\n",
    "noun\tTrailing twelve months\tT12\n",
    "noun\tTax Anticipation Note\tTAN\n",
    "noun\tTrade Activity Report\tTAR\n",
    "noun\tN/A\tTASQ\n",
    "noun\tTransfer Control Protocol Internet Protocol\tTCP/IP\n",
    "noun\tTransfer Control Protocol/Internet Protocol\tTCP/IP\n",
    "noun\tTask Checklist Questions\tTCQ\n",
    "noun\tTaxpayer Equity Fiscal Responsibility Act\tTEFRA\n",
    "noun\tTERM ID Inquiry\tTERM\n",
    "noun\tTape Input File\tTIF\n",
    "noun\tTransfer Initiation Form\tTIF\n",
    "noun\tTax Identification Number\tTIN\n",
    "noun\tTransition Management Services\tTMS\n",
    "noun\tTransfer of Account\tTOA\n",
    "noun\tTransfer on Death\tTOD\n",
    "noun\tTop Operating Model\tTOM\n",
    "noun\tTargeted Percentage of Volume\tTPOV\n",
    "noun\tThird Party Verbal\tTPV\n",
    "noun\tTax Reform Act\tTRA\n",
    "noun\tReport Directory\tTRAC\n",
    "noun\tTravel Rule Indicator\tTRI\n",
    "noun\tTransaction Identifier\tTRID\n",
    "noun\tTime Weighted Return\tTWR\n",
    "noun\tUniform Books and Records\tUBR\n",
    "noun\tUncollected Funds\tUCF\n",
    "noun\tUser Defined\tUD\n",
    "noun\tUniform Gift to Minors Act\tUGMA\n",
    "noun\tUltra High Net Worth\tUHNW\n",
    "noun\tUniversal International Free phone Number\tUIFN\n",
    "noun\tUnit Investment Trust\tUIT\n",
    "noun\tUnified Managed Account\tUMA\n",
    "noun\tUnited States Dollar\tUSD\n",
    "noun\tUniform Transfers to Minors Act\tUTMA\n",
    "noun\tUser Experience\tUX\n",
    "noun\tVirtual Branch Office\tVBO\n",
    "noun\tVerbal Distribution Authentication\tVDA\n",
    "noun\tVoluntary Employees' Beneficiary Association Trust\tVEBA\n",
    "noun\tVersatile Investment Program\tVIP\n",
    "noun\tVoice Response Unit\tVRU\n",
    "noun\tVirginia State College Plan\tVSCP\n",
    "noun\tVariable Standing Instruction\tVSI\n",
    "noun\tSales vs. Purchase\tVSP\n",
    "noun\tVendor Support Systems\tVSS\n",
    "noun\tVirtual Training Team\tVTT\n",
    "noun\tVariable Weighted Average Price\tVWAP\n",
    "noun\tWealth Advisory Associate\tWAA\n",
    "noun\tWide Area Network\tWAN\n",
    "noun\tWorldLink Check\tWC\n",
    "noun\tWorkflow Dashboard\tWFD\n",
    "noun\tWidely Held Fixed Investment Trusts\tWHFIT\n",
    "noun\tWhen Issued\tWI\n",
    "noun\tWorldLink\tWL\n",
    "noun\tWritten Supervisory Procedure\tWSP\n",
    "noun\tWithhold to Cover\tWTC\n",
    "noun\tWorldLink Wires\tWW\n",
    "noun\tCross Reference\tXREF\n",
    "noun\tPatriot Act Entity\tPAE\n",
    "noun\tPatriot Act Address\tPAA\n",
    "noun\tUnified Estate\tUE\n",
    "noun\tUnified Estate\tu.e\n",
    "noun\tct-w4p\tCT\n",
    "noun\tAccount Opening\tA.O\n",
    "noun\tAccount Opening\tAO\n",
    "noun\t3210 alert\t3210s\n",
    "noun\t3210 alert\tfina\n",
    "noun\t3210 alert\tfinra\n",
    "noun\t3d\t32d\n",
    "noun\t3d\t3dao\n",
    "noun\t3d\tin3d\n",
    "noun\t3d\tpti\n",
    "noun\t3d\ttpi\n",
    "noun\t401k account\t401\n",
    "noun\t401k account\t401a\n",
    "noun\t401k account\t401k\n",
    "noun\t401k account\t401kin\n",
    "noun\t401k account\t401kplan\n",
    "noun\t401k account\t401l\n",
    "noun\t403b account\t403\n",
    "noun\t403b account\t403b\n",
    "noun\t403b account\tirb\n",
    "noun\t407 approval letter\t407\n",
    "noun\t457f account\t457\n",
    "noun\t457f account\t457f\n",
    "noun\t529 account\t529\n",
    "noun\t529 account\t529s\n",
    "noun\t529 account\tin529\n",
    "noun\t529 college savings plan application and client agreement\t529app\n",
    "noun\t529 saving for college account\tsfc\n",
    "noun\t529 state tuition plan account\tstp\n",
    "noun\t72t distribution\t72t\n",
    "noun\t72t distribution\t72td\n",
    "noun\t72t distribution\t72ts\n",
    "noun\tacat cancellation form\tatsacatc\n",
    "noun\taccount application and client agreement\taaca\n",
    "noun\taccount inception\tincept\n",
    "noun\taccount inception\tincepted\n",
    "noun\taccount inception\tinception\n",
    "noun\taccount inception and update tool\taiut\n",
    "noun\taccount linking service\tlinking\n",
    "noun\taccount management portal\tamp\n",
    "noun\taccount name\tname\n",
    "noun\taccount number\tacct#\n",
    "noun\taccount number\tnumber\n",
    "noun\taccount plating\teplating\n",
    "noun\taccount plating\tpate\n",
    "noun\taccount plating\tplaiting\n",
    "noun\taccount plating\tplaitng\n",
    "noun\taccount plating\tplat\n",
    "noun\taccount plating\tplate\n",
    "noun\taccount plating\tplated\n",
    "noun\taccount plating\tplatin\n",
    "noun\taccount plating\tplating\n",
    "noun\taccount plating\tplatings\n",
    "noun\taccount plating\tplatting\n",
    "noun\taccount statement\tstatement\n",
    "noun\taccount statement\tstatements\n",
    "noun\taccount swing\tswing\n",
    "noun\taccount transfer form\tacats\n",
    "noun\taccount transfer form\tatsatff\n",
    "noun\taccount type change\taatc\n",
    "noun\taccount type change\tact\n",
    "noun\taccount type change\tatc\n",
    "noun\taccount type change\tchange\n",
    "noun\taccount type change\tchanged\n",
    "noun\taccount type change\tchanging\n",
    "noun\taccount type change\tconvert\n",
    "noun\taccount type change\tconverting\n",
    "noun\taccount type change\tmake\n",
    "noun\taccount type change\tnatc\n",
    "noun\taccount type change\ttransfer\n",
    "noun\taccount type change\ttransition\n",
    "noun\taccount type change\tturn\n",
    "noun\taccount type change\tupdate\n",
    "noun\taccount type change\txfr\n",
    "noun\tacknowledgement by person employed by a broker-dealer finra or an sro letter\tsro\n",
    "noun\taddress\taddres\n",
    "noun\taddress\ttheaddress\n",
    "noun\taffidavit of domicile and debts\taod\n",
    "noun\taffidavit of domicile and debts\tavd\n",
    "noun\taffidavit of domicile and debts\tdomicile\n",
    "noun\talert\talet\n",
    "noun\talert\telrt\n",
    "noun\talert\tmalert\n",
    "noun\talternative investments advisory account\taia\n",
    "noun\tamortization\tamortizing\n",
    "noun\tamortization\tamotirization\n",
    "noun\tannuity\tannu9ity\n",
    "noun\tannuity\tannuities\n",
    "noun\tannuity\tannuityy\n",
    "noun\tanti money laundering program\taml\n",
    "noun\tauthorized individual\ta.i\n",
    "noun\tauthorized individual\tais\n",
    "noun\tauthorized individual\tallai\n",
    "noun\tauthorized individual\tauth\n",
    "noun\tauthorized individual\tauthorized\n",
    "noun\tauthorized individual\tsigner\n",
    "noun\tauthorized individual\tsigners\n",
    "noun\tauthorized individual\tsignor\n",
    "noun\tauthorized persons and enabling resolutions for corporations form\tcaa\n",
    "noun\tautomated clearing house transaction\tach\n",
    "noun\tautomated customer account transfer\tacat\n",
    "noun\tautomated customer account transfer\tacated\n",
    "noun\tautomated customer account transfer\tacating\n",
    "noun\tautomated customer account transfer\tacats\n",
    "noun\tautomated customer account transfer\tachat\n",
    "noun\tautomated required minimum distribution\tautormd\n",
    "noun\tautomated required minimum distribution\tdrma\n",
    "noun\tautomatic sweep\tsweep\n",
    "noun\tautomatic sweep\tsweeping\n",
    "noun\tautomatic sweep\tswept\n",
    "noun\tb notice\tbnotice\n",
    "noun\tbackup withholding\twithholding\n",
    "noun\tbalance\tbalances\n",
    "noun\tbank deposit program\tbdp\n",
    "noun\tbank middle market custody account\tbfc\n",
    "noun\tbank middle market dvp account\tbdc\n",
    "noun\tbanking service\tbanking\n",
    "noun\tbasic securities account\tbsa\n",
    "noun\tbeneficial owner\tb.o.\n",
    "noun\tbeneficial owner\tbeneowner\n",
    "noun\tbeneficial owner\tbos\n",
    "noun\tbeneficial owner certification\tboc\n",
    "noun\tbeneficial owner certification\tubo\n",
    "noun\tbeneficiary\tbeanies\n",
    "noun\tbeneficiary\tbeen\n",
    "noun\tbeneficiary\tben\n",
    "noun\tbeneficiary\tbene\n",
    "noun\tbeneficiary\tbened\n",
    "noun\tbeneficiary\tbenef\n",
    "noun\tbeneficiary\tbenefciary\n",
    "noun\tbeneficiary\tbenefiacries\n",
    "noun\tbeneficiary\tbenefiaries\n",
    "noun\tbeneficiary\tbeneficary\n",
    "noun\tbeneficiary\tbeneficciaries\n",
    "noun\tbeneficiary\tbeneficiares\n",
    "noun\tbeneficiary\tbeneficiaries\n",
    "noun\tbeneficiary\tbeneficiaries'\n",
    "noun\tbeneficiary\tbeneficiaryies\n",
    "noun\tbeneficiary\tbeneficiarys\n",
    "noun\tbeneficiary\tbeneficicary\n",
    "noun\tbeneficiary\tbenefiiary\n",
    "noun\tbeneficiary\tbenefiicary\n",
    "noun\tbeneficiary\tbenefiociaries\n",
    "noun\tbeneficiary\tbeneies\n",
    "noun\tbeneficiary\tbeneificiaries\n",
    "noun\tbeneficiary\tbenes\n",
    "noun\tbeneficiary\tbenficiaries'\n",
    "noun\tbeneficiary\tbeni\n",
    "noun\tbeneficiary\tbenies\n",
    "noun\tbeneficiary\tbenne\n",
    "noun\tbeneficiary\tbennie\n",
    "noun\tbeneficiary\tbennies\n",
    "noun\tbeneficiary\tbennis\n",
    "noun\tbeneficiary\tbens\n",
    "noun\tbeneficiary\tbne\n",
    "noun\tbeneficiary\trbene\n",
    "noun\tbeneficiary\tthebene\n",
    "noun\tbeneficiary's full name\tname\n",
    "noun\tblanket approval letter\tbal\n",
    "noun\tblanket approval letter\tblanket\n",
    "noun\tblock buys trading only red flag\t13a\n",
    "noun\tblp - business other account\tgbo\n",
    "noun\tblp - charity organization account\tgwc\n",
    "noun\tblp - incorporated account\tgbi\n",
    "noun\tblp - individual account\tgwi\n",
    "noun\tblp - joint account\tgwj\n",
    "noun\tblp - listed company account\tgwl\n",
    "noun\tblp - personal holding company account\tgwp\n",
    "noun\tblp - sole prop account\tgbs\n",
    "noun\tblp - trust account\tgwt\n",
    "noun\tblp account\tblp\n",
    "noun\tbranch\tbrach\n",
    "noun\tbranch\tbranc\n",
    "noun\tbranch\tbranched\n",
    "noun\tbranch\tbranches\n",
    "noun\tbranch\tbranchs\n",
    "noun\tbranch\tbrnach\n",
    "noun\tbranch\tbrnaches\n",
    "noun\tbranch office managers\tbom\n",
    "noun\tbranch office support system\tboss\n",
    "noun\tbranch workflow\tbranchworkflow\n",
    "noun\tbranch workflow\tbwf\n",
    "noun\tbranch written supervisory procedure\twsp\n",
    "noun\tbranch-issued check\tchecks\n",
    "noun\tbroker dealer\tb.d\n",
    "noun\tbroker dealer\tbor\n",
    "noun\tbroker dealer\tbroker\n",
    "noun\tbroker dealer\tstockbroker\n",
    "noun\tbroker identification number\tbin\n",
    "noun\tbrokerage account\tbrokerage\n",
    "noun\tbusiness active assets account\tbaaa\n",
    "noun\tbusiness active assets account\tbus\n",
    "noun\tbusiness active assets account\tbusiness\n",
    "noun\tbusiness active assets account\tbussn\n",
    "noun\tbusiness basic securities account\tbbsa\n",
    "noun\tbusiness retirement account\tarp\n",
    "noun\tbusiness retirement account\tbrp\n",
    "noun\tbusiness retirement account\tqpr\n",
    "noun\tbusiness retirement account\tqps\n",
    "noun\tbusiness retirement account\tqrp\n",
    "noun\tbusiness retirement account\tqrps\n",
    "noun\tbusiness retirement account\tqualified\n",
    "noun\tbusiness service manager\tbesm\n",
    "noun\tbusiness service manager\tbsm\n",
    "noun\tbusiness service manager\tbsn\n",
    "noun\tbusiness service manager\tmanager\n",
    "noun\tcaptive insurance account\tcaptive\n",
    "noun\tcase management request\tcase\n",
    "noun\tcase management request\tcases\n",
    "noun\tcash\tmoney\n",
    "noun\tcash account\tcash\n",
    "noun\tcash on delivery\tcod\n",
    "noun\tcashless exercise account\tcsx\n",
    "noun\tcenter for investment excellence\tcie\n",
    "noun\tcentral review unit\tcru\n",
    "noun\tcertificate\tcert\n",
    "noun\tcertificate\tcertificates\n",
    "noun\tcertificate\tcertification\n",
    "noun\tcertificate of deposit\tcds\n",
    "noun\tcg advisor loa consenting to principal trading\tcgploapt\n",
    "noun\tcharitable remainder trust account\tcrt\n",
    "noun\tcharitable remainder trust account\tcruts\n",
    "noun\tcheck\tcehcks\n",
    "noun\tcheck\tchecck\n",
    "noun\tcheck\tchecking\n",
    "noun\tcheck\tchecks\n",
    "noun\tcheck\tchekc\n",
    "noun\tcheck\tpaycheck\n",
    "noun\tcheck writing privilege\tcheckwriitng\n",
    "noun\tcheck writing privilege\tcheckwriting\n",
    "noun\tcheck writing privilege\tcheckwritting\n",
    "noun\tchild\tchildren\n",
    "noun\tchild support enforcement - allow trading red flag\t5c1\n",
    "noun\tchild support enforcement - block all transactions red flag\t5c2\n",
    "noun\tchild support enforcement- liquidation orders only red flag\t5c3\n",
    "noun\tcity\tlondon\n",
    "noun\tcity\tnyc\n",
    "noun\tcivil dispute - allow trading red flag\t5d1\n",
    "noun\tcivil dispute - block all transactions red flag\t5d2\n",
    "noun\tcivil dispute - liquidation orders only red flag\t5d3\n",
    "noun\tclass action query system\tcaq\n",
    "noun\tclient advisory center\tcac\n",
    "noun\tclient disclosure document\tcdd\n",
    "noun\tclient profile\tprofile\n",
    "noun\tclient profile system\tcps\n",
    "noun\tclient service associate\tcsa\n",
    "noun\tclient web support\tcws\n",
    "noun\tclosed jurisdiction with loan red flag\t15b\n",
    "noun\tcommission and processing fees\tcommission\n",
    "noun\tcommission and processing fees\tcommissions\n",
    "noun\tcommodity\tcommodities\n",
    "noun\tcommunications review center\tcrc\n",
    "noun\tcommunity property account\tcom\n",
    "noun\tcomplex risk officer\tcro\n",
    "noun\tcomplex risk officer\tcros\n",
    "noun\tconference call\tteleconference\n",
    "noun\tconnecticut higher education trust - advisor plan account\tchet\n",
    "noun\tconnecticut w-4 p form\tct24p\n",
    "noun\tconnecticut w-4 p form\tct4p\n",
    "noun\tconnecticut w-4 p form\tctw4p\n",
    "noun\tconsolidated year end summary statement\tcyess\n",
    "noun\tconsulting and evaluation services account\tces\n",
    "noun\tconsulting group advisor account\tcga\n",
    "noun\tcontingent beneficiary\tcontingent\n",
    "noun\tcontingent beneficiary\tcontingents\n",
    "noun\tcontingent deferred sale charge\tcdsc\n",
    "noun\tcontrafirm\tcontra\n",
    "noun\tcontribution\tconbtrib\n",
    "noun\tcontribution\tconbtribution\n",
    "noun\tcontribution\tcongtrib\n",
    "noun\tcontribution\tcont\n",
    "noun\tcontribution\tcontib\n",
    "noun\tcontribution\tcontibs\n",
    "noun\tcontribution\tcontirb\n",
    "noun\tcontribution\tcontirbs\n",
    "noun\tcontribution\tcontirbution\n",
    "noun\tcontribution\tcontirbutions\n",
    "noun\tcontribution\tcontirbuton\n",
    "noun\tcontribution\tcontr\n",
    "noun\tcontribution\tcontrb\n",
    "noun\tcontribution\tcontrbution\n",
    "noun\tcontribution\tcontri\n",
    "noun\tcontribution\tcontrib\n",
    "noun\tcontribution\tcontribn\n",
    "noun\tcontribution\tcontribs\n",
    "noun\tcontribution\tcontribtuion\n",
    "noun\tcontribution\tcontribtuiosn\n",
    "noun\tcontribution\tcontribtution\n",
    "noun\tcontribution\tcontribuitn\n",
    "noun\tcontribution\tcontribuiton\n",
    "noun\tcontribution\tcontribute\n",
    "noun\tcontribution\tcontributing\n",
    "noun\tcontribution\tcontributioin\n",
    "noun\tcontribution\tcontributions\n",
    "noun\tcontribution\tcontributios\n",
    "noun\tcontribution\tcontributons\n",
    "noun\tcontribution\tcontris\n",
    "noun\tcontribution\tocntributon\n",
    "noun\tcontribution coding\tcoding\n",
    "noun\tcontribution deadline\tdeadline\n",
    "noun\tcontribution deadline\tdeadlines\n",
    "noun\tcontribution limit\tlimit\n",
    "noun\tcontribution limit\tlimits\n",
    "noun\tcontribution limit\tllimit\n",
    "noun\tcontribution limit\tmax\n",
    "noun\tcontribution recharacterization\trech\n",
    "noun\tcontribution recharacterization\trechacterization\n",
    "noun\tcontribution recharacterization\trechar\n",
    "noun\tcoogan trust account\tcoogan\n",
    "noun\tcorporation account\tcorp\n",
    "noun\tcorporation account\tcorporaiton\n",
    "noun\tcorporation account\tcorporate\n",
    "noun\tcorporation account\tcorporation\n",
    "noun\tcorporation account\tcorporations\n",
    "noun\tcorporation account\tcorps\n",
    "noun\tcorporation account\tinc\n",
    "noun\tcountry\taustrailia\n",
    "noun\tcountry\taustralia\n",
    "noun\tcountry\tbahamas\n",
    "noun\tcountry\tbelgium\n",
    "noun\tcountry\tbvi\n",
    "noun\tcountry\tcanada\n",
    "noun\tcountry\tcanadian\n",
    "noun\tcountry\tchina\n",
    "noun\tcountry\tcountries\n",
    "noun\tcountry\tegypt\n",
    "noun\tcountry\tfrance\n",
    "noun\tcountry\tgermany\n",
    "noun\tcountry\tguam\n",
    "noun\tcountry\tguatemala\n",
    "noun\tcountry\tguyana\n",
    "noun\tcountry\tireland\n",
    "noun\tcountry\tisrael\n",
    "noun\tcountry\tjapan\n",
    "noun\tcountry\tmexico\n",
    "noun\tcountry\tmonaco\n",
    "noun\tcountry\tpakastan\n",
    "noun\tcountry\tscotland\n",
    "noun\tcountry\tsingapore\n",
    "noun\tcountry\tspain\n",
    "noun\tcountry\tswiterland\n",
    "noun\tcountry\tthailand\n",
    "noun\tcountry\tu.k\n",
    "noun\tcountry\tu.s\n",
    "noun\tcountry\tu.s.\n",
    "noun\tcountry\tusa\n",
    "noun\tcountry\tvenezula\n",
    "noun\tcourt order - allow trading red flag\t5a1\n",
    "noun\tcourt order - block all transactions red flag\t5a2\n",
    "noun\tcourt order - liquidation orders only red flag\t5a3\n",
    "noun\tcoverdell education savings account\tcesa\n",
    "noun\tcoverdell education savings account\tconverdell\n",
    "noun\tcoverdell education savings account\tcoverdell\n",
    "noun\tcoverdell education savings account\tcoverdells\n",
    "noun\tcoverdell education savings account\tesa\n",
    "noun\tcure document\tcure\n",
    "noun\tcure document\tdoc\n",
    "noun\tcurrency\tdollar\n",
    "noun\tcurrency\tdollars\n",
    "noun\tcurrency\teuro\n",
    "noun\tcurrency\teuros\n",
    "noun\tcurrency\tpound\n",
    "noun\tcusip number\tcusip\n",
    "noun\tcustodian\tcus\n",
    "noun\tcustodian\tcusodian\n",
    "noun\tcustodian\tcusto\n",
    "noun\tcustodian\tcustodians\n",
    "noun\tcustodian\tcustodina\n",
    "noun\tcustodian account\tcus\n",
    "noun\tcustodian account\tcust\n",
    "noun\tcustodian account\tcustod\n",
    "noun\tcustodian account\tcustodial\n",
    "noun\tcustodian account\tcustodian\n",
    "noun\tcustomer identification program\tcip\n",
    "noun\tcustomer identification program process\tciped\n",
    "noun\tdata quality - block all activity red flag\t16b\n",
    "noun\tdata quality - restrict buy trades only red flag\t16d\n",
    "noun\tdata quality - restrict deposits and withdrawals red flag\t16c\n",
    "noun\tdate of birth\tbirthdate\n",
    "noun\tdate of birth\tbirthday\n",
    "noun\tdate of birth\td.o.b\n",
    "noun\tdate of birth\tdob\n",
    "noun\tdate of death\tdod\n",
    "noun\tdaughter\tdaughters\n",
    "noun\tdeadline\tdeadlines\n",
    "noun\tdeath certificate\tdct\n",
    "noun\tdeath distribution\tdeath\n",
    "noun\tdeath distribution\tdeathdist\n",
    "noun\tdeath distribution\tdistribution\n",
    "noun\tdebit card\tdebit\n",
    "noun\tdebit card\tdebti\n",
    "noun\tdeceased account owner red flag\tdeceasd\n",
    "noun\tdeceased client\tdec\n",
    "noun\tdeceased client\tdecadent\n",
    "noun\tdeceased client\tdecease\n",
    "noun\tdeceased client\tdeceased\n",
    "noun\tdeceased client\tdecedant\n",
    "noun\tdeceased client\tdecedent\n",
    "noun\tdeceased client\tdecedents\n",
    "noun\tdeceased client\tdecednt\n",
    "noun\tdeceased client\tdecendents\n",
    "noun\tdeceased client\tdecent\n",
    "noun\tdeceased client\tdecents\n",
    "noun\tdeductible asset\tdeductible\n",
    "noun\tdefault beneficiary order\tdefault\n",
    "noun\tdelivery versus payment\tdvp\n",
    "noun\tdeposit\tdeosited\n",
    "noun\tdeposit\tdepoit\n",
    "noun\tdeposit\tdeposited\n",
    "noun\tdeposit\tdeposits\n",
    "noun\tdeposit\tdeposti\n",
    "noun\tdepository trust & clearing corporation\tdtcc\n",
    "noun\tdepository trust check\tdtc\n",
    "noun\tdepository trust company transfer\tdtc\n",
    "noun\tdirect rollover\tddro\n",
    "noun\tdirected share business account\tdsb\n",
    "noun\tdirected share program\tdsp\n",
    "noun\tdisbursement\tdisburse\n",
    "noun\tdisbursement\tdisbursed\n",
    "noun\tdisbursement\tdisbursements\n",
    "noun\tdisbursement\tdisbursing\n",
    "noun\tdisbursement\tdisbusement\n",
    "noun\tdisbursement restriction -lending operations red flag\t12a\n",
    "noun\tdiscount rate\tdiscount\n",
    "noun\tdiscretionary fee waiver allowance\tdfwa\n",
    "noun\tdisplay account restriction red flag\t11a\n",
    "noun\tdistribution\tdiddist\n",
    "noun\tdistribution\tdis\n",
    "noun\tdistribution\tdis.t\n",
    "noun\tdistribution\tdisribution\n",
    "noun\tdistribution\tdist\n",
    "noun\tdistribution\tdist.\n",
    "noun\tdistribution\tdistibituion\n",
    "noun\tdistribution\tdistibution\n",
    "noun\tdistribution\tdistirbuiton\n",
    "noun\tdistribution\tdistirbution\n",
    "noun\tdistribution\tdistirubtion\n",
    "noun\tdistribution\tdistrbution\n",
    "noun\tdistribution\tdistribtuion\n",
    "noun\tdistribution\tdistribuition\n",
    "noun\tdistribution\tdistribuiton\n",
    "noun\tdistribution\tdistribut9ion\n",
    "noun\tdistribution\tdistribute\n",
    "noun\tdistribution\tdistributin\n",
    "noun\tdistribution\tdistributionj\n",
    "noun\tdistribution\tdistributionrequest\n",
    "noun\tdistribution\tdistributions\n",
    "noun\tdistribution\tdistribution_zip_\n",
    "noun\tdistribution\tdistributoin\n",
    "noun\tdistribution\tdistributoion\n",
    "noun\tdistribution\tdistributrion\n",
    "noun\tdistribution\tdistribuytion\n",
    "noun\tdistribution\tdistriibution\n",
    "noun\tdistribution\tdistriubiton\n",
    "noun\tdistribution\tdistrobution\n",
    "noun\tdistribution\tdistrubution\n",
    "noun\tdistribution\tdists\n",
    "noun\tdistribution\tdistz\n",
    "noun\tdistribution\tditribution\n",
    "noun\tdistribution\tdiust\n",
    "noun\tdistribution\tdost\n",
    "noun\tdistribution\tdsitribution\n",
    "noun\tdistribution\thdistribution\n",
    "noun\tdistribution\tw;dist\n",
    "noun\tdistribution form\tdistribution\n",
    "noun\tdistribution form\tdistributions\n",
    "noun\tdistribution form\tdistrubition\n",
    "noun\tdistribution options\tdistribution\n",
    "noun\tdistribution options\tdistributions\n",
    "noun\tdistribution options\tmethod\n",
    "noun\tdistribution options\toption\n",
    "noun\tdistribution options\toptions\n",
    "noun\tdividend\tdiv\n",
    "noun\tdividend\tdividends\n",
    "noun\tdivorce distribution\tdivorce\n",
    "noun\tdivorce distribution\tdivorces\n",
    "noun\tdocument sharing automation tool\tdocshare\n",
    "noun\tdocument sharing automation tool\tdocushare\n",
    "noun\tduplicate distribution\tduplicated\n",
    "noun\tdurable power of attorney full discretionary authorization form\tgvd\n",
    "noun\tearly dispute resolution department\tedr\n",
    "noun\teauthorization\teauth\n",
    "noun\tedelivery preferences\tedelivery\n",
    "noun\tedelivery preferences\tedilivery\n",
    "noun\tedelivery preferences\tedlivery\n",
    "noun\telectronic funds transfer\teft\n",
    "noun\temail\temailed\n",
    "noun\temail\temails\n",
    "noun\temail\temaoil\n",
    "noun\temail address\teamil\n",
    "noun\temail address\temaail\n",
    "noun\temail address\temail\n",
    "noun\temployer identification number\tein\n",
    "noun\temployment information\temployment\n",
    "noun\temployment retirement income security act of 1974\terisa\n",
    "noun\tenhanced due diligence process\tedd\n",
    "noun\tenhanced due diligence questionnaire\tanedd\n",
    "noun\tenhanced due diligence questionnaire\tedd\n",
    "noun\tenhanced due diligence questionnaire\teddauthenticated\n",
    "noun\tenhanced due diligence questionnaire\teddd\n",
    "noun\tenhanced due diligence questionnaire\tedds\n",
    "noun\tenhanced due diligence questionnaire\tedd}ff\n",
    "noun\tenhanced due diligence questionnaire\teed\n",
    "noun\tenhanced due diligence questionnaire\tinedd\n",
    "noun\tentitlements provisioning and reporting team\tepr\n",
    "noun\tentity name\tameritrade\n",
    "noun\tentity name\tautovest\n",
    "noun\tentity name\tbarney\n",
    "noun\tentity name\tbis\n",
    "noun\tentity name\tblackstone\n",
    "noun\tentity name\tcbm\n",
    "noun\tentity name\tchase\n",
    "noun\tentity name\tcomerica\n",
    "noun\tentity name\tecl\n",
    "noun\tentity name\tfidelity\n",
    "noun\tentity name\thewitt\n",
    "noun\tentity name\tlincoln\n",
    "noun\tentity name\tlionhearted\n",
    "noun\tentity name\tm.s\n",
    "noun\tentity name\tmccm\n",
    "noun\tentity name\tmerril\n",
    "noun\tentity name\tmorgan\n",
    "noun\tentity name\tmssb\n",
    "noun\tentity name\tname\n",
    "noun\tentity name\tpenchecks\n",
    "noun\tentity name\tpenchex\n",
    "noun\tentity name\tpenco\n",
    "noun\tentity name\tpensco\n",
    "noun\tentity name\tpfizer\n",
    "noun\tentity name\tschwab\n",
    "noun\tentity name\tschwabb\n",
    "noun\tentity name\tsuntrust\n",
    "noun\tentity name\ttiaa\n",
    "noun\tentity name\ttransamerica\n",
    "noun\tentity name\tvanguard\n",
    "noun\tentity name\tvoya\n",
    "noun\tentity name\twells\n",
    "noun\tequal credit opportunity act\tecoa\n",
    "noun\tequity\tequities\n",
    "noun\tequity\tsec\n",
    "noun\tequity\tsecurities\n",
    "noun\tequity\tsecurity\n",
    "noun\tequity\tshare\n",
    "noun\tequity\tshares\n",
    "noun\tequity\tshars\n",
    "noun\tequity\tstock\n",
    "noun\tequity\tstocks\n",
    "noun\terisa held away exception alert\teha\n",
    "noun\terisa plan held away account\teha\n",
    "noun\terisa plan held away account\tehas\n",
    "noun\terisa plan held away account\terisa\n",
    "noun\terisa plan held away account\terisas\n",
    "noun\tescheatment operations department\tescheatment\n",
    "noun\tesign\tdesign\n",
    "noun\tesign\teasign\n",
    "noun\tesign\teign\n",
    "noun\tesign\teisgn\n",
    "noun\tesign\tesgn\n",
    "noun\tesign\tesigh\n",
    "noun\tesign\tesignable\n",
    "noun\tesign\tesignature\n",
    "noun\tesign\tesigned\n",
    "noun\tesign\tesignhow\n",
    "noun\tesign\tesigning\n",
    "noun\tesign\tesignpacklage\n",
    "noun\tesign\tesing\n",
    "noun\tesign\tinesign\n",
    "noun\tesign\tnesign\n",
    "noun\tesign\tpackage\n",
    "noun\tesign\tpacket\n",
    "noun\tesign\ttheesign\n",
    "noun\testate account\test\n",
    "noun\testate account\testate\n",
    "noun\testate beneficiary\tesate\n",
    "noun\testate document\test\n",
    "noun\texcess contribution\texces\n",
    "noun\texcess contribution\texcess\n",
    "noun\texcess contribution\texcessdue\n",
    "noun\texcess contribution\texcuses\n",
    "noun\texcess contribution\texess\n",
    "noun\texcess contribution\texzcess\n",
    "noun\texcess contribution\txcess\n",
    "noun\texchange traded fund\tetf\n",
    "noun\texecutor\texcurtor\n",
    "noun\texecutor\texec\n",
    "noun\texecutor\texecuter\n",
    "noun\texecutor\texecutier\n",
    "noun\texecutor\texecutors\n",
    "noun\texpress creditline\tecl\n",
    "noun\texpress creditline\tsbl\n",
    "noun\texternal custodian ira/keogh account\tkeogh\n",
    "noun\texternal power of attorney\texternal\n",
    "noun\tfederal tax withholding\tfed\n",
    "noun\tfederal tax withholding\tfederal\n",
    "noun\tfee\tfees\n",
    "noun\tfee\twasfee\n",
    "noun\tfiduciary certification\tfiduciary\n",
    "noun\tfiduciary certification and trust account agreement\ttrd\n",
    "noun\tfield services unit of legal compliance\tfsu\n",
    "noun\tfield services unit of legal compliance\tlegal\n",
    "noun\tfinancial institution questionnaire\tffi\n",
    "noun\tfinancial institution questionnaire\tfiq\n",
    "noun\tfinra 3210 rule\t3210\n",
    "noun\tfinra 3210 rule\tfinra\n",
    "noun\tfinra 3210 rule\tfirna\n",
    "noun\tfixed income\tbond\n",
    "noun\tfixed income\tbonds\n",
    "noun\tfixed income service team\tfis\n",
    "noun\tfixed rate note\tfrn\n",
    "noun\tfixed recurring distribution\trecurring\n",
    "noun\tforeign account tax compliance act\tfatca\n",
    "noun\tforeign exchange new account summary form\tctmfenas\n",
    "noun\tforeign financial institution\tffi\n",
    "noun\tforeign related entity account\tforedign\n",
    "noun\tfraud - allow trading red flag\t5f1\n",
    "noun\tfraud - block all transactions red flag\t5f2\n",
    "noun\tfraud - liquidation orders only red flag\t5f3\n",
    "noun\tfraudulent activity\tfraud\n",
    "noun\tfund\tfunds\n",
    "noun\tfunds transfer service enrollment form\tfts\n",
    "noun\tfunds transfer system\tfts\n",
    "noun\tfutures account agreement\tfaa\n",
    "noun\tgeneral partnership account\tpar\n",
    "noun\tgeneral partnership account\tppartnerhsip\n",
    "noun\tgeneral purpose - international account control red flag\t15a\n",
    "noun\tgeneral purpose plus block check or card order red flag\t15d\n",
    "noun\tgeneration skipping trust account\tgst\n",
    "noun\tgift\tdonation\n",
    "noun\tgift\tdonations\n",
    "noun\tgift\tgifts\n",
    "noun\tglobal currency express\tgce\n",
    "noun\tglobal product screener\tgps\n",
    "noun\tgovernment entity account\tgov\n",
    "noun\tgovernment issued photo id\tgd1\n",
    "noun\tgovernment issued photo id\tgdm\n",
    "noun\tgovernment issued photo id\tgovid\n",
    "noun\tgovernment issued photo id\tgvo\n",
    "noun\tgrantor\tgran\n",
    "noun\tgrantor\tgrantors\n",
    "noun\tgrantor\tsettlor\n",
    "noun\tgrantor\ttrustor\n",
    "noun\tgrantor retained trust account\tgrat\n",
    "noun\tguardian\tconservator\n",
    "noun\tguardian\tconservators\n",
    "noun\tguardian\tconservitor\n",
    "noun\tguardian\tgdn\n",
    "noun\tguardian\tgua\n",
    "noun\tguardian\tguardians\n",
    "noun\tguardian\tguardianship\n",
    "noun\tguardianship account\tconservator\n",
    "noun\tguardianship account\tconservatorship\n",
    "noun\tguardianship account\tgua\n",
    "noun\tguardianship account\tguard\n",
    "noun\tguardianship account\tguardian\n",
    "noun\tguardianship account\tguardianship\n",
    "noun\thard dollar fee - community property account\tmcp\n",
    "noun\thard dollar fee - iro account\tmhi\n",
    "noun\thard dollar fee - joint tenants by the entirety account\tmje\n",
    "noun\thard dollar fee - joint tenants in common account\tmjc\n",
    "noun\thard dollar fee - non-profit organization account\tmnp\n",
    "noun\thard dollar fee - non-profit organization account\tnpo\n",
    "noun\thard dollar fee - religious organization account\tmpr\n",
    "noun\thard dollar fee - testamentary trust account\tmtt\n",
    "noun\thard dollar fee - unincorporated association account\tmpa\n",
    "noun\thard dollar fee -guardianship account\tmpg\n",
    "noun\thard dollar fee -incorporated account\tmpi\n",
    "noun\thard dollar fee a/c ind account\tmhn\n",
    "noun\thard dollar fee erisa trust account\tmpe\n",
    "noun\thard dollar fee government agency account\tmgv\n",
    "noun\thard dollar fee limited liability company account\tmpl\n",
    "noun\thard dollar fee living trust account\tmpt\n",
    "noun\thard dollar fee partnership account\tmpp\n",
    "noun\thigh risk address\thra\n",
    "noun\thouseholding\thouseheld\n",
    "noun\thouseholding\thousehold\n",
    "noun\thouseholding\thouseholded\n",
    "noun\thouseholding\thouseholds\n",
    "noun\thouseholding\thouseholing\n",
    "noun\thouseholding\thoushold\n",
    "noun\thouseholding\thousholding\n",
    "noun\thouseholding department\thouseholding\n",
    "noun\tid\tidentification\n",
    "noun\tid\tids\n",
    "noun\tin-kind contribution\tinkind\n",
    "noun\tin-kind contribution\tinkine\n",
    "noun\tin-kind distribution\tinkind\n",
    "noun\tincapacitated person\tincapacitated\n",
    "noun\tincome\tearnings\n",
    "noun\tincorporated non-profit organization account\tfoundation\n",
    "noun\tincorporated non-profit organization account\tincorporated\n",
    "noun\tincorporated non-profit organization account\tnpi\n",
    "noun\tindividual 401k account\tikk\n",
    "noun\tindividual 401k account\tindivk\n",
    "noun\tindividual 401k account\tindv\n",
    "noun\tindividual 401k account\tindvk\n",
    "noun\tindividual 401k account\tindyk\n",
    "noun\tindividual active assets account\taaa\n",
    "noun\tindividual active assets account\taaaa\n",
    "noun\tindividual retirement account\tiira\n",
    "noun\tindividual retirement account\tir.a\n",
    "noun\tindividual retirement account\tira\n",
    "noun\tindividual retirement account\tira'\n",
    "noun\tindividual retirement account\tiras\n",
    "noun\tindividual retirement account adoption agreement\tira\n",
    "noun\tindividual retirement account distribution form\trdd\n",
    "noun\tinherited ira account\tinh\n",
    "noun\tinherited ira account\tinherited\n",
    "noun\tinherited ira account\tinheritedira\n",
    "noun\tinherited ira account\tira\n",
    "noun\tinitial public offering\tipo\n",
    "noun\tinsurance product\tinsurance\n",
    "noun\tinsurance ticketing system\tits\n",
    "noun\tinterest amount\taccruedinterest\n",
    "noun\tinterest amount\tint\n",
    "noun\tinterest amount\tinterest\n",
    "noun\tinterest rate\trate\n",
    "noun\tinterest rate\trates\n",
    "noun\tinterest rate management application\tirma\n",
    "noun\tinterested party user\tinterested\n",
    "noun\tinternal power of attorney\tinternal\n",
    "noun\tinternational account\tforeign\n",
    "noun\tinternational account\tint\n",
    "noun\tinternational account\tinternantional\n",
    "noun\tinternational account\tinternational\n",
    "noun\tinternational account\tintl\n",
    "noun\tinternational account\tnra\n",
    "noun\tinternational client tax certification\tgqn\n",
    "noun\tinvestment club account\tclu\n",
    "noun\tinvestment management services program account\tims\n",
    "noun\tinvestment policy statement\tips\n",
    "noun\tinvestment product\tinvestment\n",
    "noun\tinvestment product\tinvestments\n",
    "noun\tinvestment services fee\tisf\n",
    "noun\tinvoice\tbill\n",
    "noun\tira - the morgan stanley roth ira account opening booklet\tretirarb\n",
    "noun\tira designation of beneficiary form\tirb\n",
    "noun\tira outside custodian account\tiro\n",
    "noun\tirrevocable life insurance trust account\tilt\n",
    "noun\tirrevocable living trust account\tirr\n",
    "noun\tirrevocable living trust account\tirrev\n",
    "noun\tirrevocable living trust account\tirrevo\n",
    "noun\tirrevocable living trust account\tirrevoc\n",
    "noun\tirrevocable living trust account\tirrevocable\n",
    "noun\tirrevocable living trust account\tirrevocalbe\n",
    "noun\tirrevocable living trust account\tirrevocbale\n",
    "noun\tirs form 1099\t1099\n",
    "noun\tirs form 1099-q: payments from qualified education programs\t1099q\n",
    "noun\tirs form 1099-r\t1009r\n",
    "noun\tirs form 1099-r\t1099r\n",
    "noun\tirs form 1099-r\t1099rs\n",
    "noun\tirs form 5173\t5173\n",
    "noun\tirs form 5498\t5498\n",
    "noun\tirs form 5498\t5498s\n",
    "noun\tis client living indicator\tdeceased\n",
    "noun\tjoint account\tajoint\n",
    "noun\tjoint account\tjint\n",
    "noun\tjoint account\tjnt\n",
    "noun\tjoint account\tjoin\n",
    "noun\tjoint account\tjoint\n",
    "noun\tjoint account\tjoints\n",
    "noun\tjoint account\tjpoint\n",
    "noun\tjoint account agreement with right of survivorship\tjawros\n",
    "noun\tjoint production number\tjpn\n",
    "noun\tjoint tenancy in common account\tjtc\n",
    "noun\tjoint tenant with rights of survivorship account\tjawros\n",
    "noun\tjoint tenant with rights of survivorship account\tjros\n",
    "noun\tjoint tenant with rights of survivorship account\tjrs\n",
    "noun\tjoint tenant with rights of survivorship account\tjrwros\n",
    "noun\tjoint tenant with rights of survivorship account\tjtwos\n",
    "noun\tjoint tenant with rights of survivorship account\tjtwros\n",
    "noun\tjoint tenant with rights of survivorship account\tjtwrs\n",
    "noun\tjoint tenant with rights of survivorship account\tjtwtros\n",
    "noun\tjoint tenant with rights of survivorship account\tjwros\n",
    "noun\tjoint tenant with rights of survivorship account\twros\n",
    "noun\tjoint tenants by the entirety account\tent\n",
    "noun\tjoint tenants by the entirety account\tentirity\n",
    "noun\tjournal\tjjournal\n",
    "noun\tjournal\tjnl\n",
    "noun\tjournal\tjouirnal\n",
    "noun\tjournal\tjourn\n",
    "noun\tjournal\tjournaled\n",
    "noun\tjournal\tjournaling\n",
    "noun\tjournal\tjournall\n",
    "noun\tjournal\tjournalling\n",
    "noun\tjournal\tjournals\n",
    "noun\tjournal\tjournel\n",
    "noun\tjournal\tjourneled\n",
    "noun\tjournal\tjrl\n",
    "noun\tjournal automated workstation system\tjaws\n",
    "noun\tjournal contribution\tjournal\n",
    "noun\tjournal entry request\tjes\n",
    "noun\tjournal entry request\tjournal\n",
    "noun\tjudgment of possession\tjop\n",
    "noun\tkey controller\tkeycontroller\n",
    "noun\tknow your customer\tkyc\n",
    "noun\tlal - business other account\tlao\n",
    "noun\tlal - charity not for profit account\tlan\n",
    "noun\tlal - corporation account\tlac\n",
    "noun\tlal - individual account\tlai\n",
    "noun\tlal - joint account\tlaj\n",
    "noun\tlal - partnership account\tlap\n",
    "noun\tlal - personal holding co account\tlah\n",
    "noun\tlal - sole prop account\tlas\n",
    "noun\tlal - trust account\tlat\n",
    "noun\tlal account\tlal\n",
    "noun\tlal account\tlals\n",
    "noun\tlal add account request form\tlenlalar\n",
    "noun\tlal certification for partnerships and llcs\tlenlalpc\n",
    "noun\tlanguage\tenglish\n",
    "noun\tlanguage\tportuguese\n",
    "noun\tlast will and testament\tlswlts\n",
    "noun\tlast will and testament\tlwt\n",
    "noun\tlegal address\tlegal\n",
    "noun\tletter of authorization\tloa\n",
    "noun\tletter of authorization\tloas\n",
    "noun\tletter of authorization\tlta\n",
    "noun\tletter of credit\tloc\n",
    "noun\tletter of explanation\tloe\n",
    "noun\tletter of instruction\tloi\n",
    "noun\tletter of intent\tloi\n",
    "noun\tletters of guardianship\tconservatorship\n",
    "noun\tletters of guardianship\tltrs\n",
    "noun\tletters testamentary\tllt\n",
    "noun\tletters testamentary\tltt\n",
    "noun\tletters testamentary\ttestamentary\n",
    "noun\tlexisnexis\tlexis\n",
    "noun\tlife tenant account\tlif\n",
    "noun\tlifeview portal\tlifeview\n",
    "noun\tlimited liability company account\tlcc\n",
    "noun\tlimited liability company account\tllc\n",
    "noun\tlimited liability company account\tllcs\n",
    "noun\tlimited liability company account\tlle\n",
    "noun\tlimited liability company account\tpllc\n",
    "noun\tlimited liability entity agreement\tllc\n",
    "noun\tlimited liability entity agreement\tlle\n",
    "noun\tlimited partnership\tlpr\n",
    "noun\tlimited partnership\tpartnership\n",
    "noun\tlimited partnership account\tlimited\n",
    "noun\tlimited partnership account\tlpr\n",
    "noun\tlimited partnership account\tpartner\n",
    "noun\tlimited partnership account\tpartnership\n",
    "noun\tliquidation of assets\tliquidatation\n",
    "noun\tliquidation of assets\tliquidate\n",
    "noun\tliquidation of assets\tliquidated\n",
    "noun\tliquidation of assets\tliquidation\n",
    "noun\tliquidity access line financial statement\tlenlalfs\n",
    "noun\tliquidity access line trust certification\tlenlaltr\n",
    "noun\tliving trust account\taliving\n",
    "noun\tliving trust account\tliv\n",
    "noun\tliving trust account\tliving\n",
    "noun\tloan\tloans\n",
    "noun\tloan\tlona\n",
    "noun\tloan account\tloan\n",
    "noun\tlocal signature page\telsp\n",
    "noun\tlocal signature page\tlps\n",
    "noun\tlocal signature page\tlsp\n",
    "noun\tlocal signature page\tlsps\n",
    "noun\tlocal signature page\tsignature\n",
    "noun\tlocal signature page\tslp\n",
    "noun\tlocal signature page\tslsp\n",
    "noun\tlondon interbank offered rate\tlibor\n",
    "noun\tlump sum distribution\tlumpsum\n",
    "noun\tmailing address\tmailing\n",
    "noun\tmanaged account\tmanage\n",
    "noun\tmanaged account\tmanaged\n",
    "noun\tmanaged advisory portfolio solutions\tmaps\n",
    "noun\tmargin\temargin\n",
    "noun\tmargin\tmaring\n",
    "noun\tmargin\tmrgin\n",
    "noun\tmargin account\tmargin\n",
    "noun\tmargin and loan agreement\tmla\n",
    "noun\tmarried\tmarriage\n",
    "noun\tmexican pesos\tmxn\n",
    "noun\tmichigan w-4 p form\tmi4p\n",
    "noun\tmichigan w-4 p form\tmiw4p\n",
    "noun\tmiddle markets custody account\tmmc\n",
    "noun\tmiddle markets dvp account\tfmm\n",
    "noun\tmiddle name\tmiddle\n",
    "noun\tminimum advisory fee\tminimum\n",
    "noun\tminor beneficiary\tminor\n",
    "noun\tminor beneficiary\tminors\n",
    "noun\tmissing enhanced due diligence red flag\tedd\n",
    "noun\tmissing politically exposed person info red flag\tpep\n",
    "noun\tmodel\tmodell\n",
    "noun\tmoney market fund\tmmf\n",
    "noun\tmorgan stanley global impact funding trust account\tdaf\n",
    "noun\tmorgan stanley global impact funding trust account\tgift\n",
    "noun\tmorgan stanley online\tmso\n",
    "noun\tmorgan stanley online\tmsol\n",
    "noun\tmorgan stanley online\tmsonline\n",
    "noun\tmorgan stanley online\tonline\n",
    "noun\tmorgan stanley online payment\tbillpay\n",
    "noun\tmorgan stanley virtual advisor\tmsva\n",
    "noun\tmortgage\tmortage\n",
    "noun\tmsai fraud operations review red flag\t17a\n",
    "noun\tmutual fund\tfund\n",
    "noun\tmutual fund\tfunds\n",
    "noun\tmutual fund block red flag\t15c\n",
    "noun\tnational new accounts red flag\tnna_reflags\n",
    "noun\tnational securities clearing corporation\tnscc\n",
    "noun\tnew account opening questionnaire\tnao\n",
    "noun\tnew account opening tool\t3dnao\n",
    "noun\tnew account opening tool\t3nao\n",
    "noun\tnew account opening tool\tnao\n",
    "noun\tnew account opening tool\tnaotool\n",
    "noun\tnext best action\tnba\n",
    "noun\tnna department\tnna\n",
    "noun\tnna workflow tools\tnna\n",
    "noun\tnna workflow tools\tnnatools\n",
    "noun\tnon resident alien\tnra\n",
    "noun\tnon traditional investment\tnti\n",
    "noun\tnon-custodial ira application and agreement\tretnoncu\n",
    "noun\tnon-deductible asset\tnondeductible\n",
    "noun\tnon-probate estate account\tint\n",
    "noun\tnon-spouse beneficiary\tnonspouse\n",
    "noun\tnorth carolina w-4 p form\tnc4p\n",
    "noun\tnorth carolina w-4 p form\tnc4wp\n",
    "noun\tnotification\tnorification\n",
    "noun\tnotification\tnotice\n",
    "noun\tnotification\tnotifications\n",
    "noun\tofficer\tofficers\n",
    "noun\toffshore trust - partnership account\titp\n",
    "noun\toffshore trust - personal investment corporation account\titc\n",
    "noun\toffshore trust account\titi\n",
    "noun\tok to comply\tokcomply\n",
    "noun\tok to comply\toktocompl\n",
    "noun\tok to comply\toktocomply\n",
    "noun\tone hundred percent tax withholding\t100\n",
    "noun\toption\toptiion\n",
    "noun\toption\toptions\n",
    "noun\toptions new account form and client agreement\topa\n",
    "noun\toptions new account form and client agreement\toptions\n",
    "noun\toutside custodian ira retirement plan account\tkpo\n",
    "noun\toverseas\tinternantional\n",
    "noun\towner\town\n",
    "noun\towner\towners\n",
    "noun\towner\townrer\n",
    "noun\towner\twoner\n",
    "noun\tpaperless transfer program\tpltp\n",
    "noun\tparticipant account\tpartic\n",
    "noun\tparticipant account\tparticipant\n",
    "noun\tpartnership account\tpartnership\n",
    "noun\tpassword\totp\n",
    "noun\tpatriot act address verification document\tpaa\n",
    "noun\tpatriot act address verification document\tpaa2\n",
    "noun\tpatriot act address verification document\tpaa2be\n",
    "noun\tpatriot act address verification document\tpaa3\n",
    "noun\tpatriot act address verification document\tpaa4\n",
    "noun\tpatriot act address verification document\tpaa5\n",
    "noun\tpatriot act address verification document\tpaa6\n",
    "noun\tpatriot act address verification document\tpaaa\n",
    "noun\tpatriot act address verification document\tpaapat\n",
    "noun\tpatriot act address verification document\tpaas\n",
    "noun\tpatriot act address verification document\tpad\n",
    "noun\tpatriot act address verification document\tppa\n",
    "noun\tpatriot act date of birth verification document\tapb\n",
    "noun\tpatriot act date of birth verification document\tdob\n",
    "noun\tpatriot act date of birth verification document\tpab\n",
    "noun\tpatriot act date of birth verification document\tpad\n",
    "noun\tpatriot act date of birth verification document\tpad3\n",
    "noun\tpatriot act date of birth verification document\tpadob\n",
    "noun\tpatriot act date of birth verification document\tpob\n",
    "noun\tpatriot act entity verification document\tentity\n",
    "noun\tpatriot act entity verification document\tpae\n",
    "noun\tpatriot act entity verification document\tpaet\n",
    "noun\tpatriot act entity verification document\tspae\n",
    "noun\tpatriot act entity verification document\tthepae\n",
    "noun\tpatriot act social security number verification document\t3pas2\n",
    "noun\tpatriot act social security number verification document\tpas\n",
    "noun\tpatriot act social security number verification document\tss4\n",
    "noun\tpatriot act social security number verification document\tsss\n",
    "noun\tpatriot act verification document\tpa2\n",
    "noun\tpatriot act verification document\tpat\n",
    "noun\tpayment\tpaid\n",
    "noun\tpayment\tpay\n",
    "noun\tpayment\tpayable\n",
    "noun\tpayment\tpayed\n",
    "noun\tpayment\tpaying\n",
    "noun\tpayment\tpayments\n",
    "noun\tpayment\tpays\n",
    "noun\tpayment\tpmt\n",
    "noun\tpending account number\tpan\n",
    "noun\tpending account number\tpan#\n",
    "noun\tpending account number\tpan3\n",
    "noun\tpending account number\tpans\n",
    "noun\tperiodic accumulated income distribution\tpaid\n",
    "noun\tperiodic review attestation form\tpra\n",
    "noun\tperson name\tdavid\n",
    "noun\tperson name\tjeff\n",
    "noun\tperson name\tkimberly\n",
    "noun\tperson name\tnaem\n",
    "noun\tperson name\tname\n",
    "noun\tperson name\tnamed\n",
    "noun\tperson name\tnames\n",
    "noun\tperson name\tnamnes\n",
    "noun\tpersonal holding company account\tphc\n",
    "noun\tpersonal identification number\tpin\n",
    "noun\tpersonal identification number\tpins\n",
    "noun\tphone number\tmobile\n",
    "noun\tphone number\tnumber\n",
    "noun\tphone number\tph#\n",
    "noun\tplan account\tplain\n",
    "noun\tplan sponsor\tsponsor\n",
    "noun\tplating address\taddreees\n",
    "noun\tplating address\tplate\n",
    "noun\tplating address\tplating\n",
    "noun\tplating name\tplating\n",
    "noun\tpledged account\tcollateral\n",
    "noun\tpledged account\tcollatral\n",
    "noun\tpledged account\tpledge\n",
    "noun\tpledged account\tpledged\n",
    "noun\tpolitically exposed person\tpep\n",
    "noun\tpossible deceased client red flag\t10a\n",
    "noun\tpower of attorney\tattorney\n",
    "noun\tpower of attorney\tdpoa\n",
    "noun\tpower of attorney\tpao\n",
    "noun\tpower of attorney\tpoa\n",
    "noun\tpower of attorney\tpoacan\n",
    "noun\tpower of attorney\tpoas\n",
    "noun\tpower of attorney\tpos\n",
    "noun\tpower of attorney form\tepoas\n",
    "noun\tpower of attorney form\tmypoa\n",
    "noun\tpower of attorney form\tpao\n",
    "noun\tpower of attorney form\tppoa\n",
    "noun\tpower of attorney form\tspoa\n",
    "noun\tpower of attorney form\tthispoa\n",
    "noun\tpremier cash management\tpcm\n",
    "noun\tprepayment\tprepayent\n",
    "noun\tprepayment\tprepaymet\n",
    "noun\tprepayment\tprepeyment\n",
    "noun\tprimary user\tprimaries\n",
    "noun\tprimary user\tprimary\n",
    "noun\tprimary user\ttheprimary\n",
    "noun\tprincipal\tpri\n",
    "noun\tprincipal\tprin\n",
    "noun\tprincipal account\tpri\n",
    "noun\tprior year contribution\tcontr\n",
    "noun\tprior year contribution\tcontribution\n",
    "noun\tprivate wealth advisor\tpwa\n",
    "noun\tprivate wealth management\tpwm\n",
    "noun\tprofessional alliance group\tpag\n",
    "noun\tprospect client\tpropect\n",
    "noun\tprospect client\tprospect\n",
    "noun\tprospect client\tprospects\n",
    "noun\tqualified charitable distribution\tqcd\n",
    "noun\tqualified charitable distribution\tqcds\n",
    "noun\tqualified disclaimer\tdisclaimer\n",
    "noun\tqualified disclaimer\tdisclaimers\n",
    "noun\tqualified domestic relations order\tqdro\n",
    "noun\tqualified institutional investor\tqib\n",
    "noun\tqualified plan participant agreement\tppa\n",
    "noun\tqualified plan participant agreement\tqpp\n",
    "noun\tqualified plan participant agreement\tqppa\n",
    "noun\tqualified plan participant agreement\tqppq\n",
    "noun\tqualified plan participant agreement\tqqp\n",
    "noun\trebate\tcashback\n",
    "noun\trebate\trebates\n",
    "noun\treceive versus payment\trvp\n",
    "noun\tred flag\tflag\n",
    "noun\tred flag\tflagged\n",
    "noun\tred flag\tflags\n",
    "noun\tred flag\tredflag\n",
    "noun\trefund\tref\n",
    "noun\trefund\trefun\n",
    "noun\trefund\trefunded\n",
    "noun\trefund\trefunds\n",
    "noun\trefund\treturn\n",
    "noun\tregistered client service associate\trcsa\n",
    "noun\treligious organization\tchurch\n",
    "noun\trequest for taxpayer identification number and certification\tw9f\n",
    "noun\trequest for taxpayer identification number and certification\tw9s\n",
    "noun\trequired death distribution\trdd\n",
    "noun\trequired death distribution\trdds\n",
    "noun\trequired death distribution\trddstart\n",
    "noun\trequired minimum distribution\trmd\n",
    "noun\trequired minimum distribution\trmdcalculated\n",
    "noun\trequired minimum distribution\trmddist\n",
    "noun\trequired minimum distribution\trmdrolled\n",
    "noun\trequired minimum distribution\trmds\n",
    "noun\trequired minimum distribution\trmds!\n",
    "noun\tresearch and strategy insights\trsi\n",
    "noun\treserved living & giving\treserved\n",
    "noun\tresidual distribution\tbalance\n",
    "noun\tresidual distribution\tesidual\n",
    "noun\tresidual distribution\treisudal\n",
    "noun\tresidual distribution\tremainder\n",
    "noun\tresidual distribution\treseidual\n",
    "noun\tresidual distribution\tresidual\n",
    "noun\tresidual distribution\tresiduals\n",
    "noun\tresidual distribution\tresiudals\n",
    "noun\trestriction\tlimitations\n",
    "noun\trestriction\trestrict\n",
    "noun\trestriction\trestrictions\n",
    "noun\tretail banking institution account\tban\n",
    "noun\tretirement account\tretireemnt\n",
    "noun\tretirement account\tretirement\n",
    "noun\tretirement account\tretm\n",
    "noun\tretirement plan manager account\trmp\n",
    "noun\tretirement plan manager account\trpm\n",
    "noun\tretirement plan manager account\trpms\n",
    "noun\tretirement plan manager agreement\trpm\n",
    "noun\tretirement plan manager plan account\trpm\n",
    "noun\tretirement plan manager subaccount\trpa\n",
    "noun\tretirement plan manager subaccount establishment form\tretsaef\n",
    "noun\tretirement plan manager subaccount establishment form\tsef\n",
    "noun\tretirement plan manager team\trpm\n",
    "noun\tretirement support services operations\trss\n",
    "noun\tretirement system\trtm\n",
    "noun\treturn on asset\troa\n",
    "noun\treturn on equity\troe\n",
    "noun\treturned stock certificate\treorg\n",
    "noun\trevocable living trust account\trev\n",
    "noun\trevocable living trust account\trevoc\n",
    "noun\trevocable living trust account\trevocable\n",
    "noun\treward point\tpoints\n",
    "noun\trights of accumulation\troa\n",
    "noun\trollover\trol\n",
    "noun\trollover\trolled\n",
    "noun\trollover\trolling\n",
    "noun\trollover\trolllover\n",
    "noun\trollover\trolllovers\n",
    "noun\trollover\trollout\n",
    "noun\trollover\trolls\n",
    "noun\trollover\trollvoer\n",
    "noun\trollover\trolover\n",
    "noun\trollover ira account\trol\n",
    "noun\trollover ira account\troll\n",
    "noun\trollover ira account\trollover\n",
    "noun\trollover ira account\trollovers\n",
    "noun\trollover ira account\trolover\n",
    "noun\troth conversion\tconversion\n",
    "noun\troth conversion\tconversions\n",
    "noun\troth distribution\tcontributions\n",
    "noun\troth inherited ira account\tcnr\n",
    "noun\troth inherited remainder ira account\tcne\n",
    "noun\troth ira account\trot\n",
    "noun\troth ira account\troth\n",
    "noun\troth ira account\troths\n",
    "noun\tsalary reduction sar-sep account\tsarsep\n",
    "noun\tsar sep participant account\tspc\n",
    "noun\tsar sep principal account\tspl\n",
    "noun\tsar-sep ira plan employer adoption agreement\tretsarg\n",
    "noun\tsecondary user\tsecondary\n",
    "noun\tsecurities investor protection corporation insured\tsipc\n",
    "noun\tsecurity journal\tsecurity\n",
    "noun\tself-directed retirement account\tsdr\n",
    "noun\tself-directed retirement account\tsdra\n",
    "noun\tself-directed retirement account\tsdras\n",
    "noun\tseparately managed account\tsma\n",
    "noun\tservice portal inquiry\tsri\n",
    "noun\tsibling\tsiblings\n",
    "noun\tsignature\tsig\n",
    "noun\tsignature\tsign\n",
    "noun\tsignature\tsignatures\n",
    "noun\tsignature\tsignaute\n",
    "noun\tsignature\tsigned\n",
    "noun\tsignature\tsigning\n",
    "noun\tsignature\tsigniture\n",
    "noun\tsignature\tsigns\n",
    "noun\tsignature\tsigs\n",
    "noun\tsimple ira account\tsimple\n",
    "noun\tsimple ira account\tsimples\n",
    "noun\tsimple ira account\twimple\n",
    "noun\tsimple ira participant account\tspb\n",
    "noun\tsimple ira plan employer adoption agreement\tretsmplg\n",
    "noun\tsimple ira principal account\tspi\n",
    "noun\tsimplified employee pension account\tsep\n",
    "noun\tsimplified employee pension account\tseps\n",
    "noun\tsimplified employee pension participant account\tspa\n",
    "noun\tsimplified employee pension principal account\tspr\n",
    "noun\tsingle account\tind\n",
    "noun\tsingle account\tindi\n",
    "noun\tsingle account\tindiv\n",
    "noun\tsingle account\tindivi\n",
    "noun\tsingle account\tindivid\n",
    "noun\tsingle account\tindividual\n",
    "noun\tsingle account\tindv\n",
    "noun\tsingle account\tsing\n",
    "noun\tsingle account\tsingle\n",
    "noun\tsingle advisory contract\tsac\n",
    "noun\tsingle advisory contract\tsacbe\n",
    "noun\tsingle advisory contract\tsacs\n",
    "noun\tsingle advisory contract\tsar\n",
    "noun\tsingle advisory contract\tsca\n",
    "noun\tsingle advisory contract\tssac\n",
    "noun\tsingle member limited liabilities company account\tdre\n",
    "noun\tsingle member limited liabilities company account\tllc\n",
    "noun\tsingle member limited liabilities company account\tsmllc\n",
    "noun\tsocial security number\tsnn\n",
    "noun\tsocial security number\tss#\n",
    "noun\tsocial security number\tssn\n",
    "noun\tsocial security number\tssn#\n",
    "noun\tsocial security number\tssns\n",
    "noun\tsocial security number card\tsan\n",
    "noun\tsocial security number card\tsscard\n",
    "noun\tsocial security number card\tssn\n",
    "noun\tsole proprietorship account\tsol\n",
    "noun\tson\tsons\n",
    "noun\tsort name\tshort\n",
    "noun\tsort name\tsortname\n",
    "noun\tspecial needs trust account\tsnt\n",
    "noun\tspousal beneficiary\twifes\n",
    "noun\tspousal distribution\tdistribution\n",
    "noun\tspouse\thusband\n",
    "noun\tspouse\thusbands\n",
    "noun\tspouse\tspousal\n",
    "noun\tspouse\twife\n",
    "noun\tspouse\twifes\n",
    "noun\tstate\tcali\n",
    "noun\tstate\tcalifornia\n",
    "noun\tstate\tconnecitcut\n",
    "noun\tstate\tconnecticut\n",
    "noun\tstate\td.c\n",
    "noun\tstate\td.c.\n",
    "noun\tstate\tdallas\n",
    "noun\tstate\tdelaware\n",
    "noun\tstate\tflorida\n",
    "noun\tstate\tillinois\n",
    "noun\tstate\tillnois\n",
    "noun\tstate\tiowa\n",
    "noun\tstate\tiowas\n",
    "noun\tstate\tkansas\n",
    "noun\tstate\tkentucky\n",
    "noun\tstate\tlouisiana\n",
    "noun\tstate\tlouisianan\n",
    "noun\tstate\tmaine\n",
    "noun\tstate\tmaryland\n",
    "noun\tstate\tmass\n",
    "noun\tstate\tmassachusetts\n",
    "noun\tstate\tmichigan\n",
    "noun\tstate\tmississippi\n",
    "noun\tstate\tn.c\n",
    "noun\tstate\tnebraska\n",
    "noun\tstate\tnevada\n",
    "noun\tstate\tohio\n",
    "noun\tstate\toregon\n",
    "noun\tstate\torgen\n",
    "noun\tstate\tpennsylvania\n",
    "noun\tstate\ts.c\n",
    "noun\tstate\tstates\n",
    "noun\tstate\ttennessee\n",
    "noun\tstate\ttexas\n",
    "noun\tstate\tvermont\n",
    "noun\tstate\tvirginia\n",
    "noun\tstate\twashington\n",
    "noun\tstate tax withholding\twithhold\n",
    "noun\tstock plan services account\tsps\n",
    "noun\tstreet name\tstreet\n",
    "noun\tsub account\tsub\n",
    "noun\tsub account\tsubaccount\n",
    "noun\tsub account\tsubaccounts\n",
    "noun\tsub account\tsubacct\n",
    "noun\tsub account\tsubaccts\n",
    "noun\tsub account\tsubacount\n",
    "noun\tsub account\tsubs\n",
    "noun\tsub trust account\tsubtrust\n",
    "noun\tsubaccount establishment form\tsef\n",
    "noun\tsuccessor owner\tsuccessor\n",
    "noun\tsystematic investment plan\tsip\n",
    "noun\tsystematic withdrawal plan\tswp\n",
    "noun\ttax\ttaxable\n",
    "noun\ttax\ttaxation\n",
    "noun\ttax\ttaxdes\n",
    "noun\ttax\ttaxed\n",
    "noun\ttax\ttaxes\n",
    "noun\ttax\ttaxs\n",
    "noun\ttax enforcement - allow trading red flag\t5b1\n",
    "noun\ttax enforcement - block all transactions red flag\t5b2\n",
    "noun\ttax enforcement - liquidation orders only red flag\t5b3\n",
    "noun\ttax id\tatin\n",
    "noun\ttax id\tftin\n",
    "noun\ttax id\titin\n",
    "noun\ttax id\ttax\n",
    "noun\ttax id\ttaxid\n",
    "noun\ttax id\ttid\n",
    "noun\ttax id\ttin\n",
    "noun\ttax id\ttin3\n",
    "noun\ttax id\ttins\n",
    "noun\ttax withholding\ttax\n",
    "noun\ttax withholding\ttaxes\n",
    "noun\ttax withholding\ttaxwithholdings\n",
    "noun\ttax withholding\twithhholding\n",
    "noun\ttax withholding\twithhodlings\n",
    "noun\ttax withholding\twithholding\n",
    "noun\ttax withholding\twithholdings\n",
    "noun\ttax withholding\twitholdings\n",
    "noun\ttaxline team\ttaxline\n",
    "noun\tteam joint account\ttfa\n",
    "noun\ttermination fee\ttermination\n",
    "noun\ttestamentary trust account\ttru\n",
    "noun\tthe morgan stanley traditional ira account opening booklet\tretirat\n",
    "noun\tthe tax equity and fiscal responsibility act\ttefra\n",
    "noun\tthird party administrator\ttpa\n",
    "noun\tthird party pledge agreement\tlenlaltp\n",
    "noun\tthird party pledge and guaranty agreement\tlenlalpg\n",
    "noun\tthird party verbal authorization form\ttpv\n",
    "noun\tthird party verbal authorization form\tverbal\n",
    "noun\tthriftsavingsplan\ttsp\n",
    "noun\ttrade\ttrad\n",
    "noun\ttrade\ttrader\n",
    "noun\ttrade\ttrades\n",
    "noun\ttrade\ttrading\n",
    "noun\ttraditional inherited remainder ira account\tcnt\n",
    "noun\ttraditional ira account\ttra.d\n",
    "noun\ttraditional ira account\ttrad\n",
    "noun\ttraditional ira account\ttradition\n",
    "noun\ttraditional ira account\ttraditional\n",
    "noun\ttraditional ira account\ttradtional\n",
    "noun\ttraditional ira account\ttransitional\n",
    "noun\ttransfer\tfts\n",
    "noun\ttransfer\ttranfer\n",
    "noun\ttransfer\ttranfser\n",
    "noun\ttransfer\ttrans\n",
    "noun\ttransfer\ttransf\n",
    "noun\ttransfer\ttransferred\n",
    "noun\ttransfer\ttransferring\n",
    "noun\ttransfer\ttransfers\n",
    "noun\ttransfer\ttransfewr\n",
    "noun\ttransfer\ttrsnafer\n",
    "noun\ttransfer\txfer\n",
    "noun\ttransfer\txfger\n",
    "noun\ttransfer\tytransfer\n",
    "noun\ttransfer on death\tt.o.d\n",
    "noun\ttransfer on death\ttod\n",
    "noun\ttransfer on death\ttods\n",
    "noun\ttransfer on death agreement\ttod\n",
    "noun\ttransfer on death agreement\ttods\n",
    "noun\ttransfer on death beneficiary\ttod\n",
    "noun\ttreasury inflation protected securities\ttips\n",
    "noun\ttrust account\ttrst\n",
    "noun\ttrust account\ttrsut\n",
    "noun\ttrust account\ttrust\n",
    "noun\ttrust account\ttrusted\n",
    "noun\ttrust account\ttrustl\n",
    "noun\ttrust account\ttrusts\n",
    "noun\ttrust account\tturst\n",
    "noun\ttrust account\ttust\n",
    "noun\ttrust beneficiary\tbeneficiary\n",
    "noun\ttrust beneficiary\tbeneficiarytrust\n",
    "noun\ttrust beneficiary\ttrusts\n",
    "noun\ttrust company account\tsbg\n",
    "noun\ttrust name\tname\n",
    "noun\ttrust name\ttitle\n",
    "noun\ttrust name\ttrusts\n",
    "noun\ttrust name\tttle\n",
    "noun\ttrustee\ttrstee\n",
    "noun\ttrustee\ttrsutee\n",
    "noun\ttrustee\ttruste\n",
    "noun\ttrustee\ttrusteee\n",
    "noun\ttrustee\ttrustees\n",
    "noun\ttrustee\ttrustes\n",
    "noun\ttrustee\ttrustess\n",
    "noun\ttrustee\ttrutee\n",
    "noun\ttrustee\tttee\n",
    "noun\ttrustee\tttees\n",
    "noun\ttrustee\tttrusee\n",
    "noun\ttrustee\tttrustee\n",
    "noun\ttrustee\ttustee\n",
    "noun\ttrustee certification form\ttrustee\n",
    "noun\tu.s government entities - liquidations only red flag\tgov1\n",
    "noun\tu.s. customer due diligence rule\tcdd\n",
    "noun\tu.s. dollar\tdollar\n",
    "noun\tu.s. dollar\tdollars\n",
    "noun\tu.s. dollar\tusd\n",
    "noun\tultimate beneficial owner\tubo\n",
    "noun\tultimate beneficial owner\tubos\n",
    "noun\tundeliverable mail red flag\t16a\n",
    "noun\tundeliverable mail red flag\teundleiverable\n",
    "noun\tundeliverable mail red flag\tudl\n",
    "noun\tundeliverable mail red flag\tundeliverable\n",
    "noun\tundeliverable mail red flag\tunderliverable\n",
    "noun\tunified estate request\tu.e\n",
    "noun\tunified estate workflow\teue\n",
    "noun\tunified estate workflow\tnue\n",
    "noun\tunified estate workflow\tu.e\n",
    "noun\tunified estate workflow\tu.e.\n",
    "noun\tunified estate workflow\tues\n",
    "noun\tunified estate workflow\tuew\n",
    "noun\tunified estate workflow\tuie\n",
    "noun\tunified estate workflow\tunified\n",
    "noun\tunified estate workflow\tunifiedestates\n",
    "noun\tunified managed account\tuma\n",
    "noun\tunified managed account\tumas\n",
    "noun\tuniform gift to minors act account\tugma\n",
    "noun\tuniform gift to minors act account\tugmas\n",
    "noun\tuniform transfer to minors act account\tutam\n",
    "noun\tuniform transfer to minors act account\tutma\n",
    "noun\tuniform transfer to minors act account\tutmas\n",
    "noun\tunincorporated association account\tuninc\n",
    "noun\tunincorporated association account\tunincorporated\n",
    "noun\tunit investment trust\tuit\n",
    "noun\tunrelated business income\tubi\n",
    "noun\tunrelated business taxable income\tubti\n",
    "noun\tusufruct account\tusu\n",
    "noun\tusufruct account\tusufruct\n",
    "noun\tusufruct account\tusufructory\n",
    "noun\tventure capital - joint tenants by the entirety account\tvje\n",
    "noun\tventure capital - joint tenants in common account\tvjc\n",
    "noun\tventure capital - sole proprietorship account\tvsp\n",
    "noun\tventure capital - unincorporated association account\tvua\n",
    "noun\tventure capital community property account\tvcp\n",
    "noun\tventure capital corporation account\tvci\n",
    "noun\tventure capital living trust account\tvtr\n",
    "noun\tventure capital llc account\tvlc\n",
    "noun\tventure capital partnership account\tvps\n",
    "noun\tventure capital service account\tvcs\n",
    "noun\tverbal distribution authorization\tavda\n",
    "noun\tverbal distribution authorization\tvda\n",
    "noun\tverbal distribution authorization\tvdas\n",
    "noun\tverbal distribution authorization\tverbal\n",
    "noun\tversatile investment program account\tvip\n",
    "noun\tversatile investment program account\tvipo\n",
    "noun\tversatile investment program account\tvips\n",
    "noun\tversatile investment program basic account\tvipb\n",
    "noun\tversatile investment program basic plan account\tvbp\n",
    "noun\tversatile investment program basic subaccount\tvba\n",
    "noun\tversatile investment program plus account\tplus\n",
    "noun\tversatile investment program plus account\tvipp\n",
    "noun\tversatile investment program plus plan account\tvpp\n",
    "noun\tversatile investment program plus subaccount\tvpa\n",
    "noun\tversatile investment program subaccount establishment form\tretvipsa\n",
    "noun\tvip qualified plan pre-adoption questionnaire\tretqpque\n",
    "noun\tw-8\tnnaw8\n",
    "noun\tw-8\tw82\n",
    "noun\tw-8\tw8rejected\n",
    "noun\tw-8\tw8s\n",
    "noun\tw-8 ben\tthew8ben\n",
    "noun\tw-8 ben\tw8b\n",
    "noun\tw-8 ben\tw8ben\n",
    "noun\tw-8 ben-e\tw8bene\n",
    "noun\tw-8 imy\tw8imy\n",
    "noun\twealth builder account\twbd\n",
    "noun\twebnas\twebnes\n",
    "noun\twire transfer\twire\n",
    "noun\twire transfer\twired\n",
    "noun\twire transfer\twires\n",
    "noun\twire transfer\twiring\n",
    "noun\tworkflow\twfh\n",
    "noun\tworkflow\tworkfllow\n",
    "noun\tworkflow\tworkflows\n",
    "noun\tworkflow dashboard\twfd\n",
    "noun\tworthless security\tworthless\n",
    "noun\tzip code\tzip\n",
    "noun\tzip code\tzipcodenoun\trequired minimum distribution\tzrmd\n",
    "CD\tct-w4p\tw4p\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords loaded.\n",
      "synonyms_noun_verb loaded.\n",
      "contractions loaded.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Keywords which should not be removed at all from input text.\n",
    "#\n",
    "vocab_list = [\"who\", \"what\", \"where\", \"when\", \"would\", \"which\", \"how\", \"why\", \"can\", \"may\", \"will\", \"won't\", \"does\", \n",
    "              \"does not\", \"doesn't\", \"do\", \"do i\", \"do you\", \"is it\", \"would you\", \"is there\", \"are there\", \"is it so\",\n",
    "              \"is this true\" ,\"to know\", \"is that true\", \"are we\", \"am i\", \"question is\", \"can i\", \"can we\", \"tell me\", \n",
    "              \"can you explain\", \"how ain't\", \"question\", \"answer\", \"questions\", \"answers\", \"ask\", \"can you tell\", \n",
    "              'e*trade','etrade','et','etrde', \"morgan staney\", \"ms\", \"ms@work\"]\n",
    "\n",
    "# containg all resource files\n",
    "resources_dir_path = \"/v/region/na/appl/mswm/ainlp/data/ainlp_dev/Pretrained_Models/Resources\"\n",
    "\"\"\"\n",
    "os.listdir(resources_dir_path)\n",
    "> \n",
    "['chatwords.txt',\n",
    " 'contractions.json',\n",
    " 'emojis.txt',\n",
    " 'emoticons.json',\n",
    " 'greeting_words.txt',\n",
    " 'signature_words.txt',\n",
    " 'special_characters.txt',\n",
    " 'special_stopwords.txt',\n",
    " 'stopwords.txt',\n",
    " 'synonyms_noun_verb.txt']\n",
    "\"\"\"\n",
    "\n",
    "# BASIC\n",
    "preprocessText = basic_preprocessing(resources_dir_path, vocab_list=vocab_list, custom=False)\n",
    "\n",
    "# SPACY ADVANCED\n",
    "preprocessText = spacy_preprocessing(resources_dir_path)\n",
    "\n",
    "\n",
    "def cleaning(data, text_col, clean_text_col_name=None):\n",
    "    if clean_text_col_name is None:\n",
    "        clean_text_col_name =  \"clean_%s\" % text_col\n",
    "    # add a progress tracker here...\n",
    "    data[clean_text_col_name] = preprocessText.clean(data[text_col])\n",
    "    return data\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# df = cleaning(df, <_TEXT_COLUMN_>, <_CLEANED_TEXT_COL_NAME>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-c19fc26e-be76-4033-bacb-97fed24f3609\" \n",
       "            onclick=\"_export_df('c19fc26e-be76-4033-bacb-97fed24f3609')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"c19fc26e-be76-4033-bacb-97fed24f3609\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>BASIC_CLEAN_TEXT</th>\n",
       "      <th>SPACY_CLEAN_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We have a client that wants to order a Platinu...</td>\n",
       "      <td>client want order platinum american express st...</td>\n",
       "      <td>{'word_list': ['we', 'have', 'a', 'client', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i stop process of spousal continuation ...</td>\n",
       "      <td>how do stop process spousal continuation annuity</td>\n",
       "      <td>{'word_list': ['how', 'do', 'i', 'stop', 'proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can reg D forms be sent via US Mail</td>\n",
       "      <td>can reg d form send via us mail</td>\n",
       "      <td>{'word_list': ['can', 'reg', 'd', 'forms', 'be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is qualifying under Reg D only done digitally</td>\n",
       "      <td>qualify reg d do digitally</td>\n",
       "      <td>{'word_list': ['is', 'qualifying', 'under', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can you do an ira to roth conversion inkind</td>\n",
       "      <td>can do ira roth conversion inkind</td>\n",
       "      <td>{'word_list': ['can', 'you', 'do', 'an', 'ira'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT                                   BASIC_CLEAN_TEXT                                   SPACY_CLEAN_TEXT\n",
       "0  We have a client that wants to order a Platinu...  client want order platinum american express st...  {'word_list': ['we', 'have', 'a', 'client', 't...\n",
       "1  how do i stop process of spousal continuation ...   how do stop process spousal continuation annuity  {'word_list': ['how', 'do', 'i', 'stop', 'proc...\n",
       "2                can reg D forms be sent via US Mail                    can reg d form send via us mail  {'word_list': ['can', 'reg', 'd', 'forms', 'be...\n",
       "3      Is qualifying under Reg D only done digitally                         qualify reg d do digitally  {'word_list': ['is', 'qualifying', 'under', 'r...\n",
       "4        can you do an ira to roth conversion inkind                  can do ira roth conversion inkind  {'word_list': ['can', 'you', 'do', 'an', 'ira'..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cleaning(df, 'TEXT', 'BASIC_CLEAN_TEXT')\n",
    "df = cleaning(df, 'TEXT', 'SPACY_CLEAN_TEXT')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods used: \n",
    "    \n",
    "1. Spacy preprocessing + TFIDF vectorization + KMeans Algorithm (iterative)\n",
    "2. Basic preprocessing + TFIDF vectorization + KMeans Algorithm (iterative)\n",
    "3. HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spacy preprocessing + TFIDF vectorization + KMeans Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "code_folding": [
     5,
     25,
     51,
     68,
     69,
     72,
     91,
     136,
     147,
     195,
     270,
     284,
     288,
     293,
     305,
     318,
     352,
     402,
     424,
     482,
     608,
     629,
     747,
     805,
     984
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#!/ms/dist/python/PROJ/core/3.7.1/bin/python\n",
    "\n",
    "###########################################\n",
    "# classes for parallel processing\n",
    "###########################################\n",
    "class Worker(multiprocessing.Process):\n",
    "    def __init__(self, task_queue, result_queue):\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.task_queue = task_queue\n",
    "        self.result_queue = result_queue\n",
    "    def run(self):\n",
    "        proc_name = self.name\n",
    "        while True:\n",
    "            next_task = self.task_queue.get()\n",
    "            if next_task is None:\n",
    "                # Poison pill means shutdown\n",
    "                # print(\"%s: Exiting\"%proc_name)\n",
    "                self.task_queue.task_done()\n",
    "                break\n",
    "            # print(\"Worker run -- %s: %s\" % (proc_name, next_task))\n",
    "            answer = next_task()\n",
    "            self.task_queue.task_done()\n",
    "            self.result_queue.put(answer)\n",
    "        return\n",
    "\n",
    "class Task(object):\n",
    "    def __init__(self, task_index, split, sentence_pairs, sentences):\n",
    "        self.task_index = task_index\n",
    "        self.split = split\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.sentences = sentences\n",
    "    def __call__(self):\n",
    "        task_results = []\n",
    "        for index, sentence_pair in enumerate(self.sentence_pairs):\n",
    "            sentence1 = self.sentences[sentence_pair[0]]\n",
    "            sentence2 = self.sentences[sentence_pair[1]]\n",
    "            sentence1_length = len(sentence1)\n",
    "            sentence2_length = len(sentence2)\n",
    "            # compute FuzzyWuzzy similarity\n",
    "            similarity = min(fuzz.ratio(self.sentences[sentence_pair[0]], self.sentences[sentence_pair[1]]),\n",
    "                             fuzz.ratio(self.sentences[sentence_pair[1]], self.sentences[sentence_pair[0]]))\n",
    "            task_results.append({\"sentence_pair\": sentence_pair, \"similarity\": similarity})\n",
    "        # print(\"task %s done with %s results\" % (self.task_index, len(task_results)))\n",
    "        return task_results\n",
    "    def __str__(self):\n",
    "        return \"Task index %s for %s sentences\" % (self.task_index, len(self.split))\n",
    "###########################################\n",
    "\n",
    "\n",
    "class query_clustering():\n",
    "\n",
    "    def __init__(self, resource_dir):\n",
    "        self.stopwd_file = os.path.join(resource_dir, \"stopwords.txt\")\n",
    "        self.synonym_file = os.path.join(resource_dir, \"synonyms_noun_verb.txt\")\n",
    "        self.clustering_type = \"noun\"\n",
    "        self.kmeans_rate = KMEANS_RATE\n",
    "        self.cluster_k = CLUSTER_K\n",
    "        self.kmeans_seed_init = KMEANS_SEED_INIT\n",
    "        self.kmeans_maxiter = KMEANS_MAXITER\n",
    "        self.cluster_length = CLUSTER_LEN\n",
    "        self.cohesion_threshold = COHESION_THRESHOLD\n",
    "        self.stopwords = []\n",
    "        self.synonym_dict = {}\n",
    "        self.load_stop_words()\n",
    "        self.load_syn_verb_dict()\n",
    "        # print input text\n",
    "        self.test_mode = \"no\"\n",
    "\n",
    "    def load_stop_words(self):\n",
    "        with io.open(self.stopwd_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "\n",
    "    def load_syn_verb_dict(self):\n",
    "\n",
    "        synonym_list = []\n",
    "\n",
    "        with io.open(self.synonym_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            synonym_list = [x.rstrip() for x in f.readlines()]\n",
    "            for line in synonym_list:\n",
    "                if len(line) < 4 or re.search(\"^#\", line):\n",
    "                    continue\n",
    "\n",
    "                syn_words = line.lower().split(\"\\t\")\n",
    "                pos = syn_words[0]\n",
    "                hwd = syn_words[1]\n",
    "                syn_words.pop(0)\n",
    "                syn_words.pop(0)\n",
    "\n",
    "                for s in syn_words:\n",
    "                    self.synonym_dict.setdefault(s, {})[pos] = hwd\n",
    "\n",
    "    def get_pos_list(self, results):\n",
    "        pos_ans = []\n",
    "        word_list = []\n",
    "        lemma_list = []\n",
    "        ne_list = []\n",
    "        start_end_list = []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                pos_syn = \"\"\n",
    "                pos = token['pos'].lower()\n",
    "                pos_ans.append(token['pos'])\n",
    "                word_list.append(token['word'])\n",
    "                lemma = token['lemma'].lower()\n",
    "\n",
    "                if lemma in self.stopwords and lemma not in ['want', 'against', 'further', 'online', 'same', 'under',\n",
    "                                                             'what', 'want', 'when', 'own'] \\\n",
    "                        or lemma in [\":\", \"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"\\\\\", '-pron-', '_', 'card num', '\"'] \\\n",
    "                        or pos == \":\" or pos == \".\" or re.search('^([\\W]*)$]', lemma) or len(lemma) >= 30:\n",
    "                    continue\n",
    "\n",
    "                if re.search('^nn', pos):\n",
    "                    pos_syn = 'noun'\n",
    "                elif re.search('^v', pos):\n",
    "                    pos_syn = 'verb'\n",
    "                elif re.search('^adj', pos):\n",
    "                    pos_syn = 'adj'\n",
    "\n",
    "                if lemma in list(self.synonym_dict.keys()) and \\\n",
    "                        pos_syn in list(self.synonym_dict[lemma].keys()):\n",
    "                    lemma = self.synonym_dict[lemma][pos_syn]\n",
    "\n",
    "                lemma_list.append(lemma)\n",
    "\n",
    "                ne_list.append(token['ner'])\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        # print(pos_ans); print(word_list); print(lemma_list); print(ne_list)\n",
    "        if len(lemma_list) == 0:\n",
    "            sent_lemma = \"NO_NP_KEYS\"\n",
    "        else:\n",
    "            sent_lemma = \" \".join(lemma_list)\n",
    "\n",
    "        return \" \".join(word_list), sent_lemma, \" \", \" \"\n",
    "\n",
    "    def get_sent_lemma(self, sent):\n",
    "        # allowed spacy attributes\n",
    "        props_str = [{'value': 'false', 'key': 'enforceRequirements'},\n",
    "                     {'value': 'json', 'key': 'outputFormat'},\n",
    "                     {'value': 'normalizeSpace=false, strictTreebank3=true', 'key': 'tokenize.options'},\n",
    "                     {'value': 'tokenize,ssplit,pos,lemma,ner', 'key': 'annotators'},\n",
    "                     {'value': 'true', 'key': 'ssplit.eolonly'}]\n",
    "        input_json = {'text': sent, 'props': props_str}\n",
    "        results = run_pipeline(input_json)\n",
    "        return self.get_pos_list(results)\n",
    "\n",
    "    def run_doc_clustering(self, documents, k_val):\n",
    "\n",
    "        k_val = int(k_val)\n",
    "        vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 1))\n",
    "\n",
    "        X = vectorizer.fit_transform(documents)\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        if self.clustering_type == \"fixed\":\n",
    "            true_k = 2000  # 1078\n",
    "        elif self.clustering_type == \"noun\":\n",
    "            true_k = int(np.sqrt(len(documents) / 2)) * self.kmeans_rate\n",
    "        # true_k = 800\n",
    "        elif self.clustering_type == \"verb\":\n",
    "            true_k = int(np.sqrt(len(documents) / 2))\n",
    "        else:\n",
    "            print(\"Please set up clustering type!\")\n",
    "\n",
    "        max_iter = self.kmeans_maxiter\n",
    "        # n_init = self.kmeans_seed_init\n",
    "\n",
    "        if k_val > 0 and self.clustering_type == \"noun\":\n",
    "            # print(len(documents)/2, k_val)\n",
    "            if len(documents) / 2 < k_val:\n",
    "                print(\"Number of input lines:\", len(documents))\n",
    "                print(\"K value provided:\", k_val)\n",
    "                print(\"Suggested value of K for the first level clustering is smaller than input lines divided by 2. Please try again!\")\n",
    "                print(\"Heusristic value of K if line number > 100:\", int(np.sqrt(len(documents) / 2)) * self.kmeans_rate)\n",
    "                exit()\n",
    "            else:\n",
    "                true_k = k_val\n",
    "\n",
    "        if true_k == 0:\n",
    "            true_k = 1\n",
    "\n",
    "        ninit = 50\n",
    "        r_num = len(documents) - 1\n",
    "\n",
    "        model = KMeans(n_clusters=true_k, init='k-means++', max_iter=2000, random_state=r_num, n_init=ninit)\n",
    "        model.fit(X)\n",
    "\n",
    "        return model, terms, X\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Utility functions\n",
    "############################################\n",
    "\n",
    "# Execute Spacy Pipeline\n",
    "def run_pipeline(input_json):\n",
    "    \"\"\"\n",
    "    Runs Spacy pipeline specified by params.\n",
    "    param: input_json = {'text': sentence, 'props': props_str}\n",
    "    return: output_json\n",
    "    \"\"\"\n",
    "    # Get all parameters from input JSON\n",
    "    text = input_json[\"text\"]\n",
    "    for prop in input_json[\"props\"]:\n",
    "        if prop[\"key\"] == \"annotators\":\n",
    "            operations = prop[\"value\"].split(\",\")\n",
    "\n",
    "    # Run Spacy Pipeline\n",
    "    doc = spacy_model(text)\n",
    "    doc_json = doc.to_json()  # Includes all info from spacy pipeline\n",
    "\n",
    "    output_json = {}\n",
    "    output_json[\"sentences\"] = []\n",
    "\n",
    "    # a. create necessary dictionaries\n",
    "    # a.1 Extract Entity List\n",
    "    entity_list = doc_json[\"ents\"]\n",
    "    # a.2 create token lib\n",
    "    token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "    # b. add sentence indices\n",
    "    for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "        out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "        parse = \"\"\n",
    "        basicDependencies = []\n",
    "        output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "        # c. split sentences by indices, add labels (pos, ner, dep, etc.)\n",
    "        for token in doc_json[\"tokens\"]:\n",
    "            if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                # Extract Entity label\n",
    "                ner = \"O\"\n",
    "                for entity in entity_list:\n",
    "                    if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                        ner = entity[\"label\"]\n",
    "                # Extract dependency info\n",
    "                dep = token[\"dep\"]\n",
    "                governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]: token_lib[token[\"head\"]][\"end\"]]\n",
    "                dependent = token[\"id\"] + 1\n",
    "                dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "                lemma = doc[token[\"id\"]].lemma_\n",
    "                # d. add dependencies\n",
    "                basicDependencies.append({\"dep\": dep,\n",
    "                                          \"governor\": governor,\n",
    "                                          \"governorGloss\": governorGloss,\n",
    "                                          \"dependent\": dependent,\n",
    "                                          \"dependentGloss\": dependentGloss})\n",
    "                # e. add tokens\n",
    "                out_token = {\"index\": token[\"id\"] + 1,\n",
    "                             \"word\": dependentGloss,\n",
    "                             \"originalText\": dependentGloss,\n",
    "                             \"characterOffsetBegin\": token[\"start\"],\n",
    "                             \"characterOffsetEnd\": token[\"end\"]}\n",
    "                if \"lemma\" in operations:\n",
    "                    out_token[\"lemma\"] = lemma\n",
    "                if \"pos\" in operations:\n",
    "                    out_token[\"pos\"] = token[\"tag\"]\n",
    "                if \"ner\" in operations:\n",
    "                    out_token[\"ner\"] = ner\n",
    "                out_sentence[\"tokens\"].append(out_token)\n",
    "        if \"parse\" in operations:\n",
    "            out_sentence[\"parse\"] = parse\n",
    "            out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "            out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "            out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "\n",
    "    return output_json\n",
    "\n",
    "\n",
    "def fileToString(filename, exclude_comments=True, keep_newline=False):\n",
    "    string_result = ''\n",
    "    with open(filename, mode='r') as f:\n",
    "        if exclude_comments:\n",
    "            for line in f:\n",
    "                if not line.strip().startswith(\"#\"):\n",
    "                    if keep_newline:\n",
    "                        string_result += line\n",
    "                    else:\n",
    "                        string_result += (line.strip() + ' ')\n",
    "            return string_result\n",
    "        else:\n",
    "            return f.read()\n",
    "\n",
    "def fileToJson(filename, exclude_comments=True):\n",
    "    string_rep = fileToString(filename, exclude_comments=exclude_comments)\n",
    "    return json.loads(string_rep)\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('utf-8')\n",
    "    return ftfy.fix_text(text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    for id in list(data.keys()):\n",
    "        if data[id][\"keep\"] == \"no\":\n",
    "            del data[id]\n",
    "    #sort data\n",
    "    new_data = {}\n",
    "    idx = 0\n",
    "    for id, x in list(data.items()):\n",
    "        new_data[idx] = data[id]\n",
    "        idx += 1\n",
    "    return new_data\n",
    "\n",
    "def remove_no_keys(data):\n",
    "    data_tmp = {}\n",
    "    index = 0\n",
    "    no_key_count = 0\n",
    "    for id in list(data.keys()):\n",
    "        if data[id][\"sent_lemma\"] != \"NO_NP_KEYS\":\n",
    "            data_tmp[index] = data[id]\n",
    "            index = index + 1\n",
    "        else:\n",
    "            # print(\"Number removed:\",data[id][\"sent_raw\"])\n",
    "            no_key_count = no_key_count + 1\n",
    "    return data_tmp\n",
    "\n",
    "def calculate_duplicate(data):\n",
    "    ids = list(data.keys())\n",
    "\n",
    "    case_threshold = 0.5\n",
    "\n",
    "    if len(ids) == 0:\n",
    "        return data\n",
    "    ids.pop(0)\n",
    "\n",
    "    for id in list(data.keys()): # 0, 1, 2, n-1\n",
    "        s_index = id\n",
    "\n",
    "        data.setdefault(id, {})[\"index_set\"] = set()\n",
    "\n",
    "        data[id][\"index_set\"].add(id)\n",
    "        for id_sub in ids:\n",
    "            max_rate = data[id][\"case_ratio\"]\n",
    "            #data[id][\"index_set\"].add(id)\n",
    "            if data[id][\"count\"] != 0 and data[id_sub][\"count\"] != 0 and \\\n",
    "               data[id][\"sent_norm\"] == data[id_sub][\"sent_norm\"]:\n",
    "\n",
    "                if data[id_sub][\"case_ratio\"] > max_rate and data[id_sub][\"case_ratio\"] < case_threshold:\n",
    "                    max_rate = data[id_sub][\"case_ratio\"]\n",
    "                    s_index = id_sub\n",
    "\n",
    "                data[id][\"count\"] += 1\n",
    "                data[id][\"index_set\"].add(id_sub)\n",
    "                data[id_sub][\"count\"] = 0\n",
    "        if ids != []:\n",
    "            ids.pop(0)\n",
    "        if data[id][\"count\"] != 0:\n",
    "            data[id][\"index\"] = s_index\n",
    "    return data\n",
    "\n",
    "def cluster_center_sent(clustering, cluster_set_by_sid, data):\n",
    "    document_center = []\n",
    "    sid_mapping_center = {}\n",
    "    sid_tmp = 0\n",
    "    sent_num_cid = len(cluster_set_by_sid)\n",
    "\n",
    "    dup_sid_set = []\n",
    "    for sid in cluster_set_by_sid:\n",
    "        sid_mapping_center[sid_tmp] = sid\n",
    "        if len(data[sid]['sent_lemma']) > 0:\n",
    "            dup_sid_set.append(sid)\n",
    "        document_center.append(data[sid]['sent_lemma'])\n",
    "        sid_tmp +=1\n",
    "\n",
    "    if len(dup_sid_set) == 1:\n",
    "        seedid1 = list(dup_sid_set)[0]\n",
    "        return seedid1, seedid1\n",
    "\n",
    "    # clustering includes duplcated ones\n",
    "    model_center, terms_center, X_center = clustering.run_doc_clustering(document_center, 1)\n",
    "    cluster_centers_center  = model_center.cluster_centers_ # added\n",
    "    DISTMIN = -9999\n",
    "\n",
    "    values = []\n",
    "    values_sid_set = []\n",
    "    # remove duplicated ones\n",
    "    for i in range(0,sent_num_cid):\n",
    "        sid_data = sid_mapping_center[i]\n",
    "        if data[sid_data][\"count\"] > 0:\n",
    "            d_vec = X_center[i].toarray()\n",
    "            #values.append(spatial.distance.euclidean(d_vec, cluster_centers_center[0,:]))\n",
    "            if math.isnan(spatial.distance.cosine(d_vec, cluster_centers_center[0,:])):\n",
    "                values.append(1)\n",
    "            else:\n",
    "                values.append(1 - spatial.distance.cosine(d_vec, cluster_centers_center[0,:]))\n",
    "            #score = 1 - spatial.distance.cosine(d_vec, cluster_centers[int(cid),:]) #mainid\n",
    "            values_sid_set.append(sid_data)\n",
    "\n",
    "    seedid1_tmp = values.index(max(values))\n",
    "    min_dist1 = values[seedid1_tmp]\n",
    "    values[seedid1_tmp] = DISTMIN\n",
    "    seedid1 = values_sid_set[ seedid1_tmp ]\n",
    "\n",
    "    seedid2_tmp = values.index(max(values))\n",
    "    min_dist2 = values[seedid2_tmp]\n",
    "    seedid2 = values_sid_set[ seedid2_tmp ]\n",
    "    values[seedid1_tmp] = min_dist1 # recover weight\n",
    "\n",
    "    return seedid1, seedid2, values, values_sid_set\n",
    "\n",
    "def cluster_cohesion_cosine(clustering, cluster_set_by_sid, data):\n",
    "    document_center = []\n",
    "    values = []\n",
    "\n",
    "    for sid in cluster_set_by_sid:\n",
    "        document_center.append(data[sid]['sent_lemma'])\n",
    "\n",
    "    model_center, terms_center, X_center = clustering.run_doc_clustering(document_center, 1)\n",
    "    cluster_centers_center  = model_center.cluster_centers_\n",
    "\n",
    "    for i in range(0, len(cluster_set_by_sid)):\n",
    "        d_vec = X_center[i].toarray()\n",
    "        simval = spatial.distance.cosine(d_vec, cluster_centers_center[0,:])\n",
    "        if math.isnan(simval):\n",
    "            values.append(0)\n",
    "        else:\n",
    "            values.append(1 - simval)\n",
    "    if len(values) ==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(values) / len(values)\n",
    "\n",
    "def pairwiseCohesionScore(data_list):\n",
    "    sentences = data_list\n",
    "\n",
    "    # get the sentence index pairs\n",
    "    sentence_pairs = []\n",
    "    perm = list(permutations(list(range(len(sentences))), 2))\n",
    "    for (i, j) in perm:\n",
    "        # remove duplicated pairs: e.g., given (i,j) and (j,i), keep only one\n",
    "        if i < j:\n",
    "            sentence_pairs.append((i, j))\n",
    "\n",
    "    # Establish communication queues\n",
    "    tasks = multiprocessing.JoinableQueue()\n",
    "    results = multiprocessing.Queue()\n",
    "    # start workers by having 75% of all available cores\n",
    "    num_workers = int(multiprocessing.cpu_count() * 0.60)\n",
    "    # print(\"Creating %d workers\"%num_workers)\n",
    "    workers = [Worker(tasks, results) for i in range(num_workers)]\n",
    "    for w in workers:\n",
    "        w.start()\n",
    "    # split the job based on the number of workers\n",
    "    work_split = np.array_split(list(range(len(sentence_pairs))), num_workers)\n",
    "    # print(\">>> work split:\", work_split)\n",
    "    job_number = 0\n",
    "    for i, split in enumerate(work_split):\n",
    "        if len(split) > 0:\n",
    "            task = Task(i, split, sentence_pairs[split[0]:split[-1] + 1], sentences)\n",
    "            tasks.put(task)\n",
    "            job_number += 1\n",
    "        else:\n",
    "            continue\n",
    "    # print(\">>> no job is given to task %s with split %s\"%(i, split))\n",
    "\n",
    "    # Add a poison pill for each worker so that they will finish when the task is complete\n",
    "    for i in range(num_workers):\n",
    "        tasks.put(None)\n",
    "\n",
    "    # Wait for all of the tasks to finish\n",
    "    tasks.join()\n",
    "\n",
    "    # get results from workers\n",
    "    job_results = []\n",
    "    while job_number:\n",
    "        # print(\"%s job(s) to go\"%job_number)\n",
    "        result = results.get()\n",
    "        job_results.extend(result)\n",
    "        job_number -= 1\n",
    "\n",
    "    sorted_job_results = sorted(job_results, key=lambda x: x['sentence_pair'])\n",
    "    total_pairs = len(sorted_job_results)\n",
    "    similarity_sum = 0\n",
    "\n",
    "    for x in sorted_job_results:\n",
    "        index1 = x['sentence_pair'][0]\n",
    "        index2 = x['sentence_pair'][1]\n",
    "        similarity_sum += x['similarity']\n",
    "    return similarity_sum / total_pairs\n",
    "\n",
    "def pairwiseSimCal(data_list, cluster_set, operation):\n",
    "\n",
    "    clusters = {}\n",
    "    cluster_set.clear()\n",
    "\n",
    "    if operation == 'copy':\n",
    "        cluster_set[0] = list(range(len(data_list)))\n",
    "        return\n",
    "    elif operation == 'split':\n",
    "        for index in range(len(data_list)):\n",
    "            cluster_set[index] = [index]\n",
    "        return\n",
    "    sentences = data_list\n",
    "    # get the sentence index pairs\n",
    "    sentence_pairs = []\n",
    "    perm = list(permutations(list(range(len(sentences))), 2))\n",
    "    for (i, j) in perm:\n",
    "        # remove duplicated pairs: e.g., given (i,j) and (j,i), keep only one\n",
    "        if i < j:\n",
    "            sentence_pairs.append((i, j))\n",
    "\n",
    "    # Establish communication queues\n",
    "    tasks = multiprocessing.JoinableQueue()\n",
    "    results = multiprocessing.Queue()\n",
    "\n",
    "    # start workers by having 75% of all available cores\n",
    "    num_workers = int(multiprocessing.cpu_count() * 0.60)\n",
    "    # print(\"Creating %d workers\"%num_workers)\n",
    "    workers = [Worker(tasks, results) for i in range(num_workers)]\n",
    "\n",
    "    for w in workers:\n",
    "        w.start()\n",
    "\n",
    "    # split the job based on the number of workers\n",
    "    work_split = np.array_split(list(range(len(sentence_pairs))), num_workers)\n",
    "    # print(\">>> work split:\", work_split)\n",
    "\n",
    "    job_number = 0\n",
    "\n",
    "    for i, split in enumerate(work_split):\n",
    "        if len(split) > 0:\n",
    "            task = Task(i, split, sentence_pairs[split[0]:split[-1] + 1], sentences)\n",
    "            tasks.put(task)\n",
    "            job_number += 1\n",
    "        else:\n",
    "            continue\n",
    "    # print(\">>> no job is given to task %s with split %s\"%(i, split))\n",
    "\n",
    "    # Add a poison pill for each worker so that they will finish when the task is complete\n",
    "    for i in range(num_workers):\n",
    "        tasks.put(None)\n",
    "\n",
    "    # Wait for all of the tasks to finish\n",
    "    tasks.join()\n",
    "\n",
    "    # get results from workers\n",
    "    job_results = []\n",
    "\n",
    "    while job_number:\n",
    "        # print(\"%s job(s) to go\"%job_number)\n",
    "        result = results.get()\n",
    "        job_results.extend(result)\n",
    "        job_number -= 1\n",
    "\n",
    "    # show the combined results\n",
    "    sorted_job_results = sorted(job_results, key=lambda x: x['sentence_pair'])\n",
    "\n",
    "    similarity_results = []\n",
    "    count = 0\n",
    "    for x in sorted_job_results:\n",
    "        index1 = x['sentence_pair'][0]\n",
    "        index2 = x['sentence_pair'][1]\n",
    "\n",
    "        sentence1 = sentences[index1]\n",
    "        sentence2 = sentences[index2]\n",
    "\n",
    "        similarity = x['similarity']\n",
    "        # print(\"threshold_sim set for similarity computation =\", threshold_sim)\n",
    "        if similarity > threshold_sim:\n",
    "            flag = 0\n",
    "            for i in range(count):\n",
    "                if i in clusters and (index1 in clusters[i] or index2 in clusters[i]):\n",
    "                    clusters[i].add(index1)\n",
    "                    clusters[i].add(index2)\n",
    "                    flag = 1\n",
    "                    break\n",
    "\n",
    "            if flag == 0:\n",
    "                clusters[count] = set()\n",
    "                clusters[count].add(index1)\n",
    "                clusters[count].add(index2)\n",
    "                count += 1\n",
    "\n",
    "            # print(\"%s\\t%s\\t%s -- [%s] vs. [%s]\"%(index1, index2, similarity, sentence1, sentence2))\n",
    "            similarity_results.append({'index1': index1, 'index2': index2, 'similarity': similarity, 'sentence1': sentence1, 'sentence2': sentence2})\n",
    "        else:\n",
    "            clusters[count] = set()\n",
    "            clusters[count].add(index1)\n",
    "            count += 1\n",
    "            clusters[count] = set()\n",
    "            clusters[count].add(index2)\n",
    "            count += 1\n",
    "\n",
    "    # filter and merge\n",
    "    for index in list(clusters.keys()):\n",
    "        if bool(clusters[index]):\n",
    "            for i in range(index + 1, len(list(clusters.keys()))):\n",
    "                if clusters[index].intersection(clusters[i]):\n",
    "                    clusters[index] = clusters[index].union(clusters[i])\n",
    "                    clusters[i] = set()\n",
    "\n",
    "    cluster_count = 0\n",
    "    for index in list(clusters.keys()):\n",
    "        if bool(clusters[index]):\n",
    "            cluster_count += 1\n",
    "            cluster_set[index] = []\n",
    "            for e in clusters[index]:\n",
    "                cluster_set[index].append(e)  # sid?\n",
    "############################################\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Normal Clustering\n",
    "# This was implemented by using tf and idf measure with terms. It is of two level processing: (1) top-down kmeans clustering; (2) Bottom-up\n",
    "# clustering with the results from the first step. Pick up clusters with element more than 10 for clustering wiht kmeans.\n",
    "# Calculate cohesion scores to final clusters as reference of the quality.\n",
    "def run_query_clustering(df, cluster_k):\n",
    "    \"\"\"\n",
    "    Run k-means clustering in two levels.\n",
    "    :param: df - input data\n",
    "    :param: cluster_k - if cluster_k > 0, then use this value, otherwise, calculate with heustistics\n",
    "    :return: output df\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    clustering = query_clustering(resources_dir_path)\n",
    "\n",
    "    if cohesion_threshold > 0:\n",
    "        clustering.cohesion_threshold = cohesion_threshold\n",
    "\n",
    "    data = {}\n",
    "    idx = 0\n",
    "    documents = []\n",
    "    dup_counted_file = \"no\"\n",
    "    db_id = 1\n",
    "    dup_count = 1\n",
    "    \n",
    "    for input_text in df[col_TEXT].values:\n",
    "        if len(input_text) < 3 or re.search(\"^#\",input_text) or re.search(\"^dup_cnt\",input_text.lower()):\n",
    "            continue\n",
    "        \n",
    "        input_text = str(input_text).strip()\n",
    "        input_text = sanitize_text(input_text)\n",
    "        input_text = str(input_text).strip()\n",
    "        input_text = re.sub('[\\W]*$','',input_text)\t\t# remove ending non-alpha-numeric chars\n",
    "\n",
    "        if clustering.test_mode == 'yes':\n",
    "            print(idx,\"\\t\",input_text)\n",
    "        filter_stopwds = True\n",
    "        sent_tokens, sent_lemma, sent_verb, sent_dup = clustering.get_sent_lemma(input_text)  #all\n",
    "\n",
    "        upper_cnt = sum(1 for c in input_text if c.isupper())\n",
    "        case_ratio = float(upper_cnt) / float(len(input_text))\n",
    "\n",
    "        data.setdefault(idx, {})[\"sent_lemma\"] = str(sent_lemma)\n",
    "        data.setdefault(idx, {})[\"sent_noun\"] = str(sent_lemma)\n",
    "        data.setdefault(idx, {})[\"sent_verb\"] = str(sent_verb)\n",
    "        data.setdefault(idx, {})[\"sent_tokens\"] = sent_tokens\n",
    "        data.setdefault(idx, {})[\"sent_norm\"] = input_text.lower()\n",
    "        data.setdefault(idx, {})[\"sent_raw\"] = input_text\n",
    "        data.setdefault(idx, {})[\"count\"] = int(dup_count)  # from dup counted file: 1\n",
    "        data.setdefault(idx, {})[\"case_ratio\"] = case_ratio\n",
    "        data.setdefault(idx, {})[\"clusterid\"] = \"no\"\n",
    "        data.setdefault(idx, {})[\"keep\"] = \"yes\"\n",
    "        data.setdefault(idx, {})[\"select\"] = \"no\"\n",
    "        data.setdefault(idx, {})[\"index\"] = idx  # pointing to itself when loading with dup counted file.\n",
    "        data.setdefault(idx, {})[\"db_id\"] = db_id  # pointing to itself when loading with dup counted file.\n",
    "\n",
    "        idx += 1\n",
    "        if idx % 1000 == 0:\n",
    "            print(\"Count of lemmatization:\", idx)\n",
    "            print(\"Time:\",time.time()- start,\" sec\")\n",
    "            start = time.time()\n",
    "\n",
    "    # remove duplicated sentences\n",
    "    # data = remove_duplicates(data)\n",
    "    # data = remove_no_keys(data)\n",
    "    if dup_counted_file == \"no\":\n",
    "        data = calculate_duplicate(data)\n",
    "\n",
    "    # include all sentences for clustering: multiple same sentences in the collection can change idf values\n",
    "    documents =[]\n",
    "    main_docid_id_map = {}  # id in documents vs real id in input\n",
    "    doc_id = 0\n",
    "    for id, x in list(data.items()):\n",
    "        if data[id][\"count\"] >= 0:\n",
    "            main_docid_id_map[doc_id] = id\n",
    "            documents.append(data[id]['sent_lemma'])\n",
    "            doc_id += 1\n",
    "\n",
    "    print(\"Calculating query clusters!\")\n",
    "    if len(documents) < 10:\n",
    "        print(\"The file contains sentences less than 10! Please check  and run again!\", documents)\n",
    "        exit()\n",
    "\n",
    "    ## 1st CLUSTERING  ##\n",
    "    clustering.clustering_type = \"noun\"\n",
    "    model, terms, X = clustering.run_doc_clustering(documents, cluster_k)\n",
    "\n",
    "    cluster_labels  = model.labels_\n",
    "    cluster_centers  = model.cluster_centers_\n",
    "\n",
    "    c_s_list = [(y,x) for x,y in enumerate(cluster_labels)]\n",
    "\n",
    "    cid_sid_counter = {}\n",
    "    for cid, sid in c_s_list:\n",
    "        main_sid = main_docid_id_map[sid]\n",
    "        data.setdefault(main_sid, {})[\"clusterid\"] = str(format(cid,\"05d\"))\n",
    "        cid_sid_counter.setdefault(cid, []).append(main_sid)\n",
    "\n",
    "    clustering.clustering_type = \"noun\"\n",
    "    cohesionThreshold = clustering.cohesion_threshold\n",
    "    for cid in sorted(cid_sid_counter.keys()):\n",
    "\n",
    "        documents_sub =[]\n",
    "        doc_id_sid_index = {}\n",
    "        for sid in cid_sid_counter[cid]:\n",
    "            documents_sub.append(data[sid]['sent_lemma'])\n",
    "\n",
    "        if len(documents_sub) > clustering.cluster_length:\n",
    "            cohesion_score = cluster_cohesion_cosine(clustering, cid_sid_counter[cid], data)\n",
    "\n",
    "            #### RE-CULSTERING\n",
    "            if cohesion_score < cohesionThreshold:  #### to use cohision value for reclustering\n",
    "                model_sub, terms, X_sub = clustering.run_doc_clustering(documents_sub,cluster_k)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        cluster_labels_sub = model_sub.labels_\n",
    "        c_s_list_sub = [(y,x) for x,y in enumerate(cluster_labels_sub)]\n",
    "        for cid_sub,sid_sub in c_s_list_sub:\n",
    "            # print(\"cid, sid:\",cid_sub,sid_sub)\n",
    "            sid_index = cid_sid_counter[cid][sid_sub]\n",
    "            data[sid_index]['clusterid'] = data[sid_index]['clusterid'] + \"_\"+str(format(cid_sub,\"03d\"))\n",
    "            # cid_sid_counter_sub.setdefault(cid_sub, []).append(sid_index)\n",
    "\n",
    "    clustering.clustering_type = \"noun\"  # reset to first level\n",
    "\n",
    "    # Output\n",
    "    output_data = []\n",
    "    convSeqId = 1\n",
    "    cluster_set_by_sid = {}\n",
    "    for id, x in list(data.items()):\n",
    "        if data[id]['clusterid'] != \"no\":\n",
    "            cid = data[id]['clusterid']\n",
    "            cluster_set_by_sid.setdefault(cid, []).append(id)\n",
    "\n",
    "    output_data.append(\"Cluster_ID\\tText_ID\\tText\\tSeed_Q1\\tSeed_Q2\\tDist_to_Center\\tCohesion\\r\\n\")\n",
    "    \n",
    "    T = 0.0  # recommended: 0.4\n",
    "    cid_index = 0\n",
    "    cluster_set_by_sid_filtered = {}\n",
    "    cluster_set_by_sid_filtered_coh_score = {}\n",
    "    for cid in list(cluster_set_by_sid.keys()):\n",
    "        if len(cluster_set_by_sid[cid]) > 1:\n",
    "            cohesion_score = cluster_cohesion_cosine(clustering, cluster_set_by_sid[cid], data)\n",
    "            if cohesion_score < T:\n",
    "                for s in cluster_set_by_sid[cid]:\n",
    "                    cluster_set_by_sid_filtered.setdefault(cid_index,[]).append(s)\n",
    "                    cid_index += 1\n",
    "            else:\n",
    "                cluster_set_by_sid_filtered[cid_index] = cluster_set_by_sid[cid]\n",
    "                cluster_set_by_sid_filtered_coh_score[cid_index] = cohesion_score\n",
    "                cid_index += 1\n",
    "        else:\n",
    "            cluster_set_by_sid_filtered[cid_index] = cluster_set_by_sid[cid]\n",
    "            cluster_set_by_sid_filtered_coh_score[cid_index] = 0\n",
    "            cid_index += 1\n",
    "    for cid in list(cluster_set_by_sid_filtered.keys()):\n",
    "        sent_num_cid = len(cluster_set_by_sid_filtered[cid])\n",
    "        values = []\n",
    "        values_sid = []\n",
    "\n",
    "        if sent_num_cid > 0 and len(cluster_set_by_sid_filtered[cid]) > 1:\n",
    "            seedid1,seedid2, values, values_sid = cluster_center_sent(clustering, cluster_set_by_sid_filtered[cid], data)\n",
    "        else:\n",
    "            seedid1 = cluster_set_by_sid_filtered[cid][0]\n",
    "            seedid2 = cluster_set_by_sid_filtered[cid][0]\n",
    "            values.append(1)\n",
    "            values_sid.append(seedid1)\n",
    "\n",
    "        seed_q1 = data[seedid1][\"sent_raw\"]\n",
    "        seed_q2 = data[seedid2][\"sent_raw\"]\n",
    "\n",
    "        sent_count = 0\n",
    "        for sid in cluster_set_by_sid_filtered[cid]:\n",
    "            sent_count += data[sid][\"count\"]\n",
    "        ch_score= cluster_set_by_sid_filtered_coh_score[cid]\n",
    "        for sid in cluster_set_by_sid_filtered[cid]:\n",
    "            dup_count = data[sid]['count']\n",
    "            if dup_count == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sid_norm= data[sid][\"index\"]\n",
    "                sent = data[sid_norm]['sent_raw']\n",
    "            if data[sid]['db_id'] != 'none':\n",
    "                db_seq_id = data[sid]['db_id']\n",
    "\n",
    "            sim_index = values_sid.index(sid)\n",
    "            dist_to_center = 1.0 - values[sim_index]\n",
    "\n",
    "            # cid, sid, dup_count, sent, seed_q1, seed_q2, dist_to_center, cohesion_score\n",
    "            output_data.append(\"%d\\t%s\\t%s\\t%s\\t%s\\t%5.3f\\t%5.3f\\r\\n\" % (cid, sid, sent, seed_q1, seed_q2, dist_to_center, ch_score))\n",
    "\n",
    "        cid_index += 1\n",
    "    print(\"\\nClustering process finished\\n\")\n",
    "    return output_data\n",
    "\n",
    "# Hybrid CLustering\n",
    "# This was implemented by using tf and idf measure with terms. It is of two level processing: (1) top-down kmeans clustering; (2) Further\n",
    "# clustering with the results from the first step using string similarity calculation.\n",
    "def run_query_clustering_hybrid(df, cluster_k):\n",
    "\n",
    "    clustering = query_clustering(resource_dir)\n",
    "    data = {}\n",
    "    idx = 0\n",
    "    for input_text in df[col_TEXT].values:\n",
    "        if len(input_text) < 3:\n",
    "            continue\n",
    "        \n",
    "        input_text = input_text.rstrip()\n",
    "        input_text = sanitize_text(input_text)\n",
    "        input_text = re.sub(' +',' ',input_text)\n",
    "        input_text = re.sub('^ +', '', input_text)\n",
    "        input_text = re.sub('[\\W]*$','',input_text)\n",
    "        input_text = str(input_text)\n",
    "\n",
    "        if clustering.test_mode == 'yes':\n",
    "            print(idx,\"\\t\",input_text)\n",
    "        filter_stopwds = True\n",
    "        sent_tokens, sent_lemma, sent_verb, sent_dup = clustering.get_sent_lemma(input_text)  #all\n",
    "\n",
    "        upper_cnt = sum(1 for c in input_text if c.isupper())\n",
    "        case_ratio = float(upper_cnt) / float(len(input_text))\n",
    "\n",
    "        data.setdefault(idx, {})[\"sent_lemma\"] = sent_lemma\n",
    "        data.setdefault(idx, {})[\"sent_noun\"] = sent_lemma\n",
    "        data.setdefault(idx, {})[\"sent_dup\"] = sent_dup\n",
    "        data.setdefault(idx, {})[\"sent_verb\"] = sent_verb\n",
    "        data.setdefault(idx, {})[\"sent_tokens\"] = sent_tokens\n",
    "        data.setdefault(idx, {})[\"case_ratio\"] = case_ratio\n",
    "        #data.setdefault(idx, {})[\"sent_tokens_rev\"] = sent_tokens_rev\n",
    "        data.setdefault(idx, {})[\"sent_norm\"] = input_text.lower()\n",
    "        data.setdefault(idx, {})[\"sent_raw\"] = input_text\n",
    "        data.setdefault(idx, {})[\"count\"] = 1\n",
    "        #data.setdefault(idx, {})[\"clusterid\"] = \"no\"\n",
    "        data.setdefault(idx, {})[\"keep\"] = \"yes\"\n",
    "        data.setdefault(idx, {})[\"select\"] = \"no\"\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    data = calculate_duplicate(data)\n",
    "\n",
    "    documents =[]\n",
    "\n",
    "    main_docid_id_map = {}  # id in documents vs real id in input\n",
    "    doc_id = 0\n",
    "    for id, x in list(data.items()):\n",
    "        if data[id]['count'] > 0:  # select one among the dup sent\n",
    "            documents.append(data[id]['sent_lemma'])\n",
    "            main_docid_id_map[doc_id] = id\n",
    "            doc_id += 1\n",
    "\n",
    "    print(\"Calculating query clusters!\")\n",
    "    if len(documents) > 1:\n",
    "        model, terms, X = clustering.run_doc_clustering(documents, cluster_k)\n",
    "    else:\n",
    "        print(\"There is no sentences to run clustering!\")\n",
    "        return\n",
    "    cluster_labels  = model.labels_\n",
    "    cluster_centers  = model.cluster_centers_\n",
    "\n",
    "    c_s_list = [(y,x) for x,y in enumerate(cluster_labels)]\n",
    "    cid_sid_counter = {}\n",
    "\n",
    "    # filter out noise sentences from clusters by threshold T. Mark them as \"noise\"\n",
    "    for cid, sid in c_s_list:\n",
    "        data_sid = main_docid_id_map[sid]\n",
    "        cid_sid_counter.setdefault(cid, []).append(sid)\n",
    "\n",
    "    # collecting new updated clusters\n",
    "    cluster_results = {}\n",
    "    count = 0\n",
    "    Cohesion_up  = 60\n",
    "    Cohesion_low  = 30\n",
    "    Cohesion_scores = []\n",
    "    for cid in sorted(cid_sid_counter.keys()): # first level\n",
    "        documents_sub =[]\n",
    "        sub_docid_id_map = {} # id to data_sid mapping for selected set\n",
    "        cluster_set = {}\n",
    "        id_index = 0\n",
    "        for sid in cid_sid_counter[cid]:\n",
    "            # documents_sub.append(data[sid]['sent_tokens'].lower())\n",
    "            data_sid = main_docid_id_map[sid]\n",
    "            documents_sub.append(data[data_sid]['sent_raw'].lower())\n",
    "            sub_docid_id_map[id_index] = data_sid\n",
    "            id_index += 1\n",
    "\n",
    "        if len(documents_sub) > 1:  # 10\n",
    "            cohesion_score = pairwiseCohesionScore(documents_sub)\n",
    "\n",
    "            if Cohesion_up > cohesion_score :\n",
    "                if Cohesion_low < cohesion_score:  ### Cohesion_low < score < Cohesion_up\n",
    "                    pairwiseSimCal(documents_sub, cluster_set, 'cluster') # case 1: clustering\n",
    "\n",
    "                    for cs in cluster_set: # sub_cluster\n",
    "                        doc_coh = []\n",
    "\n",
    "                        for ch in cluster_set[cs]:\n",
    "                            sid = sub_docid_id_map[ch]\n",
    "                            doc_coh.append(data[sid]['sent_raw'].lower())\n",
    "                        # measure cohesion score of each cluster\n",
    "                        if len(doc_coh) > 1:\n",
    "                            c_score = pairwiseCohesionScore(doc_coh)\n",
    "                            #print(\"cc2:\",c_score)\n",
    "                            Cohesion_scores.append(c_score)\n",
    "\n",
    "                else: ### score < Cohesion_low\n",
    "                    #print(\"remove\")\n",
    "                    pairwiseSimCal(documents_sub, cluster_set, 'split')\n",
    "            else: ### Cohesion_up < score\n",
    "                pairwiseSimCal(documents_sub, cluster_set, 'copy') # case 2: without clustering, copy only\n",
    "                #print(\"copy\",cid)\n",
    "                Cohesion_scores.append(cohesion_score)\n",
    "\n",
    "            for k in cluster_set:\n",
    "                cluster_results[count] =[]\n",
    "                for c_child in cluster_set[k]:\n",
    "                    sid = sub_docid_id_map[c_child]  #data_id\n",
    "                    cluster_results[count].append(sid) # ??? dup, keep sid\n",
    "                count += 1\n",
    "        elif len(documents_sub) == 1:\n",
    "            cluster_results[count] =[]\n",
    "            data_sid = main_docid_id_map[ cid_sid_counter[cid][0] ]\n",
    "            cluster_results[count].append(data_sid)\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    cluster_results_final_dup_count = {}  # count\n",
    "\n",
    "    for k in cluster_results:\n",
    "        dup_count = 0\n",
    "        for kk in cluster_results[k]:  #sid\n",
    "            dup_count += data[kk][\"count\"]\n",
    "        cluster_results_final_dup_count[k] = dup_count\n",
    "\n",
    "    output_data = []\n",
    "    output_data.append(\"Cluster ID\\tSent ID\\tDup_count\\tQuery\\tSeed_Q1\\tCount\\r\\n\")\n",
    "\n",
    "    for cid in cluster_results:\n",
    "        center_text_id = cluster_results[cid][0]\n",
    "        center_text = data[center_text_id][\"sent_raw\"].lower()\n",
    "        class_sent_count = cluster_results_final_dup_count[cid]\n",
    "\n",
    "        for sid in cluster_results[cid]:\n",
    "            dup_num = data[sid][\"count\"]\n",
    "            raw_text = data[sid][\"sent_raw\"].lower()\n",
    "            output_data.append(\"%s\\t%d\\t%d\\t%s\\t%s\\t%d\\r\\n\"%(cid, sid, dup_num, raw_text, center_text, class_sent_count) )\n",
    "\n",
    "    # overall cluster measurement\n",
    "    print(\"cohesion score:\", sum(Cohesion_scores)/float(len(Cohesion_scores)))\n",
    "    print(\"Clustering process finished!\")\n",
    "    return output_data\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "## MAIN ##\n",
    "###########################################\n",
    "\n",
    "# :: config ::\n",
    "col_TEXT = 'TEXT'\n",
    "    \n",
    "KMEANS_RATE = 5\n",
    "CLUSTER_K = 0\n",
    "KMEANS_SEED_INIT = 10\n",
    "KMEANS_MAXITER = 1000\n",
    "CLUSTER_LEN = 20\n",
    "COHESION_THRESHOLD = 0.35\n",
    "CLUSTER_LEN_2nd_LEVEL = 200\n",
    "CLUSTER_LEN_threshold = 2000\n",
    "OUTPUT_DUP = \"yes\"\n",
    "cohesion_threshold = 0\n",
    "threshold_sim = 60\n",
    "cluster_k = 0\n",
    "spacy_model = nlp\n",
    "\n",
    "\n",
    "def execute_clustering_method1(data, user_run_mode):\n",
    "    print(\"\\nQuery Clustering using Spacy, TFIDF and Iterative KMeans....\\n\")\n",
    "    \n",
    "    if user_run_mode == \"tfidf\":\n",
    "        # top-down (kmeans) and bottom-up (re-clustering of clusters with more than 10 members)\n",
    "        print(\"Running: tfidf-based system\")\n",
    "        output = run_query_clustering(data, 0)\n",
    "\n",
    "    elif user_run_mode == \"hybrid\":\n",
    "        # top-down (kmeans) and bottom-up (string distance)\n",
    "        print(\"Running: hybrid system.\")\n",
    "        output = run_query_clustering_hybrid(data, 0)\n",
    "    else:\n",
    "        print(\"No run_mode provided!\")\n",
    "   \n",
    "    # output df\n",
    "    cid, tid, txt, seedq1, seedq2, dtc, cos = [], [], [], [], [], [], []\n",
    "    for line in output[1:]:\n",
    "        line = line.split('\\t')\n",
    "        cid.append(line[0])\n",
    "        tid.append(line[1])\n",
    "        txt.append(line[2])\n",
    "        seedq1.append(line[3])\n",
    "        seedq2.append(line[4])\n",
    "        dtc.append(line[5])\n",
    "        cos.append(line[6])\n",
    "        out = pd.DataFrame({\"Cluster_ID\": cid,\n",
    "                            \"Text_ID\": tid,\n",
    "                            \"Text\": txt,\n",
    "                            \"Seed_Q1\": seedq1,\n",
    "                            \"Seed_Q2\": seedq2,\n",
    "                            \"Dist_to_Center\": dtc,\n",
    "                            \"Cohesion\": cos})\n",
    "        return out\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Clustering using Spacy, TFIDF and Iterative KMeans....\n",
      "\n",
      "Running: tfidf-based system\n",
      "Calculating query clusters!\n",
      "\n",
      "Clustering process finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = execute_clustering_method1(df, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-41d830f8-a0df-4480-be29-f735ddaaf997\" \n",
       "            onclick=\"_export_df('41d830f8-a0df-4480-be29-f735ddaaf997')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"41d830f8-a0df-4480-be29-f735ddaaf997\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster_ID</th>\n",
       "      <th>Text_ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Seed_Q1</th>\n",
       "      <th>Seed_Q2</th>\n",
       "      <th>Dist_to_Center</th>\n",
       "      <th>Cohesion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We have a client that wants to order a Platinu...</td>\n",
       "      <td>How does a client close an american express card</td>\n",
       "      <td>how do i close an account</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.517\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>how do i close an account</td>\n",
       "      <td>How does a client close an american express card</td>\n",
       "      <td>how do i close an account</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.517\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>when does cash plus get fee reversal for ameri...</td>\n",
       "      <td>How does a client close an american express card</td>\n",
       "      <td>how do i close an account</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.517\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>How does a client close an american express card</td>\n",
       "      <td>How does a client close an american express card</td>\n",
       "      <td>how do i close an account</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.517\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>can we close an account that is the primary ac...</td>\n",
       "      <td>How does a client close an american express card</td>\n",
       "      <td>how do i close an account</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.517\\r\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster_ID Text_ID                                               Text                                           Seed_Q1                    Seed_Q2 Dist_to_Center   Cohesion\n",
       "0          0       0  We have a client that wants to order a Platinu...  How does a client close an american express card  how do i close an account          0.570  0.517\\r\\n\n",
       "1          0      42                          how do i close an account  How does a client close an american express card  how do i close an account          0.395  0.517\\r\\n\n",
       "2          0      86  when does cash plus get fee reversal for ameri...  How does a client close an american express card  how do i close an account          0.476  0.517\\r\\n\n",
       "3          0     228   How does a client close an american express card  How does a client close an american express card  how do i close an account          0.265  0.517\\r\\n\n",
       "4          0     298  can we close an account that is the primary ac...  How does a client close an american express card  how do i close an account          0.539  0.517\\r\\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Basic preprocessing + TFIDF vectorization + KMeans Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Elbow/Silhouette Analysis\n",
    "\n",
    "Silhouette score: It is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated.\n",
    "\n",
    "Calinski-Harabasz Index: Higher the score , the better the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-e8ef772a10a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TENSORFLOW loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-hub/0.7.0/lib/tensorflow_hub/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# error message is thrown instead of an obscure error of missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# symbols at executing the imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatestModuleExporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_module_for_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_embedding_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-hub/0.7.0/lib/tensorflow_hub/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLatestModuleExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   \"\"\"Regularly exports registered modules into timestamped directories.\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-cpu/2.2.0-py37/exec/lib/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-cpu/2.2.0-py37/exec/lib/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/core/3.7.5-0/.exec/@sys/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-estimator/2.3.0/lib/tensorflow_estimator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-estimator/2.3.0/lib/tensorflow_estimator/_api/v1/estimator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-estimator/2.3.0/lib/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdnn_logit_fn_builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeansClustering\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSDCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-estimator/2.3.0/lib/tensorflow_estimator/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanned\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-estimator/2.3.0/lib/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_contextlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ms/dist/python/PROJ/tensorflow-estimator/2.3.0/lib/tensorflow_estimator/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexport_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_tracer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.types'"
     ]
    }
   ],
   "source": [
    "# TF & Keras\n",
    "ms.version.addpkg(\"absl-py\", \"0.9.0\")\n",
    "ms.version.addpkg(\"astunparse\", \"1.6.3\")\n",
    "ms.version.addpkg(\"cachetools\", \"4.1.0\")\n",
    "ms.version.addpkg(\"gast\", \"0.3.3\")\n",
    "ms.version.addpkg(\"google-auth\", \"1.16.1\")\n",
    "ms.version.addpkg(\"google-auth-oauthlib\", \"0.4.1\")\n",
    "ms.version.addpkg(\"google-pasta\", \"0.2.0\")\n",
    "ms.version.addpkg(\"grpcio\", \"1.27.2\")\n",
    "ms.version.addpkg(\"h5py\", \"2.10.0-threadsafe\")\n",
    "ms.version.addpkg(\"idna\", \"2.10\")\n",
    "ms.version.addpkg(\"importlib-metadata\", \"1.5.0\")\n",
    "ms.version.addpkg(\"markdown\", \"3.2.2\")\n",
    "ms.version.addpkg(\"oauthlib\", \"3.0.2\")\n",
    "ms.version.addpkg(\"opt-einsum\", \"3.1.0\")\n",
    "ms.version.addpkg(\"protobuf\", \"3.10.0\")\n",
    "ms.version.addpkg(\"pyasn1\", \"0.4.8\")\n",
    "ms.version.addpkg(\"pyasn1-modules\", \"0.2.4\")\n",
    "ms.version.addpkg(\"requests-oauthlib\", \"1.0.0\")\n",
    "ms.version.addpkg(\"rsa\", \"4.0\")\n",
    "ms.version.addpkg(\"termcolor\", \"1.1.0\")\n",
    "ms.version.addpkg(\"wheel\", \"0.33.6\")\n",
    "ms.version.addpkg(\"wrapt\", \"1.12.1\")\n",
    "# CPU -- Tensorflow --\n",
    "ms.version.addpkg(\"tensorflow-cpu\", \"2.2.0\")\n",
    "ms.version.addpkg(\"tensorboard\", \"2.2.2\")\n",
    "ms.version.addpkg(\"tensorboard-plugin-wit\", \"1.7.0\")\n",
    "ms.version.addpkg(\"tensorflow-estimator\", \"2.2.0\")\n",
    "ms.version.addpkg(\"tensorflow-hub\", \"0.7.0\")\n",
    "# GPU -- Tensorflow --\n",
    "# ms.version.addpkg(\"tensorflow-gpu\", \"2.1.0\")\n",
    "# ms.version.addpkg(\"tensorboard\", \"2.0.0\")\n",
    "# ms.version.addpkg(\"tensorboard-plugin-wit\", \"1.7.0\")\n",
    "# ms.version.addpkg(\"tensorflow-estimator\",\"2.0.0\")\n",
    "# ms.version.addpkg(\"tensorflow-hub\", \"0.7.0\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers as initializers, regularizers, constraints, optimizers\n",
    "from keras.layers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "# from keras.layers.core import Input, Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.models import Sequential, Model, load_model\n",
    "import tensorflow_hub as hub\n",
    "print(\"TENSORFLOW loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "col_TEXT = 'BASIC_CLEAN_TEXT'\n",
    "\n",
    "\n",
    "def vectorize_and_elbow_analysis(df):\n",
    "    \n",
    "    #### Vectorization using `USE model`\n",
    "    # load USE model\n",
    "    use_model = hub.KerasLayer(\"/v/region/na/appl/mswm/ainlp/data/ainlp_dev/Model_library/USE-Model-Large\")\n",
    "    \n",
    "    # Vectorize processed text using USE embeddings\n",
    "    X = use_model(df[col_TEXT].tolist()).numpy()\n",
    "    #### Vectorization using `USE model`\n",
    "\n",
    "    # normalize\n",
    "    X_Norm = preprocessing.normalize(X)\n",
    "\n",
    "    print(\"\\nTotal records: \", len(X_Norm))\n",
    "    print(\"Dimension: \", X_Norm.shape[1])\n",
    "    \n",
    "    # :: ELBOW ANALYSIS ::\n",
    "    \n",
    "    # K-means determine k\n",
    "    start_time = time.time()\n",
    "    \n",
    "    distortions, silhouettes, harabaszs = [], [], []\n",
    "    K = range(0, len(X_Norm), 20)\n",
    "    for k in tqdm.tqdm(K):\n",
    "        kmeanModel = KMeans(n_clusters=k, random_state=7)\n",
    "        kmeanModel.fit(X_Norm)\n",
    "        labels = kmeanModel.labels_\n",
    "        distortions.append(sum(np.min(cdist(X_Norm, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X_Norm.shape[0])\n",
    "        silhouettes.append(silhouette_score(X_Norm, labels))\n",
    "        harabaszs.append(calinski_harabasz_score(X_Norm, labels))\n",
    "    print(\"cluster analysis completed. Time (mins): \", (time.time() - start_time) / 60)\n",
    "\n",
    "    # Plot the elbow\n",
    "    plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.plot(K, silhouettes, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis showing the optimal k')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.plot(K, harabaszs, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Calinski Harabasz Score')\n",
    "    plt.title('Calinski Harabasz showing the optimal k')\n",
    "    plt.show()\n",
    "    \n",
    "    return X, X_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, X_Norm = vectorize_and_elbow_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def execute_clustering_method1(X_Norm, cluster_k):\n",
    "    \n",
    "    # Cohesion Score\n",
    "    def calculate_cohesion(cluster_id):\n",
    "        # cluster_id and coressponding members (docs belonging to cid)\n",
    "        docs = df[df['Cluster_ID'] == cluster_id][col_TEXT].tolist()\n",
    "\n",
    "        score = 0\n",
    "        if len(docs) > 2:\n",
    "            vect = TfidfVectorizer(use_idf=True, ngram_range=(1,1))\n",
    "            docs_norm = vect.fit_transform(docs)\n",
    "            k_value = int(np.sqrt(len(docs)/2))\n",
    "            cluster_model = KMeans(n_clusters=k_value, random_state=7)\n",
    "            cluster_model.fit(docs_norm)\n",
    "            cluster_centers_center  = cluster_model.cluster_centers_\n",
    "            # cohesion scores\n",
    "            values = []\n",
    "            for i in range(0, len(docs)):\n",
    "                d_vec = docs_norm[i].toarray()\n",
    "                simval = spatial.distance.cosine(d_vec, cluster_centers_center[0,:])\n",
    "                if math.isnan(simval):\n",
    "                    values.append(0)\n",
    "                else:\n",
    "                    values.append(1 - simval)\n",
    "            if len(values)==0:\n",
    "                score=0\n",
    "            else:\n",
    "                score=sum(values)/len(values)\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    # KMeans...\n",
    "    # based on elbow and silhoutte score, choosing k=3000 clusters\n",
    "    # final model with chosen k\n",
    "    CHOSEN_K = cluster_k\n",
    "    kmeanModel = KMeans(n_clusters=CHOSEN_K, random_state=7)\n",
    "    kmeanModel.fit(X_Norm)\n",
    "\n",
    "    ## Attributes ##\n",
    "\n",
    "    # Cluster IDs\n",
    "    df['Cluster_ID'] = kmeanModel.predict(X_Norm)\n",
    "\n",
    "    # Calculating distances (squared distance) to cluster center\n",
    "    X_dist = kmeanModel.transform(X_Norm)**2\n",
    "    distances = []\n",
    "    for dist_to_center in X_dist:\n",
    "        # cluster_id\n",
    "        closeset_centroid = np.argmin(dist_to_center)\n",
    "        # distance to centorid\n",
    "        dist = dist_to_center[closeset_centroid]\n",
    "        distances.append(dist)\n",
    "    df['dist_to_center'] = distances\n",
    "\n",
    "    # init Cluster: Cohesion dict\n",
    "    cluster_id_cohesion = dict(zip(sorted(df['Cluster_ID'].unique()), [0]*len(df['Cluster_ID'].unique())))\n",
    "    for cid in tqdm.tqdm(cluster_id_cohesion.keys()):\n",
    "        cluster_id_cohesion[cid] = calculate_cohesion(cid)\n",
    "    df['Cohesion'] = df['Cluster_ID'].apply(lambda x: cluster_id_cohesion[x])\n",
    "    \n",
    "    # Count per cluster\n",
    "    cluster_id_count = pd.Series(df['Cluster_ID'].value_counts().values, index=df['Cluster_ID'].value_counts().index).to_dict()\n",
    "    df['Count'] = df['Cluster_ID'].apply(lambda x: cluster_id_count[x])\n",
    "    \n",
    "    # Top 2 Canidates\n",
    "    cluster_grouped = df.sort_values(by=['Cluster_ID', 'dist_to_center']).groupby(by=['Cluster_ID'])['TEXT'].apply(lambda x: list(x)[:2])\n",
    "    cluster_candidates = pd.Series(cluster_grouped.values, index=cluster_grouped.index).to_dict()\n",
    "    df['Candidates'] = df['Cluster_ID'].apply(lambda x: cluster_candidates[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_df = execute_clustering_method1(X_Norm, cluster_k = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Visualize\n",
    "\n",
    "# Display_records = 10\n",
    "\n",
    "# x = df.Source.tolist()[:Display_records]\n",
    "# vectors = X_Norm[:Display_records]\n",
    "# kmean_indices = kmeanModel.predict(vectors)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# scatter_plot_points = pca.fit_transform(vectors)\n",
    "# x_axis = [o[0] for o in scatter_plot_points]\n",
    "# y_axis = [o[1] for o in scatter_plot_points]\n",
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# ax.scatter(x_axis, y_axis, c=['r'])\n",
    "# for i, txt in enumerate(x):\n",
    "#     ax.annotate(txt, (x_axis[i], y_axis[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### HAC Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Spacy pre-processing\n",
    "- USE embeddings vectorization\n",
    "- HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plotting some dedrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the dendrogram plot, find a horizontal rectangle with max-height that does not cross any horizontal vertical dendrogram line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [
     4,
     28
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "col_TEXT = 'BASIC_CLEAN_TEXT'\n",
    "\n",
    "\n",
    "# dendrogram\n",
    "def detailed_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "def vectorize_and_dendrogram_analysis(df):\n",
    "    \n",
    "    #### Vectorization using `USE model`\n",
    "    # load USE model\n",
    "    use_model = hub.KerasLayer(\"/v/region/na/appl/mswm/ainlp/data/ainlp_dev/Model_library/USE-Model-Large\")\n",
    "    \n",
    "    # Vectorize processed text using USE embeddings\n",
    "    X = use_model(df[col_TEXT].tolist()).numpy()\n",
    "    #### Vectorization using `USE model`\n",
    "\n",
    "    # normalize\n",
    "    X_Norm = preprocessing.normalize(X)\n",
    "\n",
    "    print(\"\\nTotal records: \", len(X_Norm))\n",
    "    print(\"Dimension: \", X_Norm.shape[1])\n",
    "    \n",
    "    # ward linkage\n",
    "    linked_array = ward(X_Norm)\n",
    "\n",
    "    detailed_dendrogram(\n",
    "        linked_array,\n",
    "        truncate_mode='lastp',\n",
    "        p=30,\n",
    "        leaf_rotation=90.,\n",
    "        leaf_font_size=10.,\n",
    "        show_contracted=True,\n",
    "        annotate_above=0.50,\n",
    "        max_d=1.0)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return X, X_Norm, linked_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "ABCD",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, X_Norm, linked_array = vectorize_and_dendrogram_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters cutting threshold distance:  6031\n"
     ]
    }
   ],
   "source": [
    "# Get number of clusters directly from dendrogram using distance threshold\n",
    "\n",
    "distance_threshold = 1.0\n",
    "\n",
    "clusters = fcluster(linked_array, distance_threshold, criterion='distance')\n",
    "\n",
    "print(\"Number of clusters cutting threshold distance: \", len(np.unique(clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "ABCD",
      "text/plain": [
       "<Figure size 1080x5760 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another Dedrogram\n",
    "\n",
    "MAX_COPHENETIC_DIST = max(linked_array[:,2]) * 0.10 # max distance between points to be considered together. can be tuned.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 80)) # set size\n",
    "ax = dendrogram(linked_array, orientation=\"right\", color_threshold=MAX_COPHENETIC_DIST, leaf_font_size=4,\n",
    "                labels=dissimilar_df['Source'].tolist())\n",
    "\n",
    "plt.tick_params(axis= 'x', which='both',  bottom='off', top='off',labelbottom='off')\n",
    "# plt.tight_layout()\n",
    "plt.savefig('ward_clusters_all.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Number of optimal clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHOSEN_CLUSTERS_HAC = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(n_clusters=5829)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hacModel = AgglomerativeClustering(n_clusters=CHOSEN_CLUSTERS_HAC, affinity='euclidean', linkage='ward')\n",
    "hacModel.fit(X_Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No way to get distances as algorithm loses it when merging to final cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Attributes for HAC ##\n",
    "\n",
    "# Cluster IDs\n",
    "df['Cluster_ID'] = c['HAC_Cluster_ID'] #hacModel.labels_\n",
    "\n",
    "# no distance function in HAC\n",
    "df['dist_to_center'] = np.nan\n",
    "\n",
    "# no cohesion values\n",
    "df['Cohesion'] = np.nan\n",
    "\n",
    "# Count per cluster\n",
    "cluster_id_count = pd.Series(df['Cluster_ID'].value_counts().values, index=df['Cluster_ID'].value_counts().index).to_dict()\n",
    "df['Count'] = df['Cluster_ID'].apply(lambda x: cluster_id_count[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sentence-Bert Iterative KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- In different notebook"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "Clustering_ComparisonTechniques",
  "createdOn": 1652684199748,
  "creationTag": {
   "lastModifiedBy": {
    "login": "pranjp"
   },
   "lastModifiedOn": 1652684199748,
   "versionNumber": 0
  },
  "creator": "pranjp",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "modifiedBy": "pranjp",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
