{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLP Cleansing, Vectorization and Similarity Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Timestamp: July, 2021\n",
    "- Author: Pranjal Pathak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "2. Vectorization using sBert\n",
    "3. Text Similarity\n",
    "4. Example\n",
    "\n",
    "**Techniques tested:**\n",
    "\n",
    "Standard Methods\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "1. Bilingual Evaluation Understudy (BLEU)\n",
    "2. Levenshtein Distance\n",
    "3. TFIDF + Euclidean Distance\n",
    "4. TFIDF + Cosine Distance\n",
    "5. TFIDF + Jaccard Distance\n",
    "6. Embeddings + Cosine Distance\n",
    "7. Embeddings + Smooth Inverse Frequency (SIF) + Cosine Distance\n",
    "8. Spacy Vectorizer + Smooth Inverse Frequency (SIF) + Cosine Distance\n",
    "9. Embeddings + Word Movers Distance (WMD)\n",
    "\n",
    "Advanced Methods\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "10. LDA + Jensen-Shannon distance\n",
    "11. Trained Attention LSTM Inception AutoEncoder Network Embeddings + Cosine Distance (**3rd BEST**)\n",
    "12. Finetuned Universal Sentence Encoder(USE) + Cosine Distance (**2nd BEST**)\n",
    "13. Fine-tuned Ultra-Fast Sentence Bert (sBERT) + Cosine Distance (**BEST**)\n",
    "14. Siamese Deep Neural Network (Pending)\n",
    "\n",
    "\n",
    "Future Scope\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "15. Siamese Manhattan LSTM + Manhattan Similarity\n",
    "16. ELMo Embeddings + Cosine Similarity\n",
    "17. BERT Embeddings + Cosine Similarity\n",
    "18. FastText embeddings (trained on 3DR) + Attention LSTM AutoEncoder (trained on 3DR) + Cosine Similarity\n",
    "19. FastText embeddings (trained on 3DR) + Attention LSTM AutoEncoder (trained on Utterances) + Cosine Similarity\n",
    "20. FastText embeddings (trained on Utterances) + Attention LSTM AutoEncoder (trained on 3DR) + Cosine Similarity\n",
    "21. FastText embeddings (trained on Utterances) + Attention LSTM AutoEncoder (trained on Utterances) + Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `Instructions`\n",
    "\n",
    "1. Get Python >= 3.6.0\n",
    "2. Create a virtual env and install requirements.txt\n",
    "3. Get Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/773632469.py:78: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  from scipy.sparse.csr import csr_matrix\n",
      "/tmp/ipykernel_565/773632469.py:79: DeprecationWarning: Please use `lil_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.lil` namespace is deprecated.\n",
      "  from scipy.sparse.lil import lil_matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n",
      "Spacy loaded.\n",
      "TensorFlow loaded.\n",
      "PyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "'''Update code from Python 3.6.10 to a stable Kernel Python Version 3.8.0 '''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "#from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/someppath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "# spacy_model_data_path = \"/Users/pranjalpathak/opt/anaconda3/envs/Python_3.6/lib/python3.6/site-packages/en_core_web_lg/en_core_web_lg-2.2.5\"\n",
    "nlp = spacy.load('en_core_web_lg')  # disabling: nlp = spacy.load(spacy_data_path, disable=['ner'])\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# TF & Keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.layers import Layer, InputSpec, BatchNormalization, Embedding, LSTM, Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import pad_sequences, CustomObjectScope\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers as initializers, regularizers, constraints, optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.models import Sequential, Model, load_model\n",
    "import tensorflow_hub as hub\n",
    "print(\"TensorFlow loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common NLP resources\n",
    "resources_dir_path = \"./data/resources/\"\n",
    "\n",
    "# embedding models\n",
    "pretrained_glove_fp = \"./models/pretrained/glove/glove.6b.100d/glove.6B.100d.vec\"\n",
    "pretrained_google_fp = \"./models/pretrained/google/GoogleNews-vectors-negative300.bin\"\n",
    "pretrained_fasttext_fp = \"./models/pretrained/fasttext/cc.en.300.bin\"\n",
    "trained_word2vec_fp = \"./models/trained/word2vec\"\n",
    "trained_doc2vec_fp = \"./models/trained/doc2vec\"\n",
    "trained_wmd_fp = \"./models/trained/wordmoversdistance\"\n",
    "trained_word2vec_glove_fp = \"./models/trained/word2vec_glove\"\n",
    "trained_word2vec_google_fp = \"./models/trained/word2vec_google\"\n",
    "trained_fasttext_fp = \"./models/trained/fasttext\"\n",
    "tuned_use_fp = \"./models/trained/USE-Model-Large\"\n",
    "tuned_sbert_fp = \"./models/trained/sentence-transformers-models/all-distilroberta-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing Unit (Unit 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessText:\n",
    "    \n",
    "    def __init__(self, resources_dir_path, custom_vocab=[], do_lemma=False):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n",
    "        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n",
    "        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n",
    "        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n",
    "        self.preserve_key = \"<$>\" # preserve special vocab\n",
    "        self.vocab_list = custom_vocab\n",
    "        self.preseve = True if len(custom_vocab) > 0 else False\n",
    "        self.load_resources()\n",
    "        self.do_lemma = do_lemma\n",
    "        return\n",
    "    \n",
    "    def load_resources(self):\n",
    "        \n",
    "        ### Build Vocab Model --> Words to keep\n",
    "        self.vocab_list = set(map(str.lower, self.vocab_list))\n",
    "        self.vocab_dict = {w: self.preserve_key.join(w.split()) for w in self.vocab_list}\n",
    "        self.re_retain_words = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n",
    "        \n",
    "        ### Build Stopwords Model --> Words to drop/delete\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        self.stopwords = list(sorted(set(self.stopwords).difference(self.vocab_list)))\n",
    "\n",
    "        ### Build Contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "        \n",
    "        ### Build Chat-words\n",
    "        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.chat_words_map_dict, self.chat_words_list = {}, []\n",
    "            chat_words = [x.rstrip() for x in f.readlines()]\n",
    "            for line in chat_words:\n",
    "                cw = line.split(\"=\")[0]\n",
    "                cw_expanded = line.split(\"=\")[1]\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "            self.chat_words_list = set(self.chat_words_list)\n",
    "        \n",
    "        ### Bukd social markups\n",
    "        # emoticons\n",
    "        with open(self.emoticons_file, \"r\") as f:\n",
    "            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n",
    "        # emojis\n",
    "        self.emojis = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        # greeting\n",
    "        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.greeting_words = [x.rstrip() for x in f.readlines()]\n",
    "        # signature\n",
    "        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.signature_words = [x.rstrip() for x in f.readlines()]\n",
    "        # spell-corrector\n",
    "        self.spell_checker = SpellChecker()   \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def reserve_keywords_from_cleaning(self, text, reset=False):\n",
    "        \"\"\" \n",
    "        Finds common words from a user-provided list of special keywords to preserve them from \n",
    "        cleaning steps. Identifies every special keyword and joins them using `self.preserve_key` during the \n",
    "        cleaning steps, and later resets it back to original word in the end.\n",
    "        \"\"\"\n",
    "        if reset is False:\n",
    "            # compile using a dict of words and their expansions, and sub them if found!\n",
    "            match_and_sub = self.re_retain_words.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n",
    "        else:\n",
    "            # reverse the change! - use this at the end of preprocessing\n",
    "            text = text.replace(self.preserve_key, \" \")\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    def basic_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            sent = str(sent).strip()\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def deep_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            # normalize text to \"utf-8\" encoding\n",
    "            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Mark important keywords such as: Domain specific, Question words(wh-words), etc, using \n",
    "            # \"self.vocab_list\". Words from this list if found in any input sentence shall be joined using \n",
    "            # a key (self.preserve_key) during pre-processing step, and later un-joined to retain them.\n",
    "            #\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=False)\n",
    "            #\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            # remove Emojis\n",
    "            sent = self.emojis.sub(r'', sent)\n",
    "            # remove emoticons\n",
    "            sent = self.emoticons.sub(r'', sent)\n",
    "            # remove common chat-words\n",
    "            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "            #\n",
    "            # removing punctuations ðŸ”¥ðŸ”¥\n",
    "            # *** disable them, when sentence structure needs to be retained ***\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n",
    "            \n",
    "            # remove sentence de-limitters ðŸ”¥ðŸ”¥\n",
    "            # *** disable them, when sentence boundary/ending is important ***\n",
    "            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)\n",
    "\n",
    "            # keep only text & numbers ðŸ”¥ðŸ”¥\n",
    "            # *** enable them, when only text and numbers matter! *** \n",
    "            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            \n",
    "            # correct spelling mistakes ðŸ”¥ðŸ”¥\n",
    "            # *** enable them when english spelling mistakes matter *** \n",
    "            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n",
    "            #\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "\n",
    "            # Remove stopwords\n",
    "            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords and \n",
    "                                                                 token.lemma_ not in self.stopwords)\n",
    "            # Lemmatize\n",
    "            if self.do_lemma:\n",
    "                sent = \" \".join(token.lemma_ for token in nlp(sent))\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).lower().strip()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Reverse the custom joining now to un-join the special words found!\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=True)\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            cleaned_sentences.append(sent.strip().lower())\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def spacy_get_pos_list(self, results):\n",
    "        word_list, pos_list, lemma_list, ner_list, start_end_list = [], [], [], [], []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                # (1). save tokens\n",
    "                word_list.append(token['word'])\n",
    "                # (2). save pos\n",
    "                pos_list.append(token['pos'])\n",
    "                # (3). save lemmas\n",
    "                lemma = token['lemma'].lower()\n",
    "                if lemma in self.stopwords: continue\n",
    "                lemma_list.append(lemma)\n",
    "                # (4). save NER\n",
    "                ner_list.append(token['ner'])\n",
    "                # (5). save start\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        output = {\"word_list\": word_list, \n",
    "                  \"lemma_list\": lemma_list, \n",
    "                  \"token_start_end_list\": start_end_list,\n",
    "                  \"pos_list\": pos_list, \"ner_list\": ner_list}\n",
    "        return output\n",
    "\n",
    "    def spacy_generate_features(self, doc, operations='tokenize,ssplit,pos,lemma,ner'):\n",
    "        \"\"\"\n",
    "        Spacy nlp pipeline to generate features such as pos, tokens, ner, dependency. Accepts doc=nlp(text)\n",
    "        \"\"\"\n",
    "        # spacy doc\n",
    "        doc_json = doc.to_json()  # Includes all operations given by spacy pipeline\n",
    "\n",
    "        # Get text\n",
    "        text = doc_json['text']\n",
    "\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        # 1. Extract Entity List\n",
    "        entity_list = doc_json[\"ents\"]\n",
    "\n",
    "        # 2. Create token lib\n",
    "        token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "        # init output json\n",
    "        output_json = {}\n",
    "        output_json[\"sentences\"] = []\n",
    "\n",
    "        # Perform spacy operations on each sent in text\n",
    "        for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "            # init parsers\n",
    "            parse = \"\"\n",
    "            basicDependencies = []\n",
    "            enhancedDependencies = []\n",
    "            enhancedPlusPlusDependencies = []\n",
    "\n",
    "            # init output json\n",
    "            out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "            output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "            # 3. Split sentences by indices(i), add labels (pos, ner, dep, etc.)\n",
    "            for token in doc_json[\"tokens\"]:\n",
    "\n",
    "                if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                    \n",
    "                    # >>> Extract Entity label\n",
    "                    ner = \"O\"\n",
    "                    for entity in entity_list:\n",
    "                        if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                            ner = entity[\"label\"]\n",
    "\n",
    "                    # >>> Extract dependency info\n",
    "                    dep = token[\"dep\"]\n",
    "                    governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                    governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]:\n",
    "                                                                                     token_lib[token[\"head\"]][\"end\"]]\n",
    "                    dependent = token[\"id\"] + 1\n",
    "                    dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "\n",
    "                    # >>> Extract lemma\n",
    "                    lemma = doc[token[\"id\"]].lemma_\n",
    "\n",
    "                    # 4. Add dependencies\n",
    "                    basicDependencies.append({\"dep\": dep,\n",
    "                                              \"governor\": governor,\n",
    "                                              \"governorGloss\": governorGloss,\n",
    "                                              \"dependent\": dependent,\n",
    "                                              \"dependentGloss\": dependentGloss})\n",
    "                    # 5. Add tokens\n",
    "                    out_token = {\"index\": token[\"id\"] + 1,\n",
    "                                 \"word\": dependentGloss,\n",
    "                                 \"originalText\": dependentGloss,\n",
    "                                 \"characterOffsetBegin\": token[\"start\"],\n",
    "                                 \"characterOffsetEnd\": token[\"end\"]}\n",
    "\n",
    "                    # 6. Add lemmas\n",
    "                    if \"lemma\" in operations:\n",
    "                        out_token[\"lemma\"] = lemma\n",
    "\n",
    "                    # 7. Add POS tagging\n",
    "                    if \"pos\" in operations:\n",
    "                        out_token[\"pos\"] = token[\"tag\"]\n",
    "\n",
    "                    # 8. Add NER\n",
    "                    if \"ner\" in operations:\n",
    "                        out_token[\"ner\"] = ner\n",
    "\n",
    "                    # Update output json\n",
    "                    out_sentence[\"tokens\"].append(out_token)\n",
    "\n",
    "            # 9. Add dependencies operation\n",
    "            if \"parse\" in operations:\n",
    "                out_sentence[\"parse\"] = parse\n",
    "                out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "                out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "                out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        return output_json\n",
    "    \n",
    "    def spacy_clean(self, input_sentences):\n",
    "        batch_size = min(int(np.ceil(len(input_sentences)/100)), 500)\n",
    "        \n",
    "        # Part 1: generate spacy textual features (pos, ner, lemma, dependencies)\n",
    "        sentences = [self.spacy_generate_features(doc) for doc in nlp.pipe(input_sentences, batch_size=batch_size, n_process=-1)]\n",
    "        \n",
    "        # Part 2: collect all the features for each sentence\n",
    "        spacy_sentences = [self.spacy_get_pos_list(sent) for sent in sentences]\n",
    "\n",
    "        return spacy_sentences\n",
    "\n",
    "\n",
    "    ## MAIN ##\n",
    "    def run_pipeline(self, sentences, operation):\n",
    "        \"\"\"\n",
    "        Main module to execute pipeline. Accepts list of strings, and desired operation.\n",
    "        \"\"\"\n",
    "        if operation==\"\":\n",
    "            raise Exception(\"Please pass a cleaning type - `basic`, `deep` or `spacy` !!\")\n",
    "\n",
    "        # run basic cleaning\n",
    "        if \"basic\" == operation.lower(): \n",
    "            return self.basic_clean(sentences)\n",
    "\n",
    "        # run deep cleaning\n",
    "        if \"deep\" == operation.lower(): \n",
    "            return self.deep_clean(sentences)\n",
    "\n",
    "        # run spacy pipeline\n",
    "        if \"spacy\" == operation.lower(): \n",
    "            return self.spacy_clean(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings ##\n",
    "\n",
    "\"\"\"\n",
    "CUSTOM VOCABULARY ::\n",
    "\n",
    "- List of words you wish to mark and retain them across the preprocessing steps - very important!\n",
    "- Example, task-specific, domain-specific keywords.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_vocab = [\"who\", \"what\", \"where\", \"when\", \"would\", \"which\", \"how\", \"why\", \"can\", \"may\", \n",
    "                \"will\", \"won't\", \"does\", \"does not\",\"doesn't\", \"do\", \"do i\", \"do you\", \"is it\", \"would you\", \n",
    "                \"is there\", \"are there\", \"is it so\", \"is this true\", \"to know\", \"is that true\", \"are we\", \n",
    "                \"am i\", \"question is\", \"can i\", \"can we\", \"tell me\", \"can you explain\", \"how ain't\", \n",
    "                \"question\", \"answer\", \"questions\", \"answers\", \"ask\", \"can you tell\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Utilities:\n",
    "- Truncate words to their root-known-word form, stripping off their adjectives, verbs, etc. (Example: \"running\" becomes \"run\", \"is\" becomes \"be\")\n",
    "- different from stemmer (PorterStemmer)\n",
    "- Can use regex based stemming..\n",
    "- Check Spacy's dependency parsing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "do_lemmatizing = True\n",
    "#do_chinking = False\n",
    "#do_chunking = False\n",
    "#do_dependencyParser = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing ##\n",
    "\n",
    "preprocessText_obj = preprocessText(resources_dir_path, custom_vocab, do_lemmatizing)\n",
    "\n",
    "def cleaning(data, text_col):\n",
    "    data[\"Basic_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"basic\")\n",
    "    data[\"Deep_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"deep\")\n",
    "    data[\"Spacy_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"spacy\")\n",
    "    return data\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# df = cleaning(df, <_TEXT_COLUMN_>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Basic_TEXT</th>\n",
       "      <th>Deep_TEXT</th>\n",
       "      <th>Spacy_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello i am good</td>\n",
       "      <td>hello i am good</td>\n",
       "      <td>hello good</td>\n",
       "      <td>{'word_list': ['hello', 'i', 'am', 'good'], 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello bye!</td>\n",
       "      <td>hello bye!</td>\n",
       "      <td>hello bye</td>\n",
       "      <td>{'word_list': ['hello', 'bye', '!'], 'lemma_li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TEXT       Basic_TEXT   Deep_TEXT  \\\n",
       "0  hello i am good  hello i am good  hello good   \n",
       "1       hello bye!       hello bye!   hello bye   \n",
       "\n",
       "                                          Spacy_TEXT  \n",
       "0  {'word_list': ['hello', 'i', 'am', 'good'], 'l...  \n",
       "1  {'word_list': ['hello', 'bye', '!'], 'lemma_li...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Execute ##\n",
    "\n",
    "df = pd.DataFrame({\"TEXT\": ['hello i am good', \"hello bye!\"]})\n",
    "cleaning(df, \"TEXT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  Vectorization Unit (Unit 2/3)\n",
    "\n",
    "- Loads trained/pretrained embeddings models.\n",
    "- Vectorizes text using user defiend model.\n",
    "- Pre-requsite: need to have pre-trained/trained/tuned embedding models in ./models/ directory before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## define a list of vectorizers to be used for task at hand\n",
    "\n",
    "VECTORIZER_LIST = ['count', 'tfidf', 'word2vec', 'fasttext', \n",
    "                   'home_trained_word2vec', 'home_trained_fasttext', \n",
    "                   'pretrained_glove', 'pretrained_google', 'pretrained_fasttext', \n",
    "                   'word2vec_glove', 'word2vec_google', \n",
    "                   'USE', 'BERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0,
     5,
     11,
     22,
     27,
     30,
     34,
     38,
     42,
     45,
     48,
     51,
     55,
     69,
     76,
     81,
     86,
     112,
     121,
     133,
     143,
     153,
     173,
     175,
     193,
     209,
     225,
     243,
     261,
     279,
     309,
     316,
     347
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    \"\"\"\n",
    "    Used to load models and then vectorize text using a passed choice of vectorizer - 'source'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = {}\n",
    "        self.modelpath = {}\n",
    "        self.vectorizer = {}\n",
    "        self.vectorizer_list = VECTORIZER_LIST\n",
    "\n",
    "    def convert(self, source, input_fp, output_fp):\n",
    "        \"\"\"\n",
    "        If need to convert GLOVE text file into vector format.\n",
    "        e.g. convert(source='glove', input_fp=downloaded_glove_file_path, output_fp=save_file_path)\n",
    "        \"\"\"\n",
    "        if source == 'glove':\n",
    "            input_file, output_file = datapath(input_fp), get_tmpfile(output_fp)\n",
    "            glove2word2vec(input_file, output_file)\n",
    "        else:\n",
    "            raise ValueError('ERROR :: You can convert only glove text file!')\n",
    "\n",
    "    def load(self, source, fp):\n",
    "        \"\"\"\n",
    "        Loads a trained vectorizer model from a given file-path.\n",
    "        \"\"\"\n",
    "        \n",
    "        if source not in self.vectorizer_list or not fp:\n",
    "            raise ValueError('ERROR :: Pass a eligible source from VECTORIZER_LIST options provided!')\n",
    "        \n",
    "        if source in ['count', 'tfidf', 'word2vec', 'fasttext']:\n",
    "            # no trained model for these\n",
    "            return\n",
    "       \n",
    "        if source == 'home_trained_word2vec':\n",
    "            self.model[source] = Word2Vec.load(fp)\n",
    "            self.model[source].init_sims(replace=True)\n",
    "        \n",
    "        elif source == 'home_trained_fasttext':\n",
    "            self.model[source] = FastText.load(fp)\n",
    "            self.model[source].init_sims(replace=True)\n",
    "        \n",
    "        elif source == 'pretrained_glove':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(fp, unicode_errors='ignore')\n",
    "        \n",
    "        elif source == 'pretrained_google':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(fp, binary=True, unicode_errors='ignore')\n",
    "        \n",
    "        elif source == 'pretrained_fasttext':\n",
    "            self.model[source] = gensim.models.wrappers.FastText.load_fasttext_format(fp)\n",
    "        \n",
    "        elif source == 'word2vec_glove':\n",
    "            self.model[source] = Word2Vec.load(fp)\n",
    "            self.model[source].init_sims(replace=True)\n",
    "        \n",
    "        elif source == 'word2vec_google':\n",
    "            self.model[source] = Word2Vec.load(fp)\n",
    "            self.model[source].init_sims(replace=True)\n",
    "        \n",
    "        elif source == 'USE':\n",
    "            self.model[source] = hub.KerasLayer(fp, trainable=True)\n",
    "            \n",
    "        elif source == 'BERT':\n",
    "            self.model[source] = {}\n",
    "            self.model[source]['tokenizer'] = AutoTokenizer.from_pretrained(fp)\n",
    "            self.model[source]['model'] = AutoModel.from_pretrained(fp)\n",
    "            self.model[source]['max_length'] = 128\n",
    "            self.model[source]['embedding_func'] = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "        else:\n",
    "            raise Exception('ERROR :: Incorrect source passed!')\n",
    "\n",
    "        self.modelpath[source] = fp\n",
    "        print(source, \" - loaded.\")\n",
    "        return\n",
    "\n",
    "    def get_model(self, source):\n",
    "        if source not in self.vectorizer_list:\n",
    "            raise ValueError('Possible value of source are:{}'.format(\",\".join(self.vectorizer_list)))\n",
    "        return self.model[source]\n",
    "\n",
    "    def get_model_path(self, source):\n",
    "        if source not in self.vectorizer_list:\n",
    "            raise ValueError('Possible value of source are:{}'.format(\",\".join(self.vectorizer_list)))\n",
    "        return self.modelpath[source]\n",
    "\n",
    "    def get_words(self, source, size=None):\n",
    "        if source in self.trained_embed_models:\n",
    "            if size is None:\n",
    "                return [w for w in self.get_model(source=source).vocab]\n",
    "            elif size is None:\n",
    "                return [w for w in self.get_model(source=source).vocab]\n",
    "            else:\n",
    "                results = []\n",
    "                for i, word in enumerate(self.get_model(source=source).vocab):\n",
    "                    if i >= size:\n",
    "                        break\n",
    "                    results.append(word)\n",
    "                return results\n",
    "        elif source in ['fasttext', 'pretrained_fasttext']:\n",
    "            if size is None:\n",
    "                return [w for w in self.get_model(source=source).wv.vocab]\n",
    "            else:\n",
    "                results = []\n",
    "                for i, word in enumerate(self.get_model(source=source).wv.vocab):\n",
    "                    if i >= size:\n",
    "                        break\n",
    "                    results.append(word)\n",
    "                return results\n",
    "        else:\n",
    "            raise ValueError('Only embedding models are allowed!')\n",
    "\n",
    "    def get_dimension(self, source):\n",
    "        if source in self.trained_embed_models:\n",
    "            return self.get_model(source=source).vectors[0].shape[0]\n",
    "        elif source in ['fasttext', 'pretrained_fasttext']:\n",
    "            word = self.get_words(source=source, size=1)[0]\n",
    "            return self.get_model(source=source).wv[word].shape[0]\n",
    "        else:\n",
    "            raise ValueError('Only embedding models are allowed!')\n",
    "\n",
    "    def get_vectors(self, source, words=None):\n",
    "        # vectorize tokens\n",
    "        if source in self.trained_embed_models:\n",
    "            if words is None:\n",
    "                words = self.get_words(source=source)\n",
    "            embedding = np.empty((len(words), self.get_dimension(source=source)), dtype=np.float32)\n",
    "            for i, word in enumerate(words):\n",
    "                embedding[i] = self.get_vector(source=source, word=word)\n",
    "            return embedding\n",
    "        else:\n",
    "            raise ValueError('Only embedding models are allowed!')\n",
    "\n",
    "    def get_vector(self, source, word, oov=None):\n",
    "        if source not in self.trained_embed_models:\n",
    "            raise ValueError('Only embedding models are allowed!')\n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        try:\n",
    "            return self.model[source][word]\n",
    "        except KeyError as e:\n",
    "            raise\n",
    "\n",
    "    def get_synonym(self, source, word, oov=None):\n",
    "        if source not in self.trained_embed_models:\n",
    "            raise ValueError('Only embedding models are allowed!')\n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        try:\n",
    "            return self.model[source].most_similar(positive=word, topn=5)\n",
    "        except KeyError as e:\n",
    "            raise\n",
    "\n",
    "    def which_distance_between_two_words(self, source, word1, word2, oov=None):\n",
    "        if source not in self.trained_embed_models:\n",
    "            raise ValueError('Only embedding models are allowed!')\n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        try:\n",
    "            return self.model[source].similarity(word1, word2)\n",
    "        except KeyError as e:\n",
    "            raise\n",
    "\n",
    "    def vectorize(self, file_1_sents, file_2_sents, vectorizers):\n",
    "        \"\"\"\n",
    "        Vectorizes (and/or trains) 2 files: 'Source' & 'Target' containing processed sentences using one/many vectorizers.\n",
    "        \n",
    "        file_1_sents  : List of **processed** sentences from File 1 (list of pd.Series)\n",
    "        file_2_sents  : List of **processed** sentences from File 2 (list of pd.Series)\n",
    "        vectorizers   : List of choice of vectorization models (e.g. ['count', 'USE', 'BERT'])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(file_1_sents, pd.Series):\n",
    "            file_1_sents = file_1_sents.tolist()\n",
    "        if isinstance(file_2_sents, pd.Series):\n",
    "            file_2_sents = file_2_sents.tolist()\n",
    "            \n",
    "        # cal size\n",
    "        file_1_size = len(file_1_sents)\n",
    "        file_2_size = len(file_2_sents)\n",
    "\n",
    "        # tokenize\n",
    "        corpus = file_1_sents + file_2_sents\n",
    "        tokenized_sentences = [sent.split() for sent in corpus]\n",
    "        # vocab = list(set([word for sent in tokenized_sentences for word in sent]))\n",
    "\n",
    "        self.vectorizer = {}\n",
    "        for source in vectorizers:\n",
    "            \n",
    "            print(\"\\nVectorizing using ::\", source)\n",
    "            \n",
    "            # trains and vectorizes\n",
    "            if source == 'count':\n",
    "                # Train\n",
    "                print('Training...')\n",
    "                start = time.time()\n",
    "                model = \"\"\n",
    "                model = CountVectorizer(ngram_range=(1, 1))\n",
    "                vectors = model.fit_transform(corpus)\n",
    "                # Vectorize\n",
    "                print('Vectorizing...')\n",
    "                file_1_vectors = vectors[0:file_1_size, :]\n",
    "                file_2_vectors = vectors[file_1_size:, :]\n",
    "                # Save\n",
    "                self.vectorizer[source] = {\"model\":model, \"file_1_vectors\":file_1_vectors, \"file_2_vectors\":file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "\n",
    "            # trains and vectorizes\n",
    "            elif source == 'tfidf':\n",
    "                # Train\n",
    "                print('Training...')\n",
    "                start = time.time()\n",
    "                model = \"\"\n",
    "                model = TfidfVectorizer(use_idf=True, ngram_range=(1, 1))\n",
    "                vectors = model.fit_transform(corpus)\n",
    "                # Vectorize\n",
    "                print('Vectorizing...')\n",
    "                file_1_vectors = vectors[0:file_1_size, :]\n",
    "                file_2_vectors = vectors[file_1_size:, :]\n",
    "                # Save\n",
    "                self.vectorizer[source] = {\"model\":model, \"file_1_vectors\":file_1_vectors, \"file_2_vectors\":file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "            \n",
    "            # trains and vectorizes\n",
    "            elif source == 'word2vec':\n",
    "                # Train\n",
    "                print('Training...')\n",
    "                start = time.time()\n",
    "                model = \"\"\n",
    "                model = Word2Vec(size=300, min_count=1, workers=40)\n",
    "                model.build_vocab(tokenized_sentences, progress_per=10000)\n",
    "                model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=model.iter)\n",
    "                # Vectorize\n",
    "                print('Vectorizing...')\n",
    "                file_1_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_1_sents]\n",
    "                file_2_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_2_sents]\n",
    "                # Save\n",
    "                # model.save(filepath)\n",
    "                self.vectorizer[source] = {\"model\":model, \"file_1_vectors\":file_1_vectors, \"file_2_vectors\":file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "            \n",
    "            # trains and vectorizes\n",
    "            elif source == 'fasttext':\n",
    "                # Train\n",
    "                print('Training...')\n",
    "                start = time.time()\n",
    "                model = \"\"\n",
    "                model = FastText(size=200, window=5, min_count=1, workers=40)\n",
    "                model.build_vocab(tokenized_sentences, progress_per=10000)\n",
    "                model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=model.epochs)\n",
    "                # Vectorize\n",
    "                print('Vectorizing...')\n",
    "                file_1_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_1_sents]\n",
    "                file_2_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_2_sents]\n",
    "                # Save\n",
    "                # model.save(filepath)\n",
    "                self.vectorizer[source] = {\"model\":model, \"file_1_vectors\":file_1_vectors, \"file_2_vectors\":file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "            \n",
    "            # vectorizes\n",
    "            elif source in ['homemade_word2vec', 'homemade_fasttext', 'pretrained_glove', 'pretrained_google', \n",
    "                            'pretrained_fasttext']:\n",
    "                \n",
    "                if source not in self.model:\n",
    "                    raise ValueError('Did not load %s model yet' % source)\n",
    "                # -- No training required - using loaded models! --\n",
    "                model = \"\"\n",
    "                model = self.model[source]\n",
    "                # Vectorize\n",
    "                print('Vectorizing...')\n",
    "                start = time.time()\n",
    "                file_1_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_1_sents]\n",
    "                file_2_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_2_sents]\n",
    "                # Save\n",
    "                self.vectorizer[source] = {\"model\":model, \"file_1_vectors\":file_1_vectors, \"file_2_vectors\":file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "            \n",
    "            # trains and vectorizes\n",
    "            elif source in ['word2vec_glove', 'word2vec_google']:\n",
    "                # pre-trained model name (glove or google)\n",
    "                pretained_source = \"pretrained_{}\".format(source.split(\"_\")[1])\n",
    "                if pretained_source not in self.model:\n",
    "                    raise ValueError('Did not load %s model yet' % pretained_source)\n",
    "                # load pre-trained model first\n",
    "                pretrained_model = self.model[pretained_source]\n",
    "                pretrained_model_fp = self.modelpath[pretained_source]\n",
    "                embedding_size = pretrained_model.vector_size\n",
    "                binary_value = False\n",
    "                if pretained_source == 'pretrained_google':\n",
    "                    binary_value = True\n",
    "                # Train\n",
    "                print('Training...')\n",
    "                start = time.time()\n",
    "                model = \"\"\n",
    "                model = Word2Vec(size=embedding_size, min_count=1, workers=40)\n",
    "                model.build_vocab(tokenized_sentences, progress_per=10000)\n",
    "                model.build_vocab([list(pretrained_model.vocab.keys())], update=True)                  # transfer learning\n",
    "                model.intersect_word2vec_format(pretrained_model_fp, binary=binary_value, lockf=1.0)   # transfer learning\n",
    "                model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=model.iter)\n",
    "                # Vectorize\n",
    "                print('Vectorizing...')\n",
    "                file_1_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_1_sents]\n",
    "                file_2_vectors = [np.mean([model.wv[w] if w in model.wv.vocab else np.ones(model.vector_size) for w in sent.split()], axis=0) for sent in file_2_sents]\n",
    "                # Save\n",
    "                self.vectorizer[source] = {\"model\":model, \"file_1_vectors\":file_1_vectors, \"file_2_vectors\":file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "            \n",
    "            # vectorizes\n",
    "            elif source == 'USE':\n",
    "                \n",
    "                if source not in self.model:\n",
    "                    raise ValueError('Did not load %s model yet' % source)\n",
    "                \n",
    "                use_model = self.model[source]\n",
    "                \n",
    "                def embed(lst):\n",
    "                    chunk_size = 5000\n",
    "                    batches = [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "                    arr=[]\n",
    "                    for x in tqdm.tqdm(batches):\n",
    "                        arr.append(use_model(x))\n",
    "                    return np.concatenate(arr)\n",
    "                \n",
    "                print('Vectorizing...')\n",
    "                start = time.time()\n",
    "                file_1_vectors = embed(file_1_sents)\n",
    "                file_2_vectors = embed(file_2_sents)\n",
    "                # Save\n",
    "                self.vectorizer[source] = {\"model\": model, \n",
    "                                           \"file_1_vectors\": file_1_vectors, \n",
    "                                           \"file_2_vectors\": file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "            \n",
    "            # vectorizes\n",
    "            elif source == 'BERT':\n",
    "                \n",
    "                if source not in self.model:\n",
    "                    raise ValueError('Did not load %s model yet' % source)\n",
    "\n",
    "                tokenizer = self.model[source]['tokenizer']\n",
    "                model = self.model[source]['model']\n",
    "                model.eval()\n",
    "                max_length = self.model[source]['max_length']\n",
    "                embedding_func = self.model[source]['embedding_func']\n",
    "\n",
    "                # Mean Pooling - Take attention mask into account for correct averaging\n",
    "                def mean_pooling(model_output, attention_mask):\n",
    "                    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "                    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                    return sum_embeddings / sum_mask\n",
    "\n",
    "                # Tokenize the text with the provided tokenizer\n",
    "                encoded_input_1 = tokenizer(file_1_sents, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "                encoded_input_2 = tokenizer(file_2_sents, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output_1 = model(**encoded_input_1)\n",
    "                    model_output_2 = model(**encoded_input_2)\n",
    "\n",
    "                # Perform mean pooling\n",
    "                print('Vectorizing...')\n",
    "                start = time.time()\n",
    "                file_1_vectors = mean_pooling(model_output_1, encoded_input_1['attention_mask'])\n",
    "                file_2_vectors = mean_pooling(model_output_2, encoded_input_2['attention_mask'])\n",
    "                # Save\n",
    "                self.vectorizer[source] = {\"model\": model, \n",
    "                                           \"file_1_vectors\": file_1_vectors, \n",
    "                                           \"file_2_vectors\": file_2_vectors}\n",
    "                print('Time ms:{}'.format(time.time() - start))\n",
    "\n",
    "                            \n",
    "            else:\n",
    "                raise ValueError('Possible value of vectorizers are:{}'.format(\",\".join(self.vectorizer_list)))\n",
    "\n",
    "        return self.vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "#### Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Remember to load trained embedding models into memory before using them here! Use VEC.load()\n",
    "\n",
    "- Input File 1: List of raw Source Sentences.\n",
    "- Input File 2: List of raw Target Sentences.\n",
    "\n",
    "\n",
    "- Preprocessing + Vectorization\n",
    "- Outputs a dict file with ID, Original, Processed and Vectors for both files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT  - loaded.\n",
      "**Trained embedding models loaded into memory!**\n"
     ]
    }
   ],
   "source": [
    "## Load existing embed models ::\n",
    "\n",
    "VEC = Embedding()\n",
    "\n",
    "# load some pretrained embedding models like Glove, Google, FastText, etc.\n",
    "VEC.load(source='pretrained_glove', fp=pretrained_glove_fp)\n",
    "VEC.load(source='pretrained_google', fp=pretrained_google_fp)\n",
    "VEC.load(source='pretrained_fasttext', fp=pretrained_fasttext_fp)\n",
    "\n",
    "# load some in-house trained models like word2vec, doc2vec, word2vec intersected with glovem, etc.\n",
    "VEC.load(source='trained_word2vec', fp=trained_word2vec_fp)\n",
    "VEC.load(source='trained_word2vec_glove', fp=trained_word2vec_glove_fp)\n",
    "VEC.load(source='trained_word2vec_google', fp=trained_word2vec_google_fp)\n",
    "VEC.load(source='trained_fasttext', fp=trained_fasttext_fp)\n",
    "\n",
    "# load some fine-tuned models on domain data\n",
    "VEC.load(source='tuned_USE', fp=tuned_use_fp)\n",
    "VEC.load(source='tuned_sBERT', fp=tuned_sbert_fp)\n",
    "\n",
    "print(\"**Trained embedding models loaded into memory!**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute ::\n",
    "\n",
    "\n",
    "f1_sents = [\"how do i close account\", \"how do i close all the accounts\"]\n",
    "f2_sents = [\"how do i close account\"]\n",
    "\n",
    "#1. Get Cosine Similarity b/w two files\n",
    "res = vect.vectorize(f1_sents, f2_sents, vectorizers=['count', 'tfidf'])\n",
    "sim_matrix = cosine_similarity(res['tfidf']['file_1_vectors'], res['tfidf']['file_2_vectors'])\n",
    "for i, f1_sent in enumerate(f1_sents):\n",
    "    for j, f2_sent in enumerate(f2_sents):\n",
    "        print(f1_sent, \"---\", f2_sent, sim_matrix[i][j])\n",
    "\n",
    "# 2. Get vectors\n",
    "for source in ['pretrained_glove', 'pretrained_google', 'pretrained_fasttext', 'home_trained_fasttext']:\n",
    "    print('Source: %s' % (source))\n",
    "    print(word_embedding.get_vector(source=source, word='fail'))\n",
    "    print(len(word_embedding.get_vector(source=source, word='fail')))\n",
    "\n",
    "# 3. Get Most Similar Words\n",
    "for source in ['pretrained_glove', 'pretrained_google', 'pretrained_fasttext', 'home_trained_fasttext']:\n",
    "    print('Source: %s' % (source))\n",
    "    print(word_embedding.get_synonym(source=source, word='fail'))\n",
    "\n",
    "# 4. Get Distance\n",
    "# check WMD documentation.\n",
    "w1 = 'king'\n",
    "w2 = 'queen'\n",
    "for source in ['pretrained_glove', 'pretrained_google', 'pretrained_fasttext', 'home_trained_fasttext']:\n",
    "    print('Source: %s' % (source))\n",
    "    print(word_embedding.which_distance_between_two_words(source=source,word1=w1, word2=w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Text Similarity Unit (Unit 3/3)\n",
    "\n",
    "Input data for every method would be 2 files in dict format. File dict will be file['sentences'] & file['vectors']\n",
    "\n",
    "    - file['sentences'] == list of spacy preprocessed sentences\n",
    "    - file['vectors']   == dict of vectorized sentences, keys are: 'count', 'tfidf', 'word2vec', ... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Techniques tested:**\n",
    "    \n",
    "1. Bilingual Evaluation Understudy (BLEU)\n",
    "2. Levenshtein\n",
    "3. TFIDF + Euclidean Distance\n",
    "4. TFIDF + Cosine Distance\n",
    "5. Jaccard Distance\n",
    "6. Embeddings + Cosine Distance\n",
    "7. Embeddings + Smooth Inverse Frequency + Cosine Distance\n",
    "8. Spacy Vectorizer + Smooth Inverse Frequency + Cosine Distance\n",
    "9. Embeddings + Word Movers Distance\n",
    "10. LDA + Jannon-Shenon Distance\n",
    "11. Trained Attention LSTM Inception AutoEncoder Network Embeddings + Cosine Distance (**3rd BEST**)\n",
    "12. Finetuned Universal Sentence Encoder(USE) + Cosine Distance (**2nd BEST**)\n",
    "13. Fine-tuned Ultra-Fast Sentence Bert + Cosine Distance (**BEST**)\n",
    "14. Siamese Deep Neural Network (Pending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. BLEU Similairty\n",
    "- 1.0 = most similar; 0.0 = least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bleu(output_dict):\n",
    "\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    list_score = []\n",
    "    for i, source in tqdm.tqdm(enumerate(source_sentences), total=len(source_sentences)):\n",
    "        for j, target in enumerate(target_sentences):\n",
    "            # BLEU i.e. sentence_bleu(ref=['query'], res='response')\n",
    "            score = sentence_bleu([source], target)\n",
    "            list_score.append(score)\n",
    "\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. Levenshtein Similairty\n",
    "- 1.0 = most similar; 0.0 = least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def levenshtein(output_dict):\n",
    "\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    list_score = []\n",
    "    for i, source in tqdm.tqdm(enumerate(source_sentences), total=len(source_sentences)):\n",
    "        for j, target in enumerate(target_sentences):\n",
    "            # fuzzy i.e. fuzz.ratio(s1, s2)\n",
    "            score = fuzz.ratio(source, target)/100.0\n",
    "            list_score.append(score)\n",
    "\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. TFIDF + Euclidean based Similarity\n",
    "\n",
    "- 1.0 = most similar; 0.0 = least similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Euclidean distance: if the distance is **small** then words in the two sentences are **close** to each other.\n",
    "- Euclidean distance based similarity = 1/(1 + euclidean_distance(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tfidf_euclidean(output_dict):\n",
    "\n",
    "    source_vectors = output_dict['Vectorization']['tfidf']['file_1_vectors']\n",
    "    target_vectors = output_dict['Vectorization']['tfidf']['file_2_vectors']\n",
    "\n",
    "    # euclidean similarity = 1/[1 + euclidean_distance(v1, v2)]\n",
    "    sim_matrix = euclidean_distances(source_vectors, target_vectors)\n",
    "    list_score = sim_matrix.reshape(-1)\n",
    "    list_score = list(map(lambda x: 1/(1.0 + x), list_score))\n",
    "\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. TFIDF + Cosine Distance\n",
    "\n",
    "- Cosine: 1.0 = most similar; 0.0 = least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tfidf_cosine(output_dict):\n",
    "\n",
    "    source_vectors = output_dict['Vectorization']['tfidf']['file_1_vectors']\n",
    "    target_vectors = output_dict['Vectorization']['tfidf']['file_2_vectors']\n",
    "\n",
    "    # cosine is cosine_similarity(v1, v2)\n",
    "    sim_matrix = cosine_similarity(source_vectors, target_vectors)\n",
    "    list_score = sim_matrix.reshape(-1)\n",
    "\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5. Jaccard Similarity\n",
    "\n",
    "- Jaccard: 1.0 = most similar; 0.0 = least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def jaccard(output_dict):\n",
    "\n",
    "    def jaccard_similarity(query, document):\n",
    "        # accepts tokenized sentences only\n",
    "        intersection = set(query).intersection(set(document))\n",
    "        union = set(query).union(set(document))\n",
    "        return len(intersection)/len(union)\n",
    "\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    list_score = []\n",
    "    for i, source in tqdm.tqdm(enumerate(source_sentences), total=len(source_sentences)):\n",
    "        for j, target in enumerate(target_sentences):\n",
    "            # jaccard sim = jaccard_similarity(tokenized_doc_1, tokenized_doc_2)\n",
    "            score = jaccard_similarity(source.split(), target.split())\n",
    "            list_score.append(score)\n",
    "\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6. Embeddings + Cosine distance\n",
    "- 1.0 = most similar; 0.0 = least similar\n",
    "- embeddings = ['word2vec', 'fasttext', 'pretrained_homemade', 'pretrained_glove', 'pretrained_google', 'pretrained_fasttext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embedding_cosine(output_dict, vectorizer=None):\n",
    "\n",
    "    # check errors\n",
    "    if vectorizer is None:\n",
    "        raise ValueError('Pass a valid vectorizer!')\n",
    "    elif vectorizer not in output_dict['Vectorization'].keys():\n",
    "        raise ValueError(\"Load vectorizer first, use 'load_data(f1,f2, vectorizers=[{}])', when loading!\".format(vectorizer))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    source_vectors = output_dict['Vectorization'][vectorizer]['file_1_vectors']\n",
    "    target_vectors = output_dict['Vectorization'][vectorizer]['file_2_vectors']\n",
    "\n",
    "    # cosine = cosine_similarity(v1, v2)\n",
    "    sim_matrix = cosine_similarity(source_vectors, target_vectors)\n",
    "    list_score = sim_matrix.reshape(-1)\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 7.   Embeddings +  Smooth Inverse Frequency + Cosine Similarity\n",
    "- 1.0 = most similar; 0.0 = least similar\n",
    "- embeddings = ['word2vec', 'fasttext', 'pretrained_homemade', 'pretrained_glove', 'pretrained_google', 'pretrained_fasttext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embedding_sif_cosine(output_dict, vectorizer=None):\n",
    "\n",
    "    def sentence2vec(sentence_list, model):\n",
    "        # tokenize sentences (with words in model's vocab)\n",
    "        tokenised_sentence_list = [[word for word in sent.split() if word in model.wv.vocab] for sent in sentence_list]\n",
    "\n",
    "        # rare case when not a single word was found in vocab\n",
    "        for index, element in enumerate(tokenised_sentence_list):\n",
    "            if element == []:\n",
    "                tokenised_sentence_list[index] = [\"unknown\"]\n",
    "\n",
    "        # use SIF to get sentence vectors\n",
    "        word_counts = Counter(itertools.chain(*tokenised_sentence_list))\n",
    "        embedding_size = model.vector_size\n",
    "        a = 0.001\n",
    "\n",
    "        sentence_set = []\n",
    "        for sentence in tokenised_sentence_list:\n",
    "            vs = np.zeros(embedding_size)\n",
    "            sentence_length = len(sentence)\n",
    "            for word in sentence:\n",
    "                a_value = a / (a + word_counts[word])                         # smooth inverse frequency, SIF\n",
    "                vs = np.add(vs, np.multiply(a_value, model.wv[word]))         # vs += sif * word_vector\n",
    "            vs = np.divide(vs, sentence_length)                               # weighted average\n",
    "            sentence_set.append(vs)\n",
    "        return sentence_set\n",
    "\n",
    "    # check errors\n",
    "    if vectorizer is None:\n",
    "        raise ValueError('Pass a valid vectorizer!')\n",
    "    elif vectorizer not in output_dict['Vectorization'].keys():\n",
    "        raise ValueError(\"Load vectorizer first, use 'load_data(f1,f2, vectorizers=[{}])', when loading!\".format(vectorizer))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # get model\n",
    "    model = output_dict['Vectorization'][vectorizer]['model']\n",
    "\n",
    "    # get processed sentences\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    # gather all sentences for SIF\n",
    "    sentence_list = []\n",
    "    sentence_list.extend(source_sentences)\n",
    "    sentence_list.extend(target_sentences)\n",
    "\n",
    "    # get SIF vectors instead\n",
    "    all_vectors = sentence2vec(sentence_list, model)\n",
    "    source_vectors_SIF = all_vectors[ :len(source_sentences)]\n",
    "    target_vectors_SIF = all_vectors[len(source_sentences): ]\n",
    "\n",
    "    # cosine = cosine_similarity(sif_v1, sif_v2)\n",
    "    sim_matrix = cosine_similarity(source_vectors_SIF, target_vectors_SIF)\n",
    "    list_score = sim_matrix.reshape(-1)\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 8.   SPACY + Smooth Inverse Frequency + Cosine Similarity\n",
    "- 1.0 = most similar; 0.0 = least similar\n",
    "- Uses spacy large model to get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def spacy_sif_cosine(output_dict):\n",
    "\n",
    "    def sentence2vec_PCA(sentence_list, nlp_model):\n",
    "        # tokenize sentences (with words in spacy nlp's vocab)\n",
    "        tokenised_sentence_list = [[word for word in sent.split() if word in nlp_model.vocab] for sent in sentence_list]\n",
    "\n",
    "        # rare case when not a single word was found in vocab\n",
    "        for index, element in enumerate(tokenised_sentence_list):\n",
    "            if element == []:\n",
    "                tokenised_sentence_list[index] = [\"unknown\"]\n",
    "\n",
    "        # use SIF to get sentence vectors\n",
    "        word_counts = Counter(itertools.chain(*tokenised_sentence_list))\n",
    "        embedding_size = nlp_model.vocab[\"a\"].vector.shape[0]\n",
    "        a = 0.001\n",
    "\n",
    "        sentence_set = []\n",
    "        for sentence in tokenised_sentence_list:\n",
    "            vs = np.zeros(embedding_size)\n",
    "            sentence_length = len(sentence)\n",
    "            for word in sentence:\n",
    "                a_value = a / (a + word_counts[word])                                  # smooth inverse frequency, SIF\n",
    "                vs = np.add(vs, np.multiply(a_value, nlp_model.vocab[word].vector))    # vs += sif * word_vector\n",
    "            vs = np.divide(vs, sentence_length)                                        # weighted average\n",
    "            sentence_set.append(vs)\n",
    "\n",
    "        # calculate PCA of this sentence set\n",
    "        pca = PCA()\n",
    "        pca.fit(np.array(sentence_set))\n",
    "        u = pca.components_[0]                                                # PCA vector\n",
    "        u = np.multiply(u, np.transpose(u))\n",
    "\n",
    "        # Padding vectors (occurs if we have less sentences than embeddings_size)\n",
    "        if len(u) < embedding_size:\n",
    "            for i in range(embedding_size - len(u)):\n",
    "                u = np.append(u, 0)                                          # add needed extension for multiplication below\n",
    "\n",
    "        # resulting final sentence vectors\n",
    "        sentence_vecs = []\n",
    "        for vs in sentence_set:\n",
    "            sub = np.multiply(u,vs)\n",
    "            sentence_vecs.append(np.subtract(vs, sub))\n",
    "\n",
    "        return sentence_vecs\n",
    "\n",
    "    # Spacy's large model\n",
    "    model = nlp\n",
    "\n",
    "    # get processed sentences\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    # gather all sentences for SIF\n",
    "    sentence_list = []\n",
    "    sentence_list.extend(source_sentences)\n",
    "    sentence_list.extend(target_sentences)\n",
    "\n",
    "    # get SIF + PCA vectors with spacy\n",
    "    all_vectors = sentence2vec_PCA(sentence_list, model)\n",
    "    source_vectors_SIF = all_vectors[ :len(source_sentences)]\n",
    "    target_vectors_SIF = all_vectors[len(source_sentences): ]\n",
    "\n",
    "    # cosine = cosine_similarity(sif_v1, sif_v2)\n",
    "    sim_matrix = cosine_similarity(source_vectors_SIF, target_vectors_SIF)\n",
    "    list_score = sim_matrix.reshape(-1)\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 9.  Embeddings + Word Mover Distance\n",
    "\n",
    "- 1.0 = most similar; 0.0 = least similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "Word Mover's Distance (WMD) uses the word embeddings of the words in two texts to **measure the minimum distance** that the words in one text need to travel in **semantic space** to reach the words in the other text.\n",
    "\n",
    "The **WMD** is measured by measuring the minimum **Earth mover's distance** between each word in the two documents in **word2vec/fasttetx** space. if the distance is small then words in the two documents are close to each other.\n",
    "\n",
    "- Word Mover Distance = WMD(v1, v2)\n",
    "- Word Mover Distance based similarity = 1/(1 + WMD(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embeddings_WMD(output_dict, vectorizer):\n",
    "\n",
    "    def WMD(sent1, sent2):\n",
    "        wmd_distance = model.wmdistance(sent1.split(), sent2.split())\n",
    "        wmd_score = 1/(1.0 + wmd_distance)\n",
    "        return wmd_score\n",
    "\n",
    "    # get model\n",
    "    model = output_dict['Vectorization'][vectorizer]['model']\n",
    "\n",
    "    # processed sentences\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    list_score = []\n",
    "    for i, source in tqdm.tqdm(enumerate(source_sentences), total=len(source_sentences)):\n",
    "        for j, target in enumerate(target_sentences):\n",
    "            # WMD = model.wmdistance(tokenized_doc_1, tokenized_doc_2)\n",
    "            score = WMD(source, target)\n",
    "            list_score.append(score)\n",
    "\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "### 10. LDA + Jensen-Shannon distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## get processed sentences\n",
    "# source_sentences = output_dict['file1']['Source_processed']\n",
    "# target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "# sentence_list = []\n",
    "# sentence_list.extend(source_sentences)\n",
    "# sentence_list.extend(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Topic modelling using LDA\n",
    "\n",
    "# def train_LDA(sentence_list):\n",
    "#     # LDA\n",
    "#     # We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "#     # We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "\n",
    "#     # list of tokenized sentences\n",
    "#     tokenized_sentences = []\n",
    "#     tokenized_sentences = list(map(str.split, sentence_list))\n",
    "#     print(\"Total sentences:\", len(tokenized_sentences))\n",
    "\n",
    "#     start = time.time()\n",
    "#     num_topics = 100\n",
    "#     chunksize = 300\n",
    "\n",
    "#     dictionary = corpora.Dictionary(tokenized_sentences)\n",
    "#     corpus = [dictionary.doc2bow(doc) for doc in tokenized_sentences]\n",
    "\n",
    "#     # low alpha means each doc is only represented by a small number of topics, and vice versa\n",
    "#     # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "#     lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=1e-2, eta=0.5e-2,\n",
    "#                    chunksize=chunksize, minimum_probability=0.0, passes=2)\n",
    "\n",
    "#     print(\"Time to train LDA model on \", len(tokenized_sentences), \"sentences: \", round((time.time() - start)/60, 4), \"min\")\n",
    "#     return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "# lda = train_LDA(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Some Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Show topics with top num_words contributing to that topic\n",
    "\n",
    "# lda.show_topics(num_topics=2, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Show contributing words in a topic (defined by 'topicid')\n",
    "\n",
    "# lda.show_topic(topicid=87, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Represent a 'sentence' or 'article' in terms of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# select_any_index = 1\n",
    "# tokenized_sent = tokenized_sentences[select_any_index]\n",
    "# print(\"Sentence: \", tokenized_sent)\n",
    "\n",
    "# # get the topic contributions for the document chosen at random above\n",
    "# bow = dictionary.doc2bow(tokenized_sent)\n",
    "# doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n",
    "\n",
    "# # bar plot of topic distribution for this document\n",
    "# fig, ax = plt.subplots(figsize=(12,6));\n",
    "# patches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\n",
    "# ax.set_xlabel('Topic ID', fontsize=15)\n",
    "# ax.set_ylabel('Topic Contribution', fontsize=15)\n",
    "# ax.set_title(\"Topic Distribution for:- \" + \" \".join(tokenized_sent), fontsize=20)\n",
    "# ax.set_xticks(np.linspace(10,100,10))\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Top 5 contributing topics and their words\n",
    "# print(\"> Top Topics\")\n",
    "# for i in doc_distribution.argsort()[-5:][::-1]:\n",
    "#     print(i, lda.show_topic(topicid=i, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Calculate JS distance b/w two queries\n",
    "\n",
    "LDA trained on corpus = source + target\n",
    "- Tokenize both sentence lists\n",
    "- Get doc_distribution of each tokenized_sentence using LDA corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def LDA_jensen_shannon(output_dict):\n",
    "#     def get_topic_distribution(tokenized_sentence):\n",
    "#         # e.g. tokenized_sentence = ['how', 'open', 'account']\n",
    "#         bow = dictionary.doc2bow(tokenized_sentence)\n",
    "#         doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n",
    "#         return doc_distribution\n",
    "\n",
    "#     # get processed sentences\n",
    "#     source_sentences = output_dict['file1']['Source_processed']\n",
    "#     target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "#     list_score = []\n",
    "#     for i, source in tqdm.tqdm(enumerate(source_sentences), total=len(source_sentences)):\n",
    "#         for j, target in enumerate(target_sentences):\n",
    "\n",
    "#             # tokenize\n",
    "#             source_tokenized = source.split()\n",
    "#             target_tokenized = target.split()\n",
    "\n",
    "#             # get LDA topic distribution (shape = [1,num_topics])\n",
    "#             source_distribution = get_topic_distribution(source_tokenized)\n",
    "#             target_distribution = get_topic_distribution(target_tokenized)\n",
    "\n",
    "#             # JS distance\n",
    "#             js_dist = jensenshannon(source_distribution, target_distribution, base=2)\n",
    "#             if str(js_dist) == \"nan\":\n",
    "#                 js_dist = 0\n",
    "#             score = 1/(1.0 + js_dist)\n",
    "#             list_score.append(score)\n",
    "\n",
    "#     return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Takes approx. 3 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "### 11. Attention LSTM Inception AutoEncoder Network Embeddings + Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note:\n",
    "\n",
    "Training for this notebook was done in `Training__TextSimilarity__AttentionLST.ipynb`. The network was trained on all utterances + faqs for fasttext embeddings as well as for the network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, regularizer=None, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.regularizer = regularizer\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.context = self.add_weight(name='context',\n",
    "                                       shape=(input_shape[-1], 1),\n",
    "                                       initializer=initializers.RandomNormal(\n",
    "                                            mean=0.0, stddev=0.05, seed=None),\n",
    "                                       regularizer=self.regularizer,\n",
    "                                       trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        attention_in = K.exp(K.squeeze(K.dot(x, self.context), axis=-1))\n",
    "        attention = attention_in/K.expand_dims(K.sum(attention_in, axis=-1), -1)\n",
    "\n",
    "        if mask is not None:\n",
    "            # use only the inputs specified by the mask\n",
    "            # import pdb; pdb.set_trace()\n",
    "            attention = attention*K.cast(mask, 'float32')\n",
    "\n",
    "        weighted_sum = K.batch_dot(K.permute_dimensions(x, [0, 2, 1]), attention)\n",
    "        return weighted_sum\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print(input_shape)\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def FastTextVec(filename):\n",
    "    embeddings = {}\n",
    "    model = FastText.load(filename)\n",
    "    model.init_sims(replace=True)\n",
    "    words = list(model.wv.vocab)\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            coefs = np.asarray(model.wv[word], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences, max_len):\n",
    "    sent_nums = []\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    # print('sequences',sequences)\n",
    "    vocab = tokenizer.word_index\n",
    "    max_length = max([len(seq) for seq in sequences])\n",
    "    #print('len(seq)',max_length)\n",
    "    for seq in sequences:\n",
    "        sent_nums.append(len(seq))\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    return vocab, data, sent_nums, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createEmbeddingMatrix(word_index, embeddings_index):\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Network Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "MAX_NB_WORDS = 2000000\n",
    "TEST_SPLIT = 0.1\n",
    "VALIDATION_SPLIT = 0.1\n",
    "max_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Load Models : (1) Embedding & (2) Attention LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" 1. Load embedding model \"\"\"\n",
    "\n",
    "# Select model to load\n",
    "embed_model_3DR = \"../Innovation Tracks/2. Keyword Generation/models/embeddings/3DR_fasttext_200dim_Feb2021.model\"\n",
    "embed_model_UTTERANCES = \"models/UTTERANCES_fasttext_200dim_Mar2021.model\"\n",
    "\n",
    "# Load embedding model\n",
    "load_model_fp = embed_model_UTTERANCES\n",
    "embeddings = FastTextVec(load_model_fp)\n",
    "\n",
    "\n",
    "\"\"\" 2. Attention LSTM Autoencoder Model \"\"\" \n",
    "\n",
    "# Select model to load\n",
    "lstm_model_3DR = \"../Innovation Tracks/2. Keyword Generation/models/inception_autoencoder/3DR_model-fasttext_inception_atten_lstmautoencoder.h5\"\n",
    "lstm_model_UTTERANCES = \"models/UTTERANCES_model-fasttext_inception_atten_lstmautoencoder.h5\"\n",
    "\n",
    "# Load attn lstm model\n",
    "load_model_fp = lstm_model_UTTERANCES\n",
    "with CustomObjectScope({'Attention': Attention}):\n",
    "    lstm_autoencoder = load_model(load_model_fp)\n",
    "\n",
    "# get intermediate layer values\n",
    "layer_name = 'word_attention'\n",
    "intermediate_layer_model_diff_arch = Model(inputs = lstm_autoencoder.get_layer('input_layer').output,\n",
    "                                           outputs = lstm_autoencoder.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Model Prediciton\n",
    "\n",
    "- Output 1 (predict_embeddings_intermediate_output): Given a list of sents, predict its embeddings.\n",
    "- Output 2 (predict_similarity_attention_lstm_autoencoder): Using predicted embeddings from output_1, find cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "code_folding": [
     0,
     23
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict_embeddings_intermediate_output(sentence_list):\n",
    "    \"\"\"\n",
    "    Returns sentence level embeddings.\n",
    "    \"\"\"\n",
    "    if isinstance(sentence_list, list):\n",
    "        vocab, data, _, _ = createVocabAndData(sentence_list, max_len)    # tokenized text into dict(words: numbers)\n",
    "        embedding_mat = createEmbeddingMatrix(vocab, embeddings)          # create dict(numbers: embedding) mapping\n",
    "\n",
    "        # convert each sent into embedding\n",
    "        df_processed = pd.DataFrame(data)\n",
    "        text_features = df_processed.columns\n",
    "        embed = df_processed[text_features].values\n",
    "        embed = embed.astype('int')\n",
    "        embed = embedding_mat[embed]\n",
    "\n",
    "        # predict final network embeddings for each sent\n",
    "        intermediate_output = intermediate_layer_model_diff_arch.predict(embed)\n",
    "        normalized_output = preprocessing.normalize(intermediate_output)\n",
    "        return normalized_output\n",
    "    else:\n",
    "        print(\"Please pass a list!\")\n",
    "              \n",
    "def predict_similarity_attention_lstm_autoencoder(output_dict):\n",
    "    \"\"\"\n",
    "    Performs similarity metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get processed sentences\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    # predict embeddings for source_sentences\n",
    "    source_vectors = predict_embeddings_intermediate_output(source_sentences)\n",
    "\n",
    "    # predict embeddings for target_sentences\n",
    "    target_vectors = predict_embeddings_intermediate_output(target_sentences)\n",
    "\n",
    "    # calculate similarity score, cosine_similarity(v1, v2)\n",
    "    list_score = []\n",
    "    attn_sim_matrix = cosine_similarity(source_vectors, target_vectors)\n",
    "    list_score = attn_sim_matrix.reshape(-1)\n",
    "\n",
    "    return formatScore(list_score)\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "\n",
    "# get network's predicted embeddings at sentence level (with word atteniton)\n",
    "# embeds = predict_embeddings_intermediate_output([\"how do i open account\", \"how to close account\"])\n",
    "\n",
    "# pass the output_dict containing list of processed sentences to get simialrity metrics\n",
    "# sim_metrics = attention_lstm_autoencoder(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 12. Universal Sentence Encoder(USE) + Cosine Similarity\n",
    "\n",
    "- Using original google large USE model, finetuning with word addtion is not possible with current arch, hence ignoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def USE(output_dict):\n",
    "\n",
    "    def embed(lst):\n",
    "        chunk_size = 5000\n",
    "        batches = [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "        arr=[]\n",
    "        for x in tqdm.tqdm(batches):\n",
    "            arr.append(use_model(x))\n",
    "        return np.concatenate(arr)\n",
    "\n",
    "    # get processed sentences\n",
    "    source_sentences = output_dict['file1']['Source_processed']\n",
    "    target_sentences = output_dict['file2']['Target_processed']\n",
    "\n",
    "    # get embeddings\n",
    "    source_embeddings = embed(source_sentences)\n",
    "    target_embeddings = embed(target_sentences)\n",
    "\n",
    "    # similarity\n",
    "    corr = np.inner(source_embeddings, target_embeddings)\n",
    "    # OR use: corr = cosine_similarity(source_embeddings, target_embeddings)\n",
    "\n",
    "    list_score = corr.reshape(-1)\n",
    "    return formatScore(list_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Source embeddings created. 0.290 mins;  shape=(137698, 512)\n",
    "- Target embeddings created. 0.001 mins;  shape=(966, 512)\n",
    "\n",
    "- USE -- Time taken (mins): 11.619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### <ins>Execute</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute using input lists and check the methods if they are working. Use the next section of final execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Remember to load trained embedding models into memory before using them here! Use VEC.load()\n",
    "\n",
    "- `Input File 1`: List of raw Source Sentences.\n",
    "- `Input File 2`: List of raw Target Sentences.\n",
    "- `Output`: pd.DataFrame, dict (file with ID, Original, Processed and Vectors for both files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "code_folding": [
     0,
     57
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(file1, file2, list_of_vectorizers=['tfidf']):\n",
    "    \"\"\"\n",
    "    Module to load two files and vectorize them. Files can be 'pd.Series', pd.DataFrame or '.txt' file with sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Processing dataset...\")\n",
    "    # read\n",
    "    if isinstance(file1, pd.DataFrame):\n",
    "        file1.rename(columns={list(file1)[0]:'Source'}, inplace = True)\n",
    "        file2.rename(columns={list(file2)[0]:'Target'}, inplace = True)\n",
    "    elif isinstance(file1, pd.Series):\n",
    "        file1 = pd.DataFrame({'Source': file1})\n",
    "        file2 = pd.DataFrame({'Target': file2})\n",
    "    elif isinstance(file1, list):\n",
    "        file1 = pd.DataFrame({'Source': pd.Series(file1)})\n",
    "        file2 = pd.DataFrame({'Target': pd.Series(file2)})\n",
    "    elif ntpath.basename(file1).endswith(\".txt\"):\n",
    "        with open(file1) as f: fileobject = io.StringIO(f.read().replace('\"\"', '\"')); f.close()\n",
    "        file1 = pd.read_csv(fileobject, sep='\\t',  lineterminator='\\n', names=['Source']).reset_index(drop=True)\n",
    "        with open(file2) as f: fileobject = io.StringIO(f.read().replace('\"\"', '\"')); f.close()\n",
    "        file2 = pd.read_csv(fileobject, sep='\\t',  lineterminator='\\n', names=['Target']).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError('Error: Please use dataframe or txt format only!')\n",
    "    file1['Source_ID'] = [f\"file1_{i}\" for i in range(len(file1))]\n",
    "    file2['Target_ID'] = [f\"file1_{i}\" for i in range(len(file2))]\n",
    "    \n",
    "    \n",
    "    # Pre-processing\n",
    "    preprocessText = preprocessText_spacy(resources_dir_path)\n",
    "    file1['Source_processed'] = [\" \".join(x[\"lemma_list\"]).lower() \\\n",
    "                                 if len(str(x).strip()) > 2 else None \\\n",
    "                                 for x in preprocessText.run_pipeline(file1.Source.tolist(), operations=['basic', 'deep', 'spacy'])]\n",
    "    \n",
    "    file2['Target_processed'] = [\" \".join(x[\"lemma_list\"]).lower() \\\n",
    "                                 if len(str(x).strip()) > 2 else None \\\n",
    "                                 for x in preprocessText.run_pipeline(file2.Target.tolist(), operations=['basic', 'deep', 'spacy'])]\n",
    "    print(\"> Pre-processing done.\")\n",
    "\n",
    "    \n",
    "    # Vectorization\n",
    "    output = {}\n",
    "    result = VEC.vectorize(file1['Source_processed'].tolist(),\n",
    "                           file2['Target_processed'].tolist(),\n",
    "                           list_of_vectorizers)\n",
    "    output = {\n",
    "        \"file1\": file1.to_dict('list'),\n",
    "        \"file2\": file2.to_dict('list'),\n",
    "        \"Vectorization\": result }\n",
    "    print(\"> Vectorization done. Vectors stored in 'dict' format: output['Vectorization'] \")\n",
    "    \n",
    "    file1 = pd.DataFrame.from_dict(output['file1'])\n",
    "    file2 = pd.DataFrame.from_dict(output['file2'])\n",
    "    file1['key'], file2['key'] = 0, 0\n",
    "    dataset = file1.merge(file2, how='outer').drop(columns=['key'])\n",
    "\n",
    "    return dataset, output\n",
    "\n",
    "def save_result(df, fp):\n",
    "    # Save large df into multiple split files\n",
    "    chunk_size = 500000\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in tqdm.tqdm(range(num_chunks)):\n",
    "        fp = fp + \"_{0:0=3d}.txt\".format(i+1)\n",
    "        subset = df[i*chunk_size: (i+1)*chunk_size]\n",
    "        subset.to_csv(fp, header=True, index=None, sep='\\t', mode='a')\n",
    "    print(\"Process executed and saved. Location:\", fp)\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# df, output_dict = load_data(file_1, file_2, list_of_vectorizers=['tfidf', 'count', 'BERT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "stopwords loaded.\n",
      "synonyms_noun_verb loaded.\n",
      "contractions loaded.\n",
      "> Pre-processing done.\n",
      "\n",
      " tfidf\n",
      "Training...\n",
      "Vectorizing...\n",
      "Time ms:0.002544403076171875\n",
      "\n",
      " BERT\n",
      "Vectorizing...\n",
      "Time ms:0.0006747245788574219\n",
      "> Vectorization done. Vectors stored in 'dict' format: output['Vectorization'] \n"
     ]
    }
   ],
   "source": [
    "file_1 = [\"How do i close UTMA account?\", \"How do i open IRA account ??\"]\n",
    "\n",
    "file_2 = [\"How do i close a trust account?\",\n",
    "          \"How to close UTMA account?\",\n",
    "          \"How do i open IRA account?\" ,\n",
    "          \"Docs required for closing acc\"]\n",
    "\n",
    "df, output_dict = load_data(file_1, file_2, list_of_vectorizers=['tfidf', 'BERT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df['BLEU'] = bleu(output_dict)\n",
    "df['lev'] = levenshtein(output_dict)\n",
    "df['tfidf_euclidean'] = tfidf_euclidean(output_dict)\n",
    "df['tfidf_cosine'] = tfidf_cosine(output_dict)\n",
    "df['jaccard'] = jaccard(output_dict)\n",
    "\n",
    "df['cos_word2vec'] = embedding_cosine(output_dict, vectorizer=\"word2vec\")\n",
    "df['cos_fasttext'] = embedding_cosine(output_dict, vectorizer=\"fasttext\")\n",
    "df['cos_homemade_fasttext'] = embedding_cosine(output_dict, vectorizer=\"homemade_fasttext\")\n",
    "df['cos_preglove'] = embedding_cosine(output_dict, vectorizer=\"pretrained_glove\")\n",
    "df['cos_pregoogle'] = embedding_cosine(output_dict, vectorizer=\"pretrained_google\")\n",
    "df['cos_prefasttext'] = embedding_cosine(output_dict, vectorizer=\"pretrained_fasttext\")\n",
    "df['cos_w2v_glove'] = embedding_cosine(output_dict, vectorizer=\"word2vec_glove\")\n",
    "df['cos_w2v_google'] = embedding_cosine(output_dict, vectorizer=\"word2vec_google\")\n",
    "df['cos_SIF_word2vec'] = embedding_sif_cosine(output_dict, vectorizer=\"word2vec\")\n",
    "df['cos_SIF_fasttext'] = embedding_sif_cosine(output_dict, vectorizer=\"fasttext\")\n",
    "df['cos_SIF_homemade_fasttext'] = embedding_sif_cosine(output_dict, vectorizer=\"homemade_fasttext\")\n",
    "df['cos_SIF_preglove'] = embedding_sif_cosine(output_dict, vectorizer=\"pretrained_glove\")\n",
    "df['cos_SIF_pregoogle'] = embedding_sif_cosine(output_dict, vectorizer=\"pretrained_google\")\n",
    "df['cos_SIF_prefasttext'] = embedding_sif_cosine(output_dict, vectorizer=\"pretrained_fasttext\")\n",
    "df['cos_SIF_w2v_glove'] = embedding_sif_cosine(output_dict, vectorizer=\"word2vec_glove\")\n",
    "df['cos_SIF_w2v_google'] = embedding_sif_cosine(output_dict, vectorizer=\"word2vec_google\")\n",
    "\n",
    "df['spacy_SIF_cosine'] = spacy_sif_cosine(output_dict)\n",
    "\n",
    "df['wmd_word2vec'] = embeddings_WMD(output_dict, vectorizer=\"word2vec\")\n",
    "df['wmd_fasttext'] = embeddings_WMD(output_dict, vectorizer=\"fasttext\")\n",
    "df['wmd_homemade_fasttext'] = embeddings_WMD(output_dict, vectorizer=\"homemade_fasttext\")\n",
    "df['wmd_preglove'] = embeddings_WMD(output_dict, vectorizer=\"pretrained_glove\")\n",
    "df['wmd_pregoogle'] = embeddings_WMD(output_dict, vectorizer=\"pretrained_google\")\n",
    "df['wmd_prefasttext'] = embeddings_WMD(output_dict, vectorizer=\"pretrained_fasttext\")\n",
    "df['wmd_w2v_glove'] = embeddings_WMD(output_dict, vectorizer=\"word2vec_glove\")\n",
    "df['wmd_w2v_google'] = embeddings_WMD(output_dict, vectorizer=\"word2vec_google\")\n",
    "\n",
    "df['LDA'] = LDA_jensen_shannon(output_dict)\n",
    "df['Attn_lstm'] = attention_lstm_autoencoder(output_dict)\n",
    "df['USE'] = USE(output_dict)\n",
    "df['Fast_BERT'] = fast_bert(output_dict)\n",
    "\n",
    "save_result(df)\n",
    "\n",
    "print(\"=> Time taken(mins):\", (time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- source embeddings created. 0.290 mins;  shape= (137698, 512)\n",
    "- target embeddings created. 0.002 mins;  shape= (966, 512)\n",
    "- Time taken (mins): 20.619174893697103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Use-case of Text Similarity - Example\n",
    "---\n",
    "1. Get a dataset\n",
    "2. Explore listed methods\n",
    "3. Evaluate based on domain knowledge and judgment\n",
    "4. Output best method results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "# Ideally a truth dataset could work - dataset with question pairs with a similairty value\n",
    "dataset_path = \"....\"\n",
    "\n",
    "\n",
    "df_validation = pd.read_excel(dataset_path)\n",
    "df_validation = df_validation.rename(columns={\"Search Term\": \"Utterance\", \"FAQ Question\": \"FAQ\"})\n",
    "\n",
    "# tokenize and filter sentences here:\n",
    "min_words = 4\n",
    "sample_size = 30\n",
    "dup_count = 5        # count of each utterance\n",
    "\n",
    "df_val = df_validation[df_validation['# Words'] >= min_words][['Utterance', 'FAQ']].dropna().reset_index(drop=True).copy()\n",
    "uttFreq = FreqDist(df_val['Utterance'].tolist())\n",
    "len(uttFreq)\n",
    "candidate_utterances = [utt for utt, freq in uttFreq.items() if freq >= dup_count][:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create positive sample space\n",
    "\n",
    "\n",
    "positive_samples = pd.DataFrame()\n",
    "\n",
    "for utt in tqdm.tqdm(candidate_utterances):\n",
    "    subset = df_val[df_val['Utterance'] == utt]\n",
    "    # size for each utterance = dup count\n",
    "    subset = subset[:dup_count]\n",
    "    subset['Similarity'] = 1\n",
    "    positive_samples = positive_samples.append(subset)\n",
    "\n",
    "positive_samples = positive_samples.reset_index(drop=True)\n",
    "positive_samples = positive_samples.rename(columns={\"Utterance\": \"Source\", \"FAQ\": \"Target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create negative sample space\n",
    "\n",
    "\n",
    "negative_samples = pd.DataFrame()\n",
    "\n",
    "for utt in tqdm.tqdm(candidate_utterances):\n",
    "    # choose subset\n",
    "    subset = df_val[df_val['Utterance'] != utt]\n",
    "\n",
    "    # modify\n",
    "    subset = shuffle(subset, random_state=7)\n",
    "    subset = subset[:100].reset_index(drop=True)\n",
    "    subset = subset[['FAQ']]\n",
    "    subset['Utterance'] = utt\n",
    "\n",
    "    # cal similarity using baseline method\n",
    "    embedding_method = \"tfidf\"\n",
    "\n",
    "    f1 = list(subset['Utterance'].unique())\n",
    "    f2 = list(subset['FAQ'].unique())\n",
    "\n",
    "    subset_dict = load_data(f1, f2, vectorizers=[embedding_method])\n",
    "    file1 = pd.DataFrame.from_dict(subset_dict['file1'])\n",
    "    file2 = pd.DataFrame.from_dict(subset_dict['file2'])\n",
    "    file1['key'] = 0\n",
    "    file2['key'] = 0\n",
    "    subset_df = file1.merge(file2, how='outer').drop(columns=['key'])\n",
    "\n",
    "    subset_df['cosine'] = tfidf_cosine(subset_dict)\n",
    "\n",
    "    # select dissimillar pairs (threshold kept very low)\n",
    "    subset = shuffle(subset_df[subset_df['cosine'] < 0.05], random_state=5)\n",
    "    subset = subset[['Source', 'Target']].reset_index(drop=True)\n",
    "\n",
    "    # size for each utterance = dup count\n",
    "    subset = subset[:dup_count]\n",
    "    subset['Similarity'] = 0\n",
    "    negative_samples = negative_samples.append(subset)\n",
    "\n",
    "negative_samples = negative_samples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## final dataset ready for computation\n",
    "\n",
    "\n",
    "validation = positive_samples.append(negative_samples).sort_values(by=['Source', 'Similarity'], ascending=[True, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Exploring methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(validation_set):\n",
    "\n",
    "    # files\n",
    "    f1 = list(validation_set['Source'].unique())\n",
    "    f2 = list(validation_set['Target'].unique())\n",
    "\n",
    "    # validation dict\n",
    "    val_dict = load_data(f1, f2, vectorizers=['count', 'tfidf', 'word2vec', 'fasttext', 'homemade_fasttext',\n",
    "                                              'pretrained_glove', 'pretrained_google', 'pretrained_fasttext',\n",
    "                                              'word2vec_glove', 'word2vec_google'])\n",
    "\n",
    "    # validation df\n",
    "    file1 = pd.DataFrame.from_dict(val_dict['file1'])\n",
    "    file2 = pd.DataFrame.from_dict(val_dict['file2'])\n",
    "    file1['key'] = 0\n",
    "    file2['key'] = 0\n",
    "    df_val = file1.merge(file2, how='outer').drop(columns=['key'])\n",
    "\n",
    "    # insert: truth values\n",
    "    scores = []\n",
    "    for s, t in zip(df_val['Source'], df_val['Target']):\n",
    "        truth_set = validation_set[(validation_set['Source'] == s) & (validation_set['Target'] == t)]\n",
    "        if len(truth_set) > 0:\n",
    "            sim_score = truth_set['Similarity'].values[0]  # 0 or 1\n",
    "        else:\n",
    "            sim_score = \"none\"\n",
    "        scores.append(sim_score)\n",
    "    df_val['Similarity'] = scores\n",
    "\n",
    "\n",
    "    # SIMILARITY TECHNIQUES\n",
    "\n",
    "    df_val['BLEU'] = bleu(val_dict)\n",
    "    df_val['lev'] = levenshtein(val_dict)\n",
    "    df_val['tfidf_euclidean'] = tfidf_euclidean(val_dict)\n",
    "    df_val['tfidf_cosine'] = tfidf_cosine(val_dict)\n",
    "    df_val['jaccard'] = jaccard(val_dict)\n",
    "\n",
    "    df_val['cos_word2vec'] = embedding_cosine(val_dict, vectorizer=\"word2vec\")\n",
    "    df_val['cos_fasttext'] = embedding_cosine(val_dict, vectorizer=\"fasttext\")\n",
    "    df_val['cos_homemade_fasttext'] = embedding_cosine(val_dict, vectorizer=\"homemade_fasttext\")\n",
    "    df_val['cos_preglove'] = embedding_cosine(val_dict, vectorizer=\"pretrained_glove\")\n",
    "    df_val['cos_pregoogle'] = embedding_cosine(val_dict, vectorizer=\"pretrained_google\")\n",
    "    df_val['cos_prefasttext'] = embedding_cosine(val_dict, vectorizer=\"pretrained_fasttext\")\n",
    "    df_val['cos_w2v_glove'] = embedding_cosine(val_dict, vectorizer=\"word2vec_glove\")\n",
    "    df_val['cos_w2v_google'] = embedding_cosine(val_dict, vectorizer=\"word2vec_google\")\n",
    "\n",
    "    df_val['cos_SIF_word2vec'] = embedding_sif_cosine(val_dict, vectorizer=\"word2vec\")\n",
    "    df_val['cos_SIF_fasttext'] = embedding_sif_cosine(val_dict, vectorizer=\"fasttext\")\n",
    "    df_val['cos_SIF_homemade_fasttext'] = embedding_sif_cosine(val_dict, vectorizer=\"homemade_fasttext\")\n",
    "    df_val['cos_SIF_preglove'] = embedding_sif_cosine(val_dict, vectorizer=\"pretrained_glove\")\n",
    "    df_val['cos_SIF_pregoogle'] = embedding_sif_cosine(val_dict, vectorizer=\"pretrained_google\")\n",
    "    df_val['cos_SIF_prefasttext'] = embedding_sif_cosine(val_dict, vectorizer=\"pretrained_fasttext\")\n",
    "    df_val['cos_SIF_w2v_glove'] = embedding_sif_cosine(val_dict, vectorizer=\"word2vec_glove\")\n",
    "    df_val['cos_SIF_w2v_google'] = embedding_sif_cosine(val_dict, vectorizer=\"word2vec_google\")\n",
    "\n",
    "    df_val['spacy_SIF_cosine'] = spacy_sif_cosine(val_dict)\n",
    "\n",
    "    df_val['wmd_word2vec'] = embeddings_WMD(val_dict, vectorizer=\"word2vec\")\n",
    "    df_val['wmd_fasttext'] = embeddings_WMD(val_dict, vectorizer=\"fasttext\")\n",
    "    df_val['wmd_homemade_fasttext'] = embeddings_WMD(val_dict, vectorizer=\"homemade_fasttext\")\n",
    "    df_val['wmd_preglove'] = embeddings_WMD(val_dict, vectorizer=\"pretrained_glove\")\n",
    "    df_val['wmd_pregoogle'] = embeddings_WMD(val_dict, vectorizer=\"pretrained_google\")\n",
    "    df_val['wmd_prefasttext'] = embeddings_WMD(val_dict, vectorizer=\"pretrained_fasttext\")\n",
    "    df_val['wmd_w2v_glove'] = embeddings_WMD(val_dict, vectorizer=\"word2vec_glove\")\n",
    "    df_val['wmd_w2v_google'] = embeddings_WMD(val_dict, vectorizer=\"word2vec_google\")\n",
    "\n",
    "    # USE\n",
    "    df_val['USE'] = USE(val_dict)\n",
    "\n",
    "    ## LDA\n",
    "    ## not-used: df_val['LDA'] = LDA_jensen_shannon(val_dict)\n",
    "\n",
    "    # ATTN LSTM\n",
    "    df_val['Attn_lstm'] = attention_lstm_autoencoder(val_dict)\n",
    "\n",
    "    return val_dict, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Dataset (along with equal sim values)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict, df_val = run_validation(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_val = df_val[df_val['Similarity'] != 'none'].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = [0.20, 0.30, 0.40, 0.45, 0.50, 0.55, 0.58, 0.60, 0.62, 0.64, 0.66, 0.68, 0.70, 0.75, 0.80, 0.90, 0.95]\n",
    "\n",
    "## find threshold for accuracy\n",
    "acc_scores = {}\n",
    "for col in df_val.columns[7:]:\n",
    "    list_score = []\n",
    "    for thr in threshold:\n",
    "        final_df_val[\"acc_{}\".format(col)] = final_df_val[col].apply(lambda x: 1 if x > thr else 0)\n",
    "        score = accuracy_score(final_df_val['Similarity'].tolist(), final_df_val[\"acc_{}\".format(col)].tolist())\n",
    "        list_score.append(score)\n",
    "    acc_scores[\"acc_{}\".format(col)] = list_score\n",
    "\n",
    "colors = list(\"rgbcmyk\")\n",
    "for x,y in acc_scores.items():\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(x)\n",
    "    plt.scatter(threshold, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_threshold = 0.50\n",
    "\n",
    "# Display accuracies using baseline threshold\n",
    "for col in df_val.columns[7:]:\n",
    "    final_df_val[\"acc_{}\".format(col)] = final_df_val[col].apply(lambda x: 1 if x > final_threshold else 0)\n",
    "    score = accuracy_score(final_df_val['Similarity'].tolist(), final_df_val[\"acc_{}\".format(col)].tolist())\n",
    "    print(col, \"-->\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING REPORT...\n",
    "\n",
    "final_df_val.to_excel(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Display results for the best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    return use_model(input)\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=2.3)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    g = sns.heatmap(corr, xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, cmap=\"YlOrRd\", ax=ax)\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(output_dict):\n",
    "    source_sents = output_dict['file1']['Source']\n",
    "    source_embeds = output_dict['file1']['Source_processed']\n",
    "    target_sents = output_dict['file2']['Target']\n",
    "    target_embeds = output_dict['file2']['Target_processed']\n",
    "\n",
    "    message_embeddings_ = embed(source_embeds + target_embeds)\n",
    "    plot_similarity(source_sents + target_sents, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_examples = [\n",
    "    'close acc',\n",
    "    'how do I open account',\n",
    "    'what IRA stands for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_examples = [\n",
    "    'How do I close an account?',\n",
    "    'Documents required for opening account',\n",
    "    'What is IRA?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_plot(load_data(source_examples, target_examples, vectorizers=['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "Similarity_ComparisonTechniques",
  "createdOn": 1648032776580,
  "creationTag": {
   "lastModifiedBy": {
    "login": "pranjp"
   },
   "lastModifiedOn": 1648032776580,
   "versionNumber": 0
  },
  "creator": "pranjp",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "python_38:Python",
   "language": "python",
   "name": "conda-env-python_38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "modifiedBy": "pranjp",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
