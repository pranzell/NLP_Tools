{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fast Semantic Similarity\n",
    "\n",
    "\n",
    "Fast sparse matrix multiplication using sentenceBert vectorization and cosine to compute contextual similarity between two text sentences. Create nGrams for each sentence and compares with the rest to generate sparse similairty matrix.\n",
    "\n",
    "\n",
    "The following pipeline computes semantic similarity in **two** approaches:\n",
    "\n",
    "    1. If there's only * one * input file with you (aka. source file):\n",
    "        \n",
    "        -  The code finds duplicates and very similar text sentences from source file.\n",
    "        -  The output is a dataframe in a cluster format, i.e. clusters of similar sentences, duplciates, etc.\n",
    "        -  The orginal ids are intact so that one could back-track to source file.\n",
    "        -  You need to set a threshold to filter similar sentences, defined by `source_min_similarity`. \n",
    "        \n",
    "        \n",
    "    2. If there are * two * files, and you wish to compare and get similar sentences from them:\n",
    "    \n",
    "        - The code performs semantic similarity b/w a master file (aka. source) and comparsion file (aka. target).\n",
    "        - The output is a dataframe with compared sentences and similarity scores.\n",
    "        - Two IDs are worked out in the process, one belongs to master (source) and one to comparison (target).\n",
    "        - You need to set a threshold to filter similar sentences, defined by `min_similarity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Steps to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Get Python >= 3.6.0\n",
    "\n",
    "- Put your desired file in connected Input Folder.\n",
    "\n",
    "\n",
    "- Set Directory paths\n",
    "\n",
    "- Your input files should atleast contain `ID` and `TEXT` columns!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- Select a `Master file` (file you wish to run similarity on OR find duplicates and very similar rows). This will be called as \"Master\" or \"Source\".\n",
    "\n",
    "\n",
    "- (Optional) Select a `Comparsion file` (if you wish to compare similarity with Master file). This will be called as \"Comparison\" or \"Target\".\n",
    "\n",
    "---\n",
    "\n",
    "- `Approach 1` ---> To collect duplicates, very similar ids (defined by `source_min_similarity`) on master file.\n",
    "    - Returns dataframe with:\n",
    "        - identified duplicate rows and\n",
    "        - very similar rows,\n",
    "        - initial generic cluster id\n",
    "\n",
    "\n",
    "---\n",
    "- `Approach 2` ---> To perform textual similarity computation b/w master and comparsion file (defined by `min_similarity`).\n",
    "    - Returns dataframe with:\n",
    "        - identified duplicate rows in master file\n",
    "        - very similar rows on master file,\n",
    "        - initial generic cluster id on master file,\n",
    "        - most similar from comparison file record\n",
    "        - similar found reocrds from comparison file\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Imports\n",
    "2. Directory Setup\n",
    "3. Preprocessing Unit\n",
    "4. Vectorization Uit\n",
    "5. Pair-Wise Fast-Similarity Matrix Creation Unit\n",
    "6. Similarity Computation Unit\n",
    "7. Execute (aka. final :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/python_38/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "/tmp/ipykernel_723/3034424534.py:79: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  from scipy.sparse.csr import csr_matrix\n",
      "/tmp/ipykernel_723/3034424534.py:80: DeprecationWarning: Please use `lil_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.lil` namespace is deprecated.\n",
      "  from scipy.sparse.lil import lil_matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/python_38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-27 00:26:31.991178: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-27 00:26:33.872360: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-27 00:26:33.872486: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-27 00:26:33.872501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-27 00:26:34.988961: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-27 00:26:34.989002: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-27 00:26:34.989029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy loaded.\n",
      "TensorFlow loaded.\n",
      "PyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "'''Update code from Python 3.6.10 to a stable Kernel Python Version 3.8.0 '''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "#from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/someppath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "# spacy_model_data_path = \"/Users/pranjalpathak/opt/anaconda3/envs/Python_3.6/lib/python3.6/site-packages/en_core_web_lg/en_core_web_lg-2.2.5\"\n",
    "nlp = spacy.load('en_core_web_lg')  # disabling: nlp = spacy.load(spacy_data_path, disable=['ner'])\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# TF & Keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.layers import Layer, InputSpec, BatchNormalization, Embedding, LSTM, Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import pad_sequences, CustomObjectScope\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers as initializers, regularizers, constraints, optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.models import Sequential, Model, load_model\n",
    "import tensorflow_hub as hub\n",
    "print(\"TensorFlow loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "input_data_fp = \"\"\n",
    "output_data_fp = \"\"\n",
    "\n",
    "# common NLP resources\n",
    "resources_dir_path = \"./data/resources/\"\n",
    "\n",
    "# embedding models\n",
    "sbert_model_fp = \"./models/sentence-transformers-models/all-distilroberta-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing Unit (Unit 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessText:\n",
    "    \n",
    "    def __init__(self, resources_dir_path, custom_vocab=[], do_lemma=False):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n",
    "        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n",
    "        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n",
    "        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n",
    "        self.preserve_key = \"<$>\" # preserve special vocab\n",
    "        self.vocab_list = custom_vocab\n",
    "        self.preseve = True if len(custom_vocab) > 0 else False\n",
    "        self.load_resources()\n",
    "        self.do_lemma = do_lemma\n",
    "        return\n",
    "    \n",
    "    def load_resources(self):\n",
    "        \n",
    "        ### Build Vocab Model --> Words to keep\n",
    "        self.vocab_list = set(map(str.lower, self.vocab_list))\n",
    "        self.vocab_dict = {w: self.preserve_key.join(w.split()) for w in self.vocab_list}\n",
    "        self.re_retain_words = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n",
    "        \n",
    "        ### Build Stopwords Model --> Words to drop/delete\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        self.stopwords = list(sorted(set(self.stopwords).difference(self.vocab_list)))\n",
    "\n",
    "        ### Build Contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "        \n",
    "        ### Build Chat-words\n",
    "        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.chat_words_map_dict, self.chat_words_list = {}, []\n",
    "            chat_words = [x.rstrip() for x in f.readlines()]\n",
    "            for line in chat_words:\n",
    "                cw = line.split(\"=\")[0]\n",
    "                cw_expanded = line.split(\"=\")[1]\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "            self.chat_words_list = set(self.chat_words_list)\n",
    "        \n",
    "        ### Bukd social markups\n",
    "        # emoticons\n",
    "        with open(self.emoticons_file, \"r\") as f:\n",
    "            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n",
    "        # emojis\n",
    "        self.emojis = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        # greeting\n",
    "        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.greeting_words = [x.rstrip() for x in f.readlines()]\n",
    "        # signature\n",
    "        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.signature_words = [x.rstrip() for x in f.readlines()]\n",
    "        # spell-corrector\n",
    "        self.spell_checker = SpellChecker()   \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def reserve_keywords_from_cleaning(self, text, reset=False):\n",
    "        \"\"\" \n",
    "        Finds common words from a user-provided list of special keywords to preserve them from \n",
    "        cleaning steps. Identifies every special keyword and joins them using `self.preserve_key` during the \n",
    "        cleaning steps, and later resets it back to original word in the end.\n",
    "        \"\"\"\n",
    "        if reset is False:\n",
    "            # compile using a dict of words and their expansions, and sub them if found!\n",
    "            match_and_sub = self.re_retain_words.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n",
    "        else:\n",
    "            # reverse the change! - use this at the end of preprocessing\n",
    "            text = text.replace(self.preserve_key, \" \")\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    def basic_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            sent = str(sent).strip()\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def deep_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            # normalize text to \"utf-8\" encoding\n",
    "            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Mark important keywords such as: Domain specific, Question words(wh-words), etc, using \n",
    "            # \"self.vocab_list\". Words from this list if found in any input sentence shall be joined using \n",
    "            # a key (self.preserve_key) during pre-processing step, and later un-joined to retain them.\n",
    "            #\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=False)\n",
    "            #\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            # remove Emojis\n",
    "            sent = self.emojis.sub(r'', sent)\n",
    "            # remove emoticons\n",
    "            sent = self.emoticons.sub(r'', sent)\n",
    "            # remove common chat-words\n",
    "            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "            #\n",
    "            # removing punctuations ðŸ”¥ðŸ”¥\n",
    "            # *** disable them, when sentence structure needs to be retained ***\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n",
    "            \n",
    "            # remove sentence de-limitters ðŸ”¥ðŸ”¥\n",
    "            # *** disable them, when sentence boundary/ending is important ***\n",
    "            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)\n",
    "\n",
    "            # keep only text & numbers ðŸ”¥ðŸ”¥\n",
    "            # *** enable them, when only text and numbers matter! *** \n",
    "            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            \n",
    "            # correct spelling mistakes ðŸ”¥ðŸ”¥\n",
    "            # *** enable them when english spelling mistakes matter *** \n",
    "            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n",
    "            #\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "\n",
    "            # Remove stopwords\n",
    "            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords and \n",
    "                                                                 token.lemma_ not in self.stopwords)\n",
    "            # Lemmatize\n",
    "            if self.do_lemma:\n",
    "                sent = \" \".join(token.lemma_ for token in nlp(sent))\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).lower().strip()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Reverse the custom joining now to un-join the special words found!\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=True)\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            cleaned_sentences.append(sent.strip().lower())\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def spacy_get_pos_list(self, results):\n",
    "        word_list, pos_list, lemma_list, ner_list, start_end_list = [], [], [], [], []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                # (1). save tokens\n",
    "                word_list.append(token['word'])\n",
    "                # (2). save pos\n",
    "                pos_list.append(token['pos'])\n",
    "                # (3). save lemmas\n",
    "                lemma = token['lemma'].lower()\n",
    "                if lemma in self.stopwords: continue\n",
    "                lemma_list.append(lemma)\n",
    "                # (4). save NER\n",
    "                ner_list.append(token['ner'])\n",
    "                # (5). save start\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        output = {\"word_list\": word_list, \n",
    "                  \"lemma_list\": lemma_list, \n",
    "                  \"token_start_end_list\": start_end_list,\n",
    "                  \"pos_list\": pos_list, \"ner_list\": ner_list}\n",
    "        return output\n",
    "\n",
    "    def spacy_generate_features(self, doc, operations='tokenize,ssplit,pos,lemma,ner'):\n",
    "        \"\"\"\n",
    "        Spacy nlp pipeline to generate features such as pos, tokens, ner, dependency. Accepts doc=nlp(text)\n",
    "        \"\"\"\n",
    "        # spacy doc\n",
    "        doc_json = doc.to_json()  # Includes all operations given by spacy pipeline\n",
    "\n",
    "        # Get text\n",
    "        text = doc_json['text']\n",
    "\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        # 1. Extract Entity List\n",
    "        entity_list = doc_json[\"ents\"]\n",
    "\n",
    "        # 2. Create token lib\n",
    "        token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "        # init output json\n",
    "        output_json = {}\n",
    "        output_json[\"sentences\"] = []\n",
    "\n",
    "        # Perform spacy operations on each sent in text\n",
    "        for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "            # init parsers\n",
    "            parse = \"\"\n",
    "            basicDependencies = []\n",
    "            enhancedDependencies = []\n",
    "            enhancedPlusPlusDependencies = []\n",
    "\n",
    "            # init output json\n",
    "            out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "            output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "            # 3. Split sentences by indices(i), add labels (pos, ner, dep, etc.)\n",
    "            for token in doc_json[\"tokens\"]:\n",
    "\n",
    "                if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                    \n",
    "                    # >>> Extract Entity label\n",
    "                    ner = \"O\"\n",
    "                    for entity in entity_list:\n",
    "                        if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                            ner = entity[\"label\"]\n",
    "\n",
    "                    # >>> Extract dependency info\n",
    "                    dep = token[\"dep\"]\n",
    "                    governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                    governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]:\n",
    "                                                                                     token_lib[token[\"head\"]][\"end\"]]\n",
    "                    dependent = token[\"id\"] + 1\n",
    "                    dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "\n",
    "                    # >>> Extract lemma\n",
    "                    lemma = doc[token[\"id\"]].lemma_\n",
    "\n",
    "                    # 4. Add dependencies\n",
    "                    basicDependencies.append({\"dep\": dep,\n",
    "                                              \"governor\": governor,\n",
    "                                              \"governorGloss\": governorGloss,\n",
    "                                              \"dependent\": dependent,\n",
    "                                              \"dependentGloss\": dependentGloss})\n",
    "                    # 5. Add tokens\n",
    "                    out_token = {\"index\": token[\"id\"] + 1,\n",
    "                                 \"word\": dependentGloss,\n",
    "                                 \"originalText\": dependentGloss,\n",
    "                                 \"characterOffsetBegin\": token[\"start\"],\n",
    "                                 \"characterOffsetEnd\": token[\"end\"]}\n",
    "\n",
    "                    # 6. Add lemmas\n",
    "                    if \"lemma\" in operations:\n",
    "                        out_token[\"lemma\"] = lemma\n",
    "\n",
    "                    # 7. Add POS tagging\n",
    "                    if \"pos\" in operations:\n",
    "                        out_token[\"pos\"] = token[\"tag\"]\n",
    "\n",
    "                    # 8. Add NER\n",
    "                    if \"ner\" in operations:\n",
    "                        out_token[\"ner\"] = ner\n",
    "\n",
    "                    # Update output json\n",
    "                    out_sentence[\"tokens\"].append(out_token)\n",
    "\n",
    "            # 9. Add dependencies operation\n",
    "            if \"parse\" in operations:\n",
    "                out_sentence[\"parse\"] = parse\n",
    "                out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "                out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "                out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        return output_json\n",
    "    \n",
    "    def spacy_clean(self, input_sentences):\n",
    "        batch_size = min(int(np.ceil(len(input_sentences)/100)), 500)\n",
    "        \n",
    "        # Part 1: generate spacy textual features (pos, ner, lemma, dependencies)\n",
    "        sentences = [self.spacy_generate_features(doc) for doc in nlp.pipe(input_sentences, batch_size=batch_size, n_process=-1)]\n",
    "        \n",
    "        # Part 2: collect all the features for each sentence\n",
    "        spacy_sentences = [self.spacy_get_pos_list(sent) for sent in sentences]\n",
    "\n",
    "        return spacy_sentences\n",
    "\n",
    "\n",
    "    ## MAIN ##\n",
    "    def run_pipeline(self, sentences, operation):\n",
    "        \"\"\"\n",
    "        Main module to execute pipeline. Accepts list of strings, and desired operation.\n",
    "        \"\"\"\n",
    "        if operation==\"\":\n",
    "            raise Exception(\"Please pass a cleaning type - `basic`, `deep` or `spacy` !!\")\n",
    "\n",
    "        # run basic cleaning\n",
    "        if \"basic\" == operation.lower(): \n",
    "            return self.basic_clean(sentences)\n",
    "\n",
    "        # run deep cleaning\n",
    "        if \"deep\" == operation.lower(): \n",
    "            return self.deep_clean(sentences)\n",
    "\n",
    "        # run spacy pipeline\n",
    "        if \"spacy\" == operation.lower(): \n",
    "            return self.spacy_clean(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings ##\n",
    "\n",
    "\"\"\"\n",
    "CUSTOM VOCABULARY ::\n",
    "\n",
    "- List of words you wish to mark and retain them across the preprocessing steps - very important!\n",
    "- Example, task-specific, domain-specific keywords.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_vocab = [\"who\", \"what\", \"where\", \"when\", \"would\", \"which\", \"how\", \"why\", \"can\", \"may\", \n",
    "                \"will\", \"won't\", \"does\", \"does not\",\"doesn't\", \"do\", \"do i\", \"do you\", \"is it\", \"would you\", \n",
    "                \"is there\", \"are there\", \"is it so\", \"is this true\", \"to know\", \"is that true\", \"are we\", \n",
    "                \"am i\", \"question is\", \"can i\", \"can we\", \"tell me\", \"can you explain\", \"how ain't\", \n",
    "                \"question\", \"answer\", \"questions\", \"answers\", \"ask\", \"can you tell\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Utilities:\n",
    "- Truncate words to their root-known-word form, stripping off their adjectives, verbs, etc. (Example: \"running\" becomes \"run\", \"is\" becomes \"be\")\n",
    "- different from stemmer (PorterStemmer)\n",
    "- Can use regex based stemming..\n",
    "- Check Spacy's dependency parsing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "do_lemmatizing = True\n",
    "#do_chinking = False\n",
    "#do_chunking = False\n",
    "#do_dependencyParser = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing ##\n",
    "\n",
    "preprocessText_obj = preprocessText(resources_dir_path, custom_vocab, do_lemmatizing)\n",
    "\n",
    "def cleaning(data, text_col):\n",
    "    data[\"Basic_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"basic\")\n",
    "    data[\"Deep_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"deep\")\n",
    "    data[\"Spacy_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"spacy\")\n",
    "    return data\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# df = cleaning(df, <_TEXT_COLUMN_>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Basic_TEXT</th>\n",
       "      <th>Deep_TEXT</th>\n",
       "      <th>Spacy_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello i am good</td>\n",
       "      <td>hello i am good</td>\n",
       "      <td>hello good</td>\n",
       "      <td>{'word_list': ['hello', 'i', 'am', 'good'], 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello bye!</td>\n",
       "      <td>hello bye!</td>\n",
       "      <td>hello bye</td>\n",
       "      <td>{'word_list': ['hello', 'bye', '!'], 'lemma_li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TEXT       Basic_TEXT   Deep_TEXT  \\\n",
       "0  hello i am good  hello i am good  hello good   \n",
       "1       hello bye!       hello bye!   hello bye   \n",
       "\n",
       "                                          Spacy_TEXT  \n",
       "0  {'word_list': ['hello', 'i', 'am', 'good'], 'l...  \n",
       "1  {'word_list': ['hello', 'bye', '!'], 'lemma_li...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Execute ##\n",
    "\n",
    "df = pd.DataFrame({\"TEXT\": ['hello i am good', \"hello bye!\"]})\n",
    "cleaning(df, \"TEXT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Vectorization Unit (Unit 2/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     2,
     11,
     33,
     38
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer, model, max_length=128, embedding_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Mean Pooling - Take attention mask into account for correct averaging\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        encoded_input = tokenizer(text, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "        return self._tokenize(text)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting required so we just return ourselves. For fine-tuning, refer to shared gpu-code!\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE-BERT VECTORIZATION\n",
    "\n",
    "# load tokenizer, model classes\n",
    "tokenizer = AutoTokenizer.from_pretrained(sbert_model_fp)\n",
    "model_bert = AutoModel.from_pretrained(sbert_model_fp)\n",
    "\n",
    "# load vectorizer\n",
    "bert_vectorizer = BertTransformer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n",
    "print(\"Bert Model '%s' loaded.\" % ntpath.basename(sbert_model_fp))\n",
    "\n",
    "## SAMPLE FOR BERT CLASS VECTORIZATION\n",
    "# corpus = df['text_col']\n",
    "# bert_matrix = F.normalize(bert_vectorizer.transform(corpus), p=2, dim=1)\n",
    "# bert_matrix = csr_matrix(bert_matrix.numpy().astype(np.float64))\n",
    "# matches = awesome_cossim_top(A = bert_matrix,\n",
    "#                              B = bert_matrix.transpose(),\n",
    "#                              ntop = mat.shape[0],\n",
    "#                              min_similarity = 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pair-wise Fast-Similairty Matrix Creation Unit (Unit 3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Generate_Similarity_Matrix(object):\n",
    "\n",
    "    def __init__(self, master, duplicates=None, master_id=None, duplicates_id=None, min_similarity=0.80, vectorizer='tfidf'):\n",
    "        # UTILITY FUNCTIONS\n",
    "        def _is_series_of_strings(series_to_test: pd.Series):\n",
    "            if not isinstance(series_to_test, pd.Series): return False\n",
    "            elif series_to_test.to_frame().applymap(lambda x: not isinstance(x, str)).squeeze(axis=1).any(): return False\n",
    "            return True\n",
    "        def _is_input_data_combination_valid(duplicates, master_id, duplicates_id):\n",
    "            if duplicates is None and (duplicates_id is not None) or duplicates is not None and ((master_id is None) ^ (duplicates_id is None)): return False\n",
    "            else: return True\n",
    "\n",
    "        # VALIDATE INPUT ARGS\n",
    "        if not _is_series_of_strings(master) or (duplicates is not None and not _is_series_of_strings(duplicates)):\n",
    "            raise TypeError('Input does not consist of pandas.Series containing only Strings')\n",
    "        if not _is_input_data_combination_valid(duplicates, master_id, duplicates_id):\n",
    "            raise Exception('List of data Series options is invalid')\n",
    "        if master_id is not None and len(master) != len(master_id):\n",
    "            raise Exception('Both master and master_id must be pandas.Series of the same length.')\n",
    "        if duplicates is not None and duplicates_id is not None and len(duplicates) != len(duplicates_id):\n",
    "            raise Exception('Both duplicates and duplicates_id must be pandas.Series of the same length.')\n",
    "\n",
    "        # SAVE INPUT ARGS\n",
    "        self._master = master\n",
    "        self._duplicates = duplicates if duplicates is not None else None\n",
    "        self._master_id = master_id if master_id is not None else None\n",
    "        self._duplicates_id = duplicates_id if duplicates_id is not None else None\n",
    "        self.min_similarity = min_similarity\n",
    "        self.vectorizer_name = vectorizer     # tfidf, bert\n",
    "\n",
    "        # CONFIG\n",
    "        self._true_max_n_matches = None\n",
    "        self._max_n_matches = len(self._master) if self._duplicates is None else len(self._duplicates)\n",
    "        self.ngram_size = 3\n",
    "        self.regex = r'[,-./]|\\s'\n",
    "        self.number_of_processes = multiprocessing.cpu_count() - 1\n",
    "        self.DEFAULT_COLUMN_NAME = 'side'\n",
    "        self.DEFAULT_ID_NAME = 'id'\n",
    "        self.LEFT_PREFIX = 'left_'\n",
    "        self.RIGHT_PREFIX = 'right_'\n",
    "        self._matches_list = pd.DataFrame()\n",
    "        self.is_build = False  # indicates if fit has been called or not\n",
    "\n",
    "        # -- INIT VECTORIZER --\n",
    "        if self.vectorizer_name==\"tfidf\":\n",
    "            def get_n_grams(string):\n",
    "                if string is not None: string = string.lower()    # lowercasing all str\n",
    "                string = re.sub(self.regex, r'', string)\n",
    "                n_grams = zip(*[string[i:] for i in range(self.ngram_size)])\n",
    "                return [''.join(n_gram) for n_gram in n_grams]\n",
    "            # - enable fit() in \"_get_tf_idf_matrices(self)\"\n",
    "            self._vectorizer = TfidfVectorizer(min_df=1, analyzer=get_n_grams, dtype=np.float64)\n",
    "        if self.vectorizer_name==\"bert\":\n",
    "            self._vectorizer = BertTransformer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n",
    "        # -- INIT VECTORIZER --\n",
    "        return\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fit a vectorizer (already init) with Master & Duplicates matrix and calculate cosine-sim without original-ids.\n",
    "        Params  : Master, Duplicates\n",
    "        Return  : dataframe{ Master_Text, Duplicates_Text, cosine_sim(vectorizer_master, vectorized_duplicates) }\n",
    "\n",
    "        \"\"\"\n",
    "        # UTILITY FUNCTIONS\n",
    "        def fix_diagonal(m: lil_matrix):\n",
    "            r = np.arange(m.shape[0])\n",
    "            m[r, r] = 1\n",
    "            return m\n",
    "        def symmetrize_matrix(m_symmetric: lil_matrix):\n",
    "            r, c = m_symmetric.nonzero()\n",
    "            m_symmetric[c, r] = m_symmetric[r, c]\n",
    "            return m_symmetric\n",
    "\n",
    "        # Vectorize the matrices\n",
    "        # - if duplicate matrix is present use it, else utilize master itself\n",
    "        master_matrix, duplicate_matrix = self.get_vectorized_matrices()\n",
    "\n",
    "        # Calculate cosine similarity b/w master & duplicates (if passed, else use master itself)\n",
    "        matches = self.build_matches(master_matrix, duplicate_matrix)\n",
    "        self._true_max_n_matches = self._max_n_matches-1\n",
    "\n",
    "        # Correct sparse matrix multiplcation\n",
    "        if self._duplicates is None:\n",
    "            # convert to lil format for best efficiency when setting matrix-elements\n",
    "            # matrix diagonal elements must be exactly 1 (numerical precision errors introduced by floating-point computations\n",
    "            #                                             in awesome_cossim_topn sometimes lead to unexpected results)\n",
    "            matches = matches.tolil()\n",
    "            matches = fix_diagonal(matches)\n",
    "            if self._max_n_matches < self._true_max_n_matches:\n",
    "                matches = symmetrize_matrix(matches)\n",
    "            matches = matches.tocsr()\n",
    "\n",
    "        # Create the basic \"matches\" dataframe with \"Master, Duplicate and Similarity\" cols only\n",
    "        r, c = matches.nonzero()\n",
    "        self._matches_list = pd.DataFrame({'master_side': r.astype(np.int64), 'dupe_side': c.astype(np.int64), 'similarity': matches.data})\n",
    "        self.is_build = True\n",
    "        return self\n",
    "\n",
    "    def get_vectorized_matrices(self):\n",
    "        \"\"\"\n",
    "        Vectorize matrices using one of the vectorizers.\n",
    "        Params    : Master, Duplicates, Vectorizer_name(\"tfidf\", \"bert\")\n",
    "        Return    : vectorizer_master, vectorized_duplicates\n",
    "        \"\"\"\n",
    "        def fit_vectorizer():\n",
    "            # if both master & duplicates series are set - concat them to fit the vectorizer on all strings at once!\n",
    "            if self._duplicates is not None:\n",
    "                strings = pd.concat([self._master, self._duplicates])\n",
    "            else:\n",
    "                strings = self._master\n",
    "            self._vectorizer.fit(strings)\n",
    "            return self._vectorizer\n",
    "\n",
    "        if self.vectorizer_name==\"tfidf\":\n",
    "            print(\"tfidf vectorization\")\n",
    "            self._vectorizer = fit_vectorizer()\n",
    "            master_matrix = self._vectorizer.transform(self._master)\n",
    "            if self._duplicates is not None:\n",
    "                duplicate_matrix = self._vectorizer.transform(self._duplicates)\n",
    "            else:\n",
    "                # IF there is no duplicate matrix, match on the master matrix itself!\n",
    "                duplicate_matrix = master_matrix\n",
    "\n",
    "        if self.vectorizer_name==\"bert\":\n",
    "            print(\"bert vectorization\")\n",
    "            master_matrix = self._vectorizer.transform(self._master)\n",
    "            # --> Convert Tensor Matrices to CSR (np.float64)\n",
    "            master_matrix = csr_matrix( F.normalize(master_matrix).numpy().astype(np.float64) )\n",
    "            if self._duplicates is not None:\n",
    "                duplicate_matrix = self._vectorizer.transform(self._duplicates)\n",
    "                duplicate_matrix = csr_matrix( F.normalize(duplicate_matrix).numpy().astype(np.float64) )\n",
    "            else:\n",
    "                # IF there is no duplicate matrix, match on the master matrix itself!\n",
    "                duplicate_matrix = master_matrix\n",
    "\n",
    "        return master_matrix, duplicate_matrix\n",
    "\n",
    "\n",
    "    def build_matches(self, master_matrix, duplicate_matrix):\n",
    "        \"\"\"\n",
    "        Builds the cosine similarity matrix of two CSR matrices.\n",
    "        Params   : vectorizer_master, vectorized_duplicates\n",
    "        Return   : cosine_sim(vectorized_master, vectorized_duplicates)\n",
    "        \"\"\"\n",
    "        # Matrix A, B\n",
    "        tf_idf_matrix_1 = master_matrix\n",
    "        tf_idf_matrix_2 = duplicate_matrix.transpose()\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        optional_kwargs = {'use_threads': self.number_of_processes > 1, 'n_jobs': self.number_of_processes}\n",
    "        cosine_sim_matrix = awesome_cossim_topn(tf_idf_matrix_1,\n",
    "                                                tf_idf_matrix_2,\n",
    "                                                self._max_n_matches,\n",
    "                                                self.min_similarity,\n",
    "                                                **optional_kwargs)\n",
    "        return cosine_sim_matrix\n",
    "\n",
    "\n",
    "    def get_matches(self):\n",
    "        \"\"\"\n",
    "        Creates the complete dataframe with index matching(ids) if passed.\n",
    "        Params  : dataframe\n",
    "        Return  : dataframe{ Master_ids, Master_Text, cosine_similarity, Duplicate_ids, Duplicates_Text }\n",
    "        \"\"\"\n",
    "        # UTILITY FUNCTIONS\n",
    "        def get_both_sides(master, duplicates, generic_name=(self.DEFAULT_COLUMN_NAME, self.DEFAULT_COLUMN_NAME), drop_index=False):\n",
    "            lname, rname = generic_name\n",
    "            left = master if master.name else master.rename(lname)\n",
    "            left = left.iloc[matches_list.master_side].reset_index(drop=drop_index)\n",
    "            if self._duplicates is None:\n",
    "                right = master if master.name else master.rename(rname)\n",
    "            else:\n",
    "                right = duplicates if duplicates.name else duplicates.rename(rname)\n",
    "            right = right.iloc[matches_list.dupe_side].reset_index(drop=drop_index)\n",
    "            return left, (right if isinstance(right, pd.Series) else right[right.columns[::-1]])\n",
    "        def prefix_column_names(data, prefix):\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                return data.rename(columns={c: f\"{prefix}{c}\" for c in data.columns})\n",
    "            else:\n",
    "                return data.rename(f\"{prefix}{data.name}\")\n",
    "\n",
    "        if self.min_similarity > 0:\n",
    "            matches_list = self._matches_list\n",
    "        else:\n",
    "            raise Exception(\"min_similarity cannot be set to less than or equal to 0!\")\n",
    "\n",
    "        # ID Retrival\n",
    "        left_side, right_side = get_both_sides(self._master, self._duplicates, drop_index=False)\n",
    "        similarity = matches_list.similarity.reset_index(drop=True)\n",
    "        if self._master_id is None:\n",
    "            # if ids are not passed\n",
    "            return pd.concat([prefix_column_names(left_side, self.LEFT_PREFIX),\n",
    "                              similarity,\n",
    "                              prefix_column_names(right_side, self.RIGHT_PREFIX)], axis=1)\n",
    "\n",
    "        else:\n",
    "            # if ids are passed, retrive ids\n",
    "            left_side_id, right_side_id = get_both_sides(self._master_id, self._duplicates_id, (self.DEFAULT_ID_NAME, self.DEFAULT_ID_NAME), drop_index=True)\n",
    "            return pd.concat([prefix_column_names(left_side, self.LEFT_PREFIX),\n",
    "                              prefix_column_names(left_side_id, self.LEFT_PREFIX),\n",
    "                              similarity,\n",
    "                              prefix_column_names(right_side_id, self.RIGHT_PREFIX),\n",
    "                              prefix_column_names(right_side, self.RIGHT_PREFIX)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Similarity Computation Engine\n",
    "\n",
    "def match_strings(master, duplicates=None, master_id=None, duplicates_id=None, min_similarity=0.80, vectorizer=None):\n",
    "    \"\"\" \n",
    "    Find pair-wise similarity b/w 'Master' & 'Duplicate' Matrices. \n",
    "    \"\"\"\n",
    "    if vectorizer and vectorizer.lower().strip() in ['tfidf', 'bert']:\n",
    "        vectorizer=vectorizer.lower().strip()\n",
    "        cos_sim_matrix = Generate_Similarity_Matrix(master, duplicates=duplicates, master_id=master_id, duplicates_id=duplicates_id, min_similarity=min_similarity, vectorizer=vectorizer)\n",
    "        cos_sim_matrix.fit()                     # run vectorizer & generate basic pair-wise cosine sim matrix\n",
    "        sim_df = cos_sim_matrix.get_matches()    # add ids if passed to sim matrix\n",
    "        return sim_df\n",
    "    else:\n",
    "        raise Exception(\"Vectorizer is not passed or incorrect! Please select one: [tfidf, bert]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERIC RUN ::\n",
    "\n",
    "\n",
    "# Textual semantic Similarity between all strings of file A:\n",
    "# matches = match_strings(master = df[<_text_col_>], master_id = df[<_id_col_>], \n",
    "#                         min_similarity=0.60, vectorizer='bert')\n",
    "\n",
    "## Textual semantic similarity between two files A and B:\n",
    "# matches = match_strings(master = df[__text_col_A__], master_id = df[__id_col_A__], \n",
    "#                         duplicates = df[__text_col_B__], duplicates_id = df[__id_col_B__], \n",
    "#                         min_similarity=0.85, vectorizer='bert')\n",
    "\n",
    "## Textual semantic similarity between two files A and B having no ids:\n",
    "# matches = match_strings(master = df[__text_col_A__], duplicates = df[__text_col_B__],\n",
    "#                         min_similarity=0.85, vectorizer='bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Similarity Computation Unit (Unit 4/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**STEPS**\n",
    "\n",
    "- Option 1: Run similarity analysis on 1 source file to get duplicates and very-similar records.\n",
    "- Option 2: Run similarity analysis between 2 files: \"source\" & \"target\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OPTION 1. Compute Similarity on 1 file (i.e. source)\n",
    "\n",
    "- Runs similarity computation on one \"source\" file to get a list of duplicate ids (dup_idx), similar ids (similar_idx) and merged duplicate-similar-ids (dup_similar_idx), and a cluster ids (cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     3,
     13,
     17,
     36,
     41,
     57,
     100,
     105
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_source_file(df, source_min_similarity=0.75):\n",
    "\n",
    "    # 1. collect duplicate ids based on \"text\" col\n",
    "    def collect_dups(data, id_col, dup_col, output_col_name):\n",
    "        dup_dict = data.reset_index()\\\n",
    "                    .groupby(data[dup_col].tolist())[id_col]\\\n",
    "                    .agg(list).reset_index().reset_index(drop=True)\\\n",
    "                    .rename(columns={\"index\": dup_col, id_col: output_col_name})\n",
    "        dup_dict = dup_dict.set_index(dup_col)[output_col_name].to_dict()\n",
    "        data[output_col_name] = data[dup_col].apply(lambda txt: dup_dict[txt])\n",
    "        return data\n",
    "\n",
    "    # 2. drop dup ids, keep first\n",
    "    def drop_dups(data, col):\n",
    "        return data.drop_duplicates(subset=[col]).reset_index(drop=True)\n",
    "\n",
    "    # 3. collect similar pairs\n",
    "    def pairwise_similarity_matrix(data, id_col, text_col, similar_id_col, min_similarity=0.75):\n",
    "        # TEXTUAL SIMILARITY\n",
    "\n",
    "        # MODULE 1 :: pair-wise textual similarity\n",
    "        matches = match_strings(master=data[text_col], master_id=data[id_col], min_similarity=min_similarity, vectorizer='bert')\n",
    "\n",
    "        # group similar-pairs together (left-join)\n",
    "        left_col_name, left_unique_id, right_unique_id = \"left_%s\" % text_col, \"left_%s\" % id_col, \"right_%s\" % id_col\n",
    "        match_df = matches.groupby([left_col_name, left_unique_id])[right_unique_id]\\\n",
    "                          .agg({similar_id_col: lambda x: sorted(set(x))})\\\n",
    "                          .reset_index().sort_values(by=[left_unique_id], ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # asthestic: drop dummy added left/right names\n",
    "        matches = matches.drop(columns=['left_index', \"right_index\"])\n",
    "        match_df = match_df.rename(columns={left_unique_id: id_col, left_col_name: text_col})\n",
    "        return matches, match_df\n",
    "\n",
    "    ## Utility: alphanumeric sort\n",
    "    _nsre = re.compile('([0-9]+)')\n",
    "    def natural_sort_key(s):\n",
    "        return [int(text) if text.isdigit() else text.lower()\n",
    "                for text in re.split(_nsre, s)]\n",
    "\n",
    "    # 4. create \"dup_similar_idx\" col - merge dup_id data with similar_id data\n",
    "    def combine_dup_similar(data, match_df, id_col, dup_id_col, similar_id_col, dup_sim_id_col):\n",
    "\n",
    "        # merge df_duplicates with df_similar\n",
    "        cols_to_use = [id_col, similar_id_col]\n",
    "        data = data.merge(match_df[cols_to_use], on=id_col, how='outer')\n",
    "\n",
    "        # create combined list: dup_ids + similar_ids\n",
    "        # --> \"dup_similar_id_col\" == \"duplicated_pairs_idx\" + \"similar_pairs_idx\n",
    "        data[dup_sim_id_col] = [sorted(set(sum(tup, []))) for tup in zip(data[dup_id_col], data[similar_id_col])]\n",
    "\n",
    "        # custom sorting (to handle alphanumeric ids)\n",
    "        if isinstance(data[dup_sim_id_col][0], str):\n",
    "            data[dup_sim_id_col] = data[dup_sim_id_col].apply(lambda x: sorted(x, key=natural_sort_key))\n",
    "        return data\n",
    "\n",
    "    # 5. merged all nested lists containing common sub-elements in \"dup_similar_id\" cols\n",
    "    def collect_similar_ids(data, id_col, dup_id_col, similar_id_col, dup_sim_id_col):\n",
    "\n",
    "        # collect nested list which needs to be merged\n",
    "        list_similar_ids = list(map(list, data[dup_sim_id_col]))\n",
    "\n",
    "        # merge all nested lists with common elements\n",
    "        g = nx.Graph()\n",
    "        edges = [g.add_edges_from(zip(p, p[1:])) if len(p)>1 else g.add_edges_from(zip(p, p[:])) for p in list_similar_ids]\n",
    "        merged_similar_idx = [sorted(c) for c in nx.connected_components(g)]\n",
    "\n",
    "        # create two mappings, one for storing cluster_id: list of ids, and one inverted dict\n",
    "        # --> \"id_clus_dict\" is the cluster id mapping for each 'unique_id'\n",
    "        temp_id = 1\n",
    "        clus_id_dict = {}  # cluster_1: merged([id1, id2,..., idn])\n",
    "        id_clus_dict = {}  # merged(id1): cluster_1; merged(id1): cluster_1; .., merged(idn): cluster_1\n",
    "        for lst in merged_similar_idx:\n",
    "            key = \"cluster_%s\"%temp_id\n",
    "            for value in lst:\n",
    "                id_clus_dict[value] = key\n",
    "            clus_id_dict[key]=lst\n",
    "            temp_id+=1\n",
    "\n",
    "        # assign dup_similar_idx based on two mappings above\n",
    "        df[dup_sim_id_col] = df[id_col].apply(lambda uid: clus_id_dict[id_clus_dict[uid]])\n",
    "\n",
    "        # create duplicate id mapping and similar id mapping files\n",
    "        dup_id_dict = {_id: ids for ids in df[dup_id_col].tolist() for _id in ids}\n",
    "        sim_id_dict = {_id: ids for ids in df[similar_id_col].tolist() for _id in ids}\n",
    "        dup_sim_id_dict = {_id: ids for ids in df[dup_sim_id_col].tolist() for _id in ids}\n",
    "\n",
    "        # custom sorting (to handle alphanumeric ids)\n",
    "        if isinstance(data[dup_sim_id_col][0], str):\n",
    "            df[dup_sim_id_col] = df[dup_sim_id_col].apply(lambda x: sorted(x, key=natural_sort_key))\n",
    "        return data, clus_id_dict, id_clus_dict, dup_id_dict, sim_id_dict, dup_sim_id_dict\n",
    "\n",
    "    # 6. Drop duplicates based on dup_similar_id_col, i.e. duplicated_id + similar_ids\n",
    "    def create_final_single_matrix(data, dup_sim_id_col):\n",
    "        data[dup_sim_id_col] = tuple(map(tuple, data[dup_sim_id_col]))\n",
    "        data = data.drop_duplicates(subset=[dup_sim_id_col]).reset_index(drop=True)\n",
    "        data[dup_sim_id_col] = list(map(list, data[dup_sim_id_col]))\n",
    "        return data\n",
    "\n",
    "    # 7. Expand each id to assign clusters\n",
    "    def create_clusters(data, id_col, idx_cluster_map):\n",
    "        data['cluster_id'] = data[id_col].apply(lambda uid: idx_cluster_map.get(uid, -1))\n",
    "        return data\n",
    "\n",
    "    # 8. Display Analytics\n",
    "    def run_stats():\n",
    "        print(\"Stats:\\n\"\n",
    "          \"\\nOrignal number of records = {}\"\n",
    "          \"\\nTotal dup count = {}\"\n",
    "          \"\\nTotal similar pairs found = {}\"\n",
    "          \"\\nFinal number of records post dup-similar rows removal = {}\".format(len(original_df),\n",
    "                                                                              len(sum(df[source_dup_id_col].tolist(), [])),\n",
    "                                                                              len(sum(df[source_similar_id_col].tolist(), [])),\n",
    "                                                                              len(df)))\n",
    "    \n",
    "    \"\"\"\n",
    "    Run similarity computation on pre-processed Master file involving Dup identification, collection & removal, finally running\n",
    "    similarity analysis on remaining unique rows.\n",
    "    param    : dataframe (single file containing - 'unique_id', 'cleaned_text')\n",
    "    return   : final dataframe with only unique rows, original dataframe with results (cluster info)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## EXECUTE ##\n",
    "    \n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise Exception(\"Please pass a dataframe object\")\n",
    "    if source_id_col not in df.columns or source_text_col not in df.columns:\n",
    "        raise Exception(\"Input dataframe should contain '%s' and '%s' fields, as set in config\" % source_id_col, source_text_col)\n",
    "    if source_clean_text_col not in df:\n",
    "        raise Exception(\"Source dataframe must be pre-processed! Perform cleaning on Source df's '%s'!\" % source_clean_text_col)\n",
    "    \n",
    "    original_df = df.copy()\n",
    "    df = collect_dups(df, source_id_col, source_clean_text_col, source_dup_id_col)\n",
    "    df = drop_dups(df, source_clean_text_col)\n",
    "    matches, match_df = pairwise_similarity_matrix(df, source_id_col, source_clean_text_col, source_similar_id_col, min_similarity=source_min_similarity)\n",
    "    df = combine_dup_similar(df, match_df, source_id_col, source_dup_id_col, source_similar_id_col, source_dup_similar_id_col)\n",
    "    df, cluster_id_map, idx_cluster_map, dup_id_dict, sim_id_dict, dup_sim_id_dict = collect_similar_ids(df,  source_id_col, source_dup_id_col, source_similar_id_col, source_dup_similar_id_col)\n",
    "    df = create_final_single_matrix(df, source_dup_similar_id_col)\n",
    "    df['cluster_id'] = df[source_id_col].apply(lambda uid: idx_cluster_map.get(uid, -1))\n",
    "    # save back in original df (without dups or similar dropped!)\n",
    "    original_df['dup_idx'] = original_df[source_id_col].apply(lambda uid: dup_id_dict.get(uid, -1))\n",
    "    original_df['similar_idx'] = original_df[source_id_col].apply(lambda uid: sim_id_dict.get(uid, -1))\n",
    "    original_df['dup_similar_idx'] = original_df[source_id_col].apply(lambda uid: dup_sim_id_dict.get(uid, -1))\n",
    "    original_df['cluster_id'] = original_df[source_id_col].apply(lambda uid: idx_cluster_map.get(uid, -1))\n",
    "    run_stats()\n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## SAMPLE EXECUTION\n",
    "## columns that should be present already in the file\n",
    "# source_id_col = 'ID'                 \n",
    "# source_text_col = 'TEXT'\n",
    "## columns that will be created during runtime\n",
    "# source_clean_text_col = 'C_TEXT'               \n",
    "# source_dup_id_col = \"dup_idx\"                       \n",
    "# source_similar_id_col = \"similar_idx\"           \n",
    "# source_dup_similar_id_col = \"dup_similar_idx\"  \n",
    "# df = cleaning(df, source_text_col, source_clean_text_col)\n",
    "# df = compute_similarity_source_file(df, source_min_similarity=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "### OPTION 2. Compute Similarity between 2 files\n",
    "\n",
    "- Runs similarity computation on two files: \"Source\" & \"Target\" to get a list of duplicate ids (dup_idx), similar ids (similar_idx) and merged duplicate-similar-ids (dup_similar_idx), and a cluster ids (cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_source_target_files(source_df, target_df, vectorization, min_similarity=0.80):\n",
    "    \"\"\"\n",
    "    Run similarity computation between 2 \"pre-processed\" files: Master and child(duplicate) file.\n",
    "    param    : dataframe_1('unique_id', 'cleaned_text'), dataframe_2('unique_id', 'cleaned_text')\n",
    "    return   : pair-wise comparison df, final df\n",
    "    \"\"\"\n",
    "    \n",
    "    # VALIDATION\n",
    "    if not isinstance(source_df, pd.DataFrame) or not isinstance(target_df, pd.DataFrame):\n",
    "        raise Exception(\"Please pass a dataframe object!\")\n",
    "    if source_id_col not in source_df.columns or source_text_col not in source_df.columns:\n",
    "        raise Exception(\"Source dataframe should contain '%s' and '%s' fields, as set in config\" % source_id_col, source_text_col)\n",
    "    if target_id_col not in target_df.columns or target_text_col not in target_df.columns:\n",
    "        raise Exception(\"Target dataframe should contain '%s' and '%s' fields, as set in config\" % target_id_col, target_text_col)\n",
    "    if source_clean_text_col not in source_df or target_clean_text_col not in target_df:\n",
    "        raise Exception(\"Dataframes are not pre-processed! Perform cleaning on both Source and Target dfs!\")\n",
    "    if vectorization not in ['tfidf', 'bert']:\n",
    "        raise Exception(\"Vectorization error - please use any one: ['tfidf', 'bert']\")\n",
    "\n",
    "    # => SOURCE/MASTER DF\n",
    "    # - drop dups + similar, keep only unique records to compare with\n",
    "    original_source_df = source_df.copy()\n",
    "    source_df[source_dup_similar_id_col] = tuple(map(tuple, source_df[source_dup_similar_id_col]))\n",
    "    source_df = source_df.drop_duplicates(subset=[source_dup_similar_id_col]).reset_index(drop=True)\n",
    "    master_ids, master_sents = source_df[source_id_col], source_df[source_clean_text_col]\n",
    "\n",
    "    # => TARGET/COMPARISON/DUPLICATES DF (target_df or duplicates_df)\n",
    "    child_ids, child_sents = target_df[target_id_col], target_df[target_clean_text_col]\n",
    "\n",
    "    # Pair-wise Fast Similairty Matrix Creation\n",
    "    matches = match_strings(master=master_sents, master_id=master_ids, duplicates=child_sents, duplicates_id=child_ids, min_similarity=min_similarity, vectorizer=vectorization)\n",
    "    matches = matches.drop(columns=['left_index', 'right_index'])\n",
    "\n",
    "    # STORE RESULTS\n",
    "    output=dict()\n",
    "    for uid in source_df[source_id_col].values:\n",
    "        # subset from matches_df\n",
    "        similar = matches[matches['left_%s' % source_id_col] == uid].sort_values(by=['similarity','right_%s' % target_id_col], ascending=[False, True])\n",
    "        # storing in source_df\n",
    "        temp_dict = dict()\n",
    "        temp_dict[\"most_similar\"], temp_dict[\"max_sim_score\"], temp_dict[\"pairs\"], temp_dict[\"count\"] = None, 0.0, None, 0\n",
    "        if len(similar)>0:\n",
    "            most_similar_id = similar['right_%s' % target_id_col].values[0]\n",
    "            temp_dict[\"most_similar\"] = target_df[target_df[target_id_col]==most_similar_id][target_text_col].values[0]\n",
    "            temp_dict[\"max_sim_score\"] = similar['similarity'].values[0]\n",
    "            temp_dict[\"pairs\"] = similar['right_%s' % target_id_col].tolist()\n",
    "            temp_dict[\"count\"] = int(len(similar['right_%s' % target_id_col].tolist()))\n",
    "        output[uid] = temp_dict\n",
    "    source_df[\"most_similar_FAQ\"] = source_df[source_id_col].apply(lambda x: output[x]['most_similar'])\n",
    "    source_df[\"max_sim_score_FAQ\"] = source_df[source_id_col].apply(lambda x: output[x]['max_sim_score'])\n",
    "    source_df[\"similar_IDs\"] = source_df[source_id_col].apply(lambda x: output[x]['pairs'])\n",
    "    source_df[\"sim_FAQ_count\"] = source_df[source_id_col].apply(lambda x: output[x]['count'])\n",
    "\n",
    "    # save back in original df (without dups or similar dropped!)\n",
    "    original_source_df = original_source_df.merge(source_df[['cluster_id', 'most_similar_FAQ', 'max_sim_score_FAQ', 'similar_IDs', 'sim_FAQ_count']], on=['cluster_id']).sort_values(by=[source_id_col]).reset_index(drop=True)\n",
    "\n",
    "    # pair-wise comparison results, original df with all columns\n",
    "    return original_source_df, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## SAMPLE EXECUTION\n",
    "# source_id_col = 'ID'                 \n",
    "# source_text_col = 'TEXT'\n",
    "# source_clean_text_col = 'C_TEXT'               \n",
    "# source_dup_id_col = \"dup_idx\"                       \n",
    "# source_similar_id_col = \"similar_idx\"           \n",
    "# source_dup_similar_id_col = \"dup_similar_idx\"  \n",
    "# source_df = cleaning(source_df, source_text_col, source_clean_text_col)\n",
    "# target_id_col = 'ID'\n",
    "# target_text_col = 'FAQ'\n",
    "# target_clean_text_col = 'clean_FAQ'\n",
    "# target_df = cleaning(target_df, target_text_col, target_clean_text_col)\n",
    "# df, pair_wise_sim_df = compute_similarity_source_target_files(source_df, target_df, vectorization=\"bert\", min_similarity=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# `Execute` - Final Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### A. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: config ::\n",
    "\n",
    "# Source File \n",
    "source_id_col = 'ID'                                # already present!, mention the column name here\n",
    "source_text_col = 'TEXT'                            # already present!, mention the column name here\n",
    "source_clean_text_col = 'clean_text'                # will be created, give a name\n",
    "source_dup_id_col = \"dup_idx\"                       # will be created, give a name\n",
    "source_similar_id_col = \"similar_idx\"               # will be created, give a name\n",
    "source_dup_similar_id_col = \"dup_similar_idx\"       # will be created, give a name\n",
    "\n",
    "\n",
    "# [OPTIONAL] \n",
    "# Target File\n",
    "target_id_col = 'ID'                                # already present!, mention the column name here\n",
    "target_text_col = 'TEXT'                            # already present!, mention the column name here\n",
    "target_clean_text_col = 'clean_TEXT2'               # will be created, give a name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     16,
     31,
     32
    ]
   },
   "outputs": [],
   "source": [
    "def read_source_file(source_fp):\n",
    "    if not os.path.exists(source_fp):\n",
    "        raise Exception(\"File not found!\")\n",
    "    if source_fp.lower().endswith('.csv'): \n",
    "        df = pd.read_csv(os.path.join(input_data_paths, source_fp))\n",
    "    elif source_fp.rsplit(\".\")[-1] in [\"xlsx\", \"xls\"]: \n",
    "        df = pd.read_excel(os.path.join(input_data_paths, source_fp))\n",
    "    else: \n",
    "        raise Exception(\"Unsupported file fromat!\")\n",
    "    # pre-processing\n",
    "    df = df[[source_id_col, source_text_col]]\n",
    "    df[source_text_col] = df[source_text_col].fillna(value=\"NONE\")\n",
    "    df = cleaning(df, source_text_col, source_clean_text_col)\n",
    "    print(\"Source file is ready.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "## OPTIONAL\n",
    "def read_target_file(target_fp):\n",
    "    if not os.path.exists(target_fp):\n",
    "        raise Exception(\"File not found!\")\n",
    "    if target_fp.lower().endswith('.csv'): \n",
    "        df = pd.read_csv(os.path.join(input_data_paths, target_fp))\n",
    "    elif target_fp.rsplit(\".\")[-1] in [\"xlsx\", \"xls\"]: \n",
    "        df = pd.read_excel(os.path.join(input_data_paths, target_fp))\n",
    "    else: \n",
    "        raise Exception(\"Unsupported file fromat!\")\n",
    "    # pre-processing\n",
    "    df = df[[target_id_col, target_text_col]]\n",
    "    df[target_text_col] = df[target_text_col].fillna(value=\"NONE\")\n",
    "    df = cleaning(df, target_text_col, target_clean_text_col)\n",
    "    print(\"Target file is ready.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file is ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-0703ebcb-f5ff-47ba-80af-5a85aede03c0\" \n",
       "            onclick=\"_export_df('0703ebcb-f5ff-47ba-80af-5a85aede03c0')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"0703ebcb-f5ff-47ba-80af-5a85aede03c0\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>How to open 529 accounts?</td>\n",
       "      <td>how open 529 account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>open 529</td>\n",
       "      <td>open 529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>how do people open 529 acc?</td>\n",
       "      <td>how do people open 529 acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>close my account</td>\n",
       "      <td>close account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>close ira accounts</td>\n",
       "      <td>close ira account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>closing all accounts</td>\n",
       "      <td>closing account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                         TEXT                  clean_text\n",
       "0  100    How to open 529 accounts?        how open 529 account\n",
       "1  101                     open 529                    open 529\n",
       "2  102  how do people open 529 acc?  how do people open 529 acc\n",
       "3  103             close my account               close account\n",
       "4  104           close ira accounts           close ira account\n",
       "5  105         closing all accounts             closing account"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load a source file ::\n",
    "\n",
    "\n",
    "source_fp = input_data_fp + \"/file_1.xlsx\"\n",
    "df_source = read_source_file(source_fp)\n",
    "df_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### C. Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert vectorization\n",
      "Stats:\n",
      "\n",
      "Orignal number of records = 6\n",
      "Total dup count = 3\n",
      "Total similar pairs found = 6\n",
      "Final number of records post dup-similar rows removal = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ms/dist/python/PROJ/ipykernel/4.8.2/lib/ipykernel_launcher.py:27: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-ec024f61-dff1-4cf6-bd2f-7cf2c9126179\" \n",
       "            onclick=\"_export_df('ec024f61-dff1-4cf6-bd2f-7cf2c9126179')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"ec024f61-dff1-4cf6-bd2f-7cf2c9126179\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>dup_idx</th>\n",
       "      <th>similar_idx</th>\n",
       "      <th>dup_similar_idx</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>How to open 529 accounts?</td>\n",
       "      <td>how open 529 account</td>\n",
       "      <td>[100]</td>\n",
       "      <td>[100, 102]</td>\n",
       "      <td>[100, 101, 102]</td>\n",
       "      <td>cluster_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>open 529</td>\n",
       "      <td>open 529</td>\n",
       "      <td>[101]</td>\n",
       "      <td>[100, 101]</td>\n",
       "      <td>[100, 101, 102]</td>\n",
       "      <td>cluster_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>how do people open 529 acc?</td>\n",
       "      <td>how do people open 529 acc</td>\n",
       "      <td>[102]</td>\n",
       "      <td>[100, 102]</td>\n",
       "      <td>[100, 101, 102]</td>\n",
       "      <td>cluster_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>close my account</td>\n",
       "      <td>close account</td>\n",
       "      <td>[103]</td>\n",
       "      <td>[103, 105]</td>\n",
       "      <td>[103, 105]</td>\n",
       "      <td>cluster_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>close ira accounts</td>\n",
       "      <td>close ira account</td>\n",
       "      <td>[104]</td>\n",
       "      <td>[104]</td>\n",
       "      <td>[104]</td>\n",
       "      <td>cluster_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>closing all accounts</td>\n",
       "      <td>closing account</td>\n",
       "      <td>[105]</td>\n",
       "      <td>[103, 105]</td>\n",
       "      <td>[103, 105]</td>\n",
       "      <td>cluster_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                         TEXT                  clean_text dup_idx similar_idx  dup_similar_idx cluster_id\n",
       "0  100    How to open 529 accounts?        how open 529 account   [100]  [100, 102]  [100, 101, 102]  cluster_1\n",
       "1  101                     open 529                    open 529   [101]  [100, 101]  [100, 101, 102]  cluster_1\n",
       "2  102  how do people open 529 acc?  how do people open 529 acc   [102]  [100, 102]  [100, 101, 102]  cluster_1\n",
       "3  103             close my account               close account   [103]  [103, 105]       [103, 105]  cluster_2\n",
       "4  104           close ira accounts           close ira account   [104]       [104]            [104]  cluster_3\n",
       "5  105         closing all accounts             closing account   [105]  [103, 105]       [103, 105]  cluster_2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Run -----------------------------------------------------\n",
    "# Compute Similarity on 1 file (i.e. source)\n",
    "#    >> runfind dups & very similar records in source file\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df_source = compute_similarity_source_file(df_source, source_min_similarity=0.75)\n",
    "df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target file is ready.\n",
      "bert vectorization\n"
     ]
    }
   ],
   "source": [
    "# 2. Run -----------------------------------------------------\n",
    "# Compute Similarity between 2 files\n",
    "#    >> run computaiton b/w source & target files\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# load another target file (for comparison)\n",
    "target_fp = input_data_fp + \"/file_2.xlsx\"\n",
    "df_target = read_target_file(target_fp)\n",
    "\n",
    "# run computaiton\n",
    "df, pair_wise_sim = compute_similarity_source_target_files(df_source,  df_target,\n",
    "                                                           vectorization=\"bert\",\n",
    "                                                           min_similarity=0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### D. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported output: df.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(output_data_fp + \"/df.csv\", index=False)\n",
    "print(\"Exported output:\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "Similarity_FastBert",
  "createdOn": 1648120119910,
  "creationTag": {
   "lastModifiedBy": {
    "login": "pranjp"
   },
   "lastModifiedOn": 1648120119910,
   "versionNumber": 0
  },
  "creator": "pranjp",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "python_38:Python",
   "language": "python",
   "name": "conda-env-python_38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "modifiedBy": "pranjp",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
